<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mmds on 湖间小筑</title>
    <link>http://hzmangel.github.io/tags/mmds/</link>
    <description>Recent content in Mmds on 湖间小筑</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>Copyright - ©2015 - hzmangel</copyright>
    <lastBuildDate>Wed, 24 Jun 2015 02:20:33 +0800</lastBuildDate>
    <atom:link href="http://hzmangel.github.io/tags/mmds/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>MMDS Notes: W2 - Nearest Neighbor Learning
</title>
      <link>http://hzmangel.github.io/post/mmds-w2-nearest-neighbor-learning/</link>
      <pubDate>Wed, 24 Jun 2015 02:20:33 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w2-nearest-neighbor-learning/</guid>
      <description>

&lt;p&gt;Locality-Sensitive Hashing，LSH，局部敏感hash或叫位置敏感hash。它的想法是在对原始数据空间的数据做Hash后，让位置相邻的数据有很大概率被放到同一个或者相近的bucket中，而不相邻的点放在一起的概率要很小。这样就会减少后期数据处理的数据集，从而简化后续的工作。&lt;/p&gt;

&lt;h2 id=&#34;相似数据集:e415793b7fb7c6a9af71ea301a4a0120&#34;&gt;相似数据集&lt;/h2&gt;

&lt;p&gt;许多数据挖掘的问题都能简化为查找相似数据集的问题，如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;查找含有相似单词的页面，用以给页面分类，或者找出页面的镜像站，或者检查剽窃等。&lt;/li&gt;
&lt;li&gt;NetFlix，理解成豆瓣就行，哪些用户有相似的爱好。&lt;/li&gt;
&lt;li&gt;以及，哪些电影有相似的粉丝。&lt;/li&gt;
&lt;li&gt;网上找到的个人信息，怎么才能确定哪些属于同一个人。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;先从相似文档找起，有三个关键技术：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Shingling：把文档，像页面啊，邮件啊，什么的，拆成sets。&lt;/li&gt;
&lt;li&gt;Minhashing：在保留相似性的基础上，把大的集合转化成短的标记。&lt;/li&gt;
&lt;li&gt;Locality-sensitive hashing：找出相似的对。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;文档相似性可以使用 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 来衡量。对于两个集合 &lt;code&gt;$S$&lt;/code&gt; 和 &lt;code&gt;$T$&lt;/code&gt; 来说，它们之间的 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 为 &lt;code&gt;$|S \cap T| / |S \cup T|$&lt;/code&gt;，记为 &lt;em&gt;SIM(S,T)&lt;/em&gt; 或 &lt;em&gt;J(S,T)&lt;/em&gt; 。不难看出，当此值为&lt;code&gt;0&lt;/code&gt;时表示两个集合没有交集，而为&lt;code&gt;1&lt;/code&gt;时则表示两个集合相等。&lt;/p&gt;

&lt;h3 id=&#34;k-shingling或叫k-gram:e415793b7fb7c6a9af71ea301a4a0120&#34;&gt;k-shingling或叫k-gram&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;k-shingling&lt;/em&gt; 是指把文档按连续的 &lt;em&gt;k&lt;/em&gt; 个字母拆成子集的方法。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;例如，给定文档&lt;code&gt;$D$&lt;/code&gt;的内容为&lt;code&gt;abcdabd&lt;/code&gt;，在&lt;code&gt;$k=2$&lt;/code&gt;的情况下，获得的 &lt;em&gt;2-shingling&lt;/em&gt; 集合为 &lt;code&gt;{ab, bc, cd, da, bd}&lt;/code&gt;。shingling有一个变种是生成一个bag而非set，此时重复的元素不会被归并，而是按照其本来出现的次数出现在最后结果中，如本例中的&lt;code&gt;ab&lt;/code&gt;将会出现2次。&lt;/p&gt;

&lt;p&gt;对于空格的处理有多种选项，较常见的是把所有空格类的东西都替换成一个空格，然后将其作为一个正常元素参与到shingling中去。即shingling后的元素可能会包含2个或多个单词。&lt;/p&gt;

&lt;p&gt;为了避免虚假的相似度， &lt;code&gt;$k$&lt;/code&gt; 的取值需要足够大。一般而言，对于短的如电子邮件之类的文件，取&lt;code&gt;$k=5$&lt;/code&gt;，而对于长的文档，如研究报告这种，取&lt;code&gt;$k=9$&lt;/code&gt;会比较好。&lt;/p&gt;

&lt;p&gt;对于shingling中的元素，可以直接使用字符串，但是更好的办法是把它通过hash变化映射到某个bucket中，而将这个bucket的编号作为shingling元素进行比较。这样可以在shingling元素空间不变的情况下，降低运行时占用的内存。而且在比较上，整数比字符串要更有优势。这一步叫做 &lt;strong&gt;Compressing Shinglings&lt;/strong&gt; 。&lt;/p&gt;

&lt;h3 id=&#34;minhashing:e415793b7fb7c6a9af71ea301a4a0120&#34;&gt;Minhashing&lt;/h3&gt;

&lt;p&gt;这一段是从 &lt;a href=&#34;https://en.wikipedia.org/wiki/MinHash&#34;&gt;wikipedia&lt;/a&gt; 上看到的定义：&lt;/p&gt;

&lt;p&gt;设有一个hash函数&lt;code&gt;$h$&lt;/code&gt;，可以将集合中的元素映射为不重复的整数值。这样对于任何集合&lt;code&gt;$S$&lt;/code&gt;，都能找到一个元素&lt;code&gt;$x$&lt;/code&gt;让&lt;code&gt;$h(S)$&lt;/code&gt;取到最小值&lt;code&gt;$h_{min}(S)$&lt;/code&gt;。这样就把对字符串比较，存储转换成了对整数的计算和存储。由于&lt;code&gt;$h_{min}(S)$&lt;/code&gt;只能得到一个值，所以需要使用 &lt;em&gt;Hash Function Family&lt;/em&gt; 去处理集合，以得到一个最小值的向量。在向量长度足够的情况下，两个集合的相似度等于最小值相等的概率。计算向量有两种办法，一种是选取足够多的hash函数，另一种是对一个hash得出的值作多次变换。&lt;/p&gt;

&lt;p&gt;在课程中，首先介绍了怎么抽取多个集合的 &lt;em&gt;Characteristic Matrix&lt;/em&gt; ，这个矩阵的每一列都是一个需要计算相似度的集合，记为&lt;code&gt;$S_{1}$&lt;/code&gt;到&lt;code&gt;$S_{N}$&lt;/code&gt;，而行是所有元素的集合，记为&lt;code&gt;$e_{1}$&lt;/code&gt;到&lt;code&gt;$e_{M}$&lt;/code&gt;。如果某个元素&lt;code&gt;$e_{i}$&lt;/code&gt;包含于集合&lt;code&gt;$S_{j}$&lt;/code&gt;中，则在矩阵相应的位置&lt;code&gt;$(i,j)$&lt;/code&gt;标上&lt;code&gt;1&lt;/code&gt;，反之则为&lt;code&gt;0&lt;/code&gt;。典型情况下这个矩阵是稀疏的。&lt;/p&gt;

&lt;p&gt;此后直接介绍了一个 &lt;em&gt;Minhashing&lt;/em&gt; 的函数簇。假设前述的 &lt;em&gt;Characteristic Matrix&lt;/em&gt; 的行排列是随机的，我们定义一个 &lt;em&gt;Minhashing&lt;/em&gt; 函数 &lt;code&gt;h(S)&lt;/code&gt; ，它的值是在特定排列下，列 &lt;code&gt;S&lt;/code&gt; 中第一次出现 &lt;code&gt;1&lt;/code&gt; 的行数。使用多个独立的哈希函数（如100个），即可为每一个集合创建一个 &lt;em&gt;signatures&lt;/em&gt; ，而多个集合的结果合并后可以生成一个新的矩阵， &lt;em&gt;signatures matrix&lt;/em&gt; 。这个矩阵的列是各个集合，而行是某一次计算 &lt;em&gt;Minhashing&lt;/em&gt; 时的结果。&lt;/p&gt;

&lt;p&gt;下面来分析下 &lt;em&gt;Jaccard Similarity*。首先看 *Characteristic Matrix&lt;/em&gt; 。设有两个需要比较的集合 &lt;code&gt;$S_{1}$&lt;/code&gt; 和 &lt;code&gt;$S_{2}$&lt;/code&gt; ，假设它们的 &lt;em&gt;Characteristic Matrix&lt;/em&gt; 为 &lt;code&gt;$M$&lt;/code&gt;，那么在矩阵 &lt;code&gt;$M$&lt;/code&gt; 中，每一行的元素只有4种组合： &lt;code&gt;(0,0)&lt;/code&gt; ，&lt;code&gt;(0,1)&lt;/code&gt; ， &lt;code&gt;(1,0)&lt;/code&gt; 和 &lt;code&gt;(1,1)&lt;/code&gt;。我们把这4种关系在M中的数量分别记为ABCD，不难看出，两个集合的相似度可以表示为 &lt;code&gt;$J(S_{1}, S_{2}) = D/(A+B+C)$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;然后再来看 &lt;em&gt;signatures matrix&lt;/em&gt; 。在某个特定的排列下，如果两个集合的 &lt;em&gt;Minhashing&lt;/em&gt; 值相同，那第它们一定是 &lt;code&gt;(1,1)&lt;/code&gt; 形式的，而其它三种形式不会有此结果）注意，此处只能保证， &lt;em&gt;Minhashing&lt;/em&gt; 值相同时，能保证这一行是 &lt;code&gt;(1,1)&lt;/code&gt;，但是一行是&lt;code&gt;(1,1)&lt;/code&gt;并不能说明这一行是 &lt;em&gt;Minhashing&lt;/em&gt; 值）。所以可以得知，两个集合 &lt;em&gt;Minhashing&lt;/em&gt; 值相等的概率，也就是两个集合的 &lt;em&gt;Jaccard&lt;/em&gt; 相似度，都是 &lt;code&gt;$J(S_{1}, S_{2}) = D/(A+B+C)$&lt;/code&gt; 。&lt;/p&gt;

&lt;p&gt;在实际实现上，给大量的数据做随机排列是比较难以实现的，所以更加通用的办法就是如wiki上说的，挑选多个 hash 函数来处理，下面是一段伪代码，计算某集合的 &lt;em&gt;Minhashing&lt;/em&gt; 向量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FOREACH hash_func_family:
  CALCULATE hi(r)

FOREACH columes:
  IF val(c) == 1:
    # Init value for SIG(i, c) is inf
    SIG(i, c) = min( SIG(i, c), hi(r) )
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;locality-sensitive-hash:e415793b7fb7c6a9af71ea301a4a0120&#34;&gt;Locality-Sensitive Hash&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;By Me: 此处的概念还有些模糊，需要再啃啃。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;经过了前面的 &lt;em&gt;Shingling&lt;/em&gt; 和 &lt;em&gt;Minhashing&lt;/em&gt; ，需要处理的数据已经减少许多了，但是对于大文档集来说还不够。如果是需要找到任意两个集合之间的相似度，那么除了计算它们每两对之间的相似度以外没有其它任何办法。但是如果只是需要找到超过某个相似度阈值的集合对，则可以使用LSH，又叫 &lt;em&gt;Nearest Neighbor search&lt;/em&gt; 。&lt;/p&gt;

&lt;p&gt;LSH的一般做法是对元素使用多次Hash，让相似的元素落入同一个bucket中（即Hash冲突），而不相似的不在。对于上面生成的 &lt;em&gt;signatures matrix&lt;/em&gt; ，一个有效的办法是把矩阵按&lt;code&gt;r&lt;/code&gt;行分成&lt;code&gt;b&lt;/code&gt;个brand，对每一个brand中的每一小块长度为&lt;code&gt;r&lt;/code&gt;的特征值做hash，下面是分析（这块还是有些地方没想清楚，先记录下来）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;设矩阵分成了&lt;code&gt;b&lt;/code&gt;个 &lt;em&gt;brand&lt;/em&gt; ，每个 &lt;em&gt;brand&lt;/em&gt; 中有 &lt;code&gt;r&lt;/code&gt; 行。某特定两个文档的 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 是 &lt;code&gt;s&lt;/code&gt; 。即在 matrix 中某个Minhashing字符串与其它有&lt;code&gt;s&lt;/code&gt;的概率相似。&lt;/li&gt;
&lt;li&gt;某个brand中选定的特征列和其它所有列相似的概率是 &lt;code&gt;$s^{r}$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;某个brand中选定的特征列和至少一个其它列不相似的概率是&lt;code&gt;$1-s^{r}$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;一个特征列和每一个brand中都有至少一个不相似列的概率是&lt;code&gt;$(1-s^{r})^{b}$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;一个特征列和至少一个brand中所有列都相似，从而成为 &lt;em&gt;candidate pair&lt;/em&gt; 的概率为 &lt;code&gt;$1-(1-s^{r})^{b}$&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个曲线是一个S型的连续曲线，我们需要做的就是通过挑选&lt;code&gt;b&lt;/code&gt;和&lt;code&gt;r&lt;/code&gt;，让这条曲线在两端尽量的平缓，而在中间部分尽可能的陡峭。这样就不会有过多的 &lt;em&gt;False Positive&lt;/em&gt; 或者 &lt;em&gt;False Negative&lt;/em&gt; 。&lt;/p&gt;

&lt;h2 id=&#34;具体使用流程:e415793b7fb7c6a9af71ea301a4a0120&#34;&gt;具体使用流程&lt;/h2&gt;

&lt;p&gt;综上所述，在实际应用中会有下面几步工作（文档比较）：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;选择整数 &lt;code&gt;$k$&lt;/code&gt; ，将输入文档转换为 &lt;em&gt;k-shingling&lt;/em&gt; 集合。此处可以通过Hash将 &lt;em&gt;k-shingles&lt;/em&gt; 转换为较短的 &lt;em&gt;bucket序号&lt;/em&gt; 。&lt;/li&gt;
&lt;li&gt;以 &lt;em&gt;shingle&lt;/em&gt; 排序 &lt;code&gt;&amp;lt;document, shingle&amp;gt;&lt;/code&gt; 对。&lt;/li&gt;
&lt;li&gt;选择长度 &lt;code&gt;$n$&lt;/code&gt; 用于 &lt;em&gt;Minhashing Signature&lt;/em&gt; ，并为所有文档计算特征值。&lt;/li&gt;
&lt;li&gt;确定一个概率 &lt;code&gt;$t$&lt;/code&gt; 作为文档相似度的阈值，选择 &lt;code&gt;$b$&lt;/code&gt; 和 &lt;code&gt;$r$&lt;/code&gt; 并保证 &lt;code&gt;$br=n$&lt;/code&gt; ，而且阈值&lt;code&gt;$t$&lt;/code&gt;接近&lt;code&gt;$(1/b)^{1/r}$&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;如果需要最大程度的避免 &lt;em&gt;False Negative&lt;/em&gt; ，那么选择 &lt;code&gt;$b$&lt;/code&gt; 和 &lt;code&gt;$r$&lt;/code&gt; 时要注意计算出来的值要小于 &lt;code&gt;$t$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;如果需要保证速度而且避免 &lt;em&gt;False Positive&lt;/em&gt; ，那么选择 &lt;code&gt;$b$&lt;/code&gt; 和 &lt;code&gt;$r$&lt;/code&gt; 时注意计算出一个高的阈值。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;使用 &lt;em&gt;LSH&lt;/em&gt; 找到所有的 *candidate pairs*。&lt;/li&gt;
&lt;li&gt;检查选择出来的特征对，确定它们的相似度都大于 &lt;code&gt;$t$&lt;/code&gt; 。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;实例:e415793b7fb7c6a9af71ea301a4a0120&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;注：此处只是列出课程中出现的示例，后续会尝试使用程序完成，再补齐说明。&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;entity-resolution:e415793b7fb7c6a9af71ea301a4a0120&#34;&gt;Entity Resolution&lt;/h3&gt;

&lt;h3 id=&#34;fingerprints:e415793b7fb7c6a9af71ea301a4a0120&#34;&gt;Fingerprints&lt;/h3&gt;

&lt;h3 id=&#34;similar-news-articles:e415793b7fb7c6a9af71ea301a4a0120&#34;&gt;Similar News Articles&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W2 - Locality-Sensitive Hashing
</title>
      <link>http://hzmangel.github.io/post/mmds-w2-locality-sensitive-hashing/</link>
      <pubDate>Wed, 17 Jun 2015 08:02:10 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w2-locality-sensitive-hashing/</guid>
      <description>

&lt;p&gt;Locality-Sensitive Hashing，LSH，局部敏感hash或叫位置敏感hash。它的想法是在对原始数据空间的数据做Hash后，让位置相邻的数据有很大概率被放到同一个或者相近的bucket中，而不相邻的点放在一起的概率要很小。这样就会减少后期数据处理的数据集，从而简化后续的工作。&lt;/p&gt;

&lt;h2 id=&#34;相似数据集:98fc395032abb3e5c8d913488aa084ef&#34;&gt;相似数据集&lt;/h2&gt;

&lt;p&gt;许多数据挖掘的问题都能简化为查找相似数据集的问题，如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;查找含有相似单词的页面，用以给页面分类，或者找出页面的镜像站，或者检查剽窃等。&lt;/li&gt;
&lt;li&gt;NetFlix，理解成豆瓣就行，哪些用户有相似的爱好。&lt;/li&gt;
&lt;li&gt;以及，哪些电影有相似的粉丝。&lt;/li&gt;
&lt;li&gt;网上找到的个人信息，怎么才能确定哪些属于同一个人。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;先从相似文档找起，有三个关键技术：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Shingling：把文档，像页面啊，邮件啊，什么的，拆成sets。&lt;/li&gt;
&lt;li&gt;Minhashing：在保留相似性的基础上，把大的集合转化成短的标记。&lt;/li&gt;
&lt;li&gt;Locality-sensitive hashing：找出相似的对。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;文档相似性可以使用 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 来衡量。对于两个集合 &lt;code&gt;$S$&lt;/code&gt; 和 &lt;code&gt;$T$&lt;/code&gt; 来说，它们之间的 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 为 &lt;code&gt;$|S \cap T| / |S \cup T|$&lt;/code&gt;，记为 &lt;em&gt;SIM(S,T)&lt;/em&gt; 或 &lt;em&gt;J(S,T)&lt;/em&gt; 。不难看出，当此值为&lt;code&gt;0&lt;/code&gt;时表示两个集合没有交集，而为&lt;code&gt;1&lt;/code&gt;时则表示两个集合相等。&lt;/p&gt;

&lt;h3 id=&#34;k-shingling或叫k-gram:98fc395032abb3e5c8d913488aa084ef&#34;&gt;k-shingling或叫k-gram&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;k-shingling&lt;/em&gt; 是指把文档按连续的 &lt;em&gt;k&lt;/em&gt; 个字母拆成子集的方法。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;例如，给定文档&lt;code&gt;$D$&lt;/code&gt;的内容为&lt;code&gt;abcdabd&lt;/code&gt;，在&lt;code&gt;$k=2$&lt;/code&gt;的情况下，获得的 &lt;em&gt;2-shingling&lt;/em&gt; 集合为 &lt;code&gt;{ab, bc, cd, da, bd}&lt;/code&gt;。shingling有一个变种是生成一个bag而非set，此时重复的元素不会被归并，而是按照其本来出现的次数出现在最后结果中，如本例中的&lt;code&gt;ab&lt;/code&gt;将会出现2次。&lt;/p&gt;

&lt;p&gt;对于空格的处理有多种选项，较常见的是把所有空格类的东西都替换成一个空格，然后将其作为一个正常元素参与到shingling中去。即shingling后的元素可能会包含2个或多个单词。&lt;/p&gt;

&lt;p&gt;为了避免虚假的相似度， &lt;code&gt;$k$&lt;/code&gt; 的取值需要足够大。一般而言，对于短的如电子邮件之类的文件，取&lt;code&gt;$k=5$&lt;/code&gt;，而对于长的文档，如研究报告这种，取&lt;code&gt;$k=9$&lt;/code&gt;会比较好。&lt;/p&gt;

&lt;p&gt;对于shingling中的元素，可以直接使用字符串，但是更好的办法是把它通过hash变化映射到某个bucket中，而将这个bucket的编号作为shingling元素进行比较。这样可以在shingling元素空间不变的情况下，降低运行时占用的内存。而且在比较上，整数比字符串要更有优势。这一步叫做 &lt;strong&gt;Compressing Shinglings&lt;/strong&gt; 。&lt;/p&gt;

&lt;h3 id=&#34;minhashing:98fc395032abb3e5c8d913488aa084ef&#34;&gt;Minhashing&lt;/h3&gt;

&lt;p&gt;这一段是从 &lt;a href=&#34;https://en.wikipedia.org/wiki/MinHash&#34;&gt;wikipedia&lt;/a&gt; 上看到的定义：&lt;/p&gt;

&lt;p&gt;设有一个hash函数&lt;code&gt;$h$&lt;/code&gt;，可以将集合中的元素映射为不重复的整数值。这样对于任何集合&lt;code&gt;$S$&lt;/code&gt;，都能找到一个元素&lt;code&gt;$x$&lt;/code&gt;让&lt;code&gt;$h(S)$&lt;/code&gt;取到最小值&lt;code&gt;$h_{min}(S)$&lt;/code&gt;。这样就把对字符串比较，存储转换成了对整数的计算和存储。由于&lt;code&gt;$h_{min}(S)$&lt;/code&gt;只能得到一个值，所以需要使用 &lt;em&gt;Hash Function Family&lt;/em&gt; 去处理集合，以得到一个最小值的向量。在向量长度足够的情况下，两个集合的相似度等于最小值相等的概率。计算向量有两种办法，一种是选取足够多的hash函数，另一种是对一个hash得出的值作多次变换。&lt;/p&gt;

&lt;p&gt;在课程中，首先介绍了怎么抽取多个集合的 &lt;em&gt;Characteristic Matrix&lt;/em&gt; ，这个矩阵的每一列都是一个需要计算相似度的集合，记为&lt;code&gt;$S_{1}$&lt;/code&gt;到&lt;code&gt;$S_{N}$&lt;/code&gt;，而行是所有元素的集合，记为&lt;code&gt;$e_{1}$&lt;/code&gt;到&lt;code&gt;$e_{M}$&lt;/code&gt;。如果某个元素&lt;code&gt;$e_{i}$&lt;/code&gt;包含于集合&lt;code&gt;$S_{j}$&lt;/code&gt;中，则在矩阵相应的位置&lt;code&gt;$(i,j)$&lt;/code&gt;标上&lt;code&gt;1&lt;/code&gt;，反之则为&lt;code&gt;0&lt;/code&gt;。典型情况下这个矩阵是稀疏的。&lt;/p&gt;

&lt;p&gt;此后直接介绍了一个 &lt;em&gt;Minhashing&lt;/em&gt; 的函数簇。假设前述的 &lt;em&gt;Characteristic Matrix&lt;/em&gt; 的行排列是随机的，我们定义一个 &lt;em&gt;Minhashing&lt;/em&gt; 函数 &lt;code&gt;h(S)&lt;/code&gt; ，它的值是在特定排列下，列 &lt;code&gt;S&lt;/code&gt; 中第一次出现 &lt;code&gt;1&lt;/code&gt; 的行数。使用多个独立的哈希函数（如100个），即可为每一个集合创建一个 &lt;em&gt;signatures&lt;/em&gt; ，而多个集合的结果合并后可以生成一个新的矩阵， &lt;em&gt;signatures matrix&lt;/em&gt; 。这个矩阵的列是各个集合，而行是某一次计算 &lt;em&gt;Minhashing&lt;/em&gt; 时的结果。&lt;/p&gt;

&lt;p&gt;下面来分析下 &lt;em&gt;Jaccard Similarity*。首先看 *Characteristic Matrix&lt;/em&gt; 。设有两个需要比较的集合 &lt;code&gt;$S_{1}$&lt;/code&gt; 和 &lt;code&gt;$S_{2}$&lt;/code&gt; ，假设它们的 &lt;em&gt;Characteristic Matrix&lt;/em&gt; 为 &lt;code&gt;$M$&lt;/code&gt;，那么在矩阵 &lt;code&gt;$M$&lt;/code&gt; 中，每一行的元素只有4种组合： &lt;code&gt;(0,0)&lt;/code&gt; ，&lt;code&gt;(0,1)&lt;/code&gt; ， &lt;code&gt;(1,0)&lt;/code&gt; 和 &lt;code&gt;(1,1)&lt;/code&gt;。我们把这4种关系在M中的数量分别记为ABCD，不难看出，两个集合的相似度可以表示为 &lt;code&gt;$J(S_{1}, S_{2}) = D/(A+B+C)$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;然后再来看 &lt;em&gt;signatures matrix&lt;/em&gt; 。在某个特定的排列下，如果两个集合的 &lt;em&gt;Minhashing&lt;/em&gt; 值相同，那第它们一定是 &lt;code&gt;(1,1)&lt;/code&gt; 形式的，而其它三种形式不会有此结果）注意，此处只能保证， &lt;em&gt;Minhashing&lt;/em&gt; 值相同时，能保证这一行是 &lt;code&gt;(1,1)&lt;/code&gt;，但是一行是&lt;code&gt;(1,1)&lt;/code&gt;并不能说明这一行是 &lt;em&gt;Minhashing&lt;/em&gt; 值）。所以可以得知，两个集合 &lt;em&gt;Minhashing&lt;/em&gt; 值相等的概率，也就是两个集合的 &lt;em&gt;Jaccard&lt;/em&gt; 相似度，都是 &lt;code&gt;$J(S_{1}, S_{2}) = D/(A+B+C)$&lt;/code&gt; 。&lt;/p&gt;

&lt;p&gt;在实际实现上，给大量的数据做随机排列是比较难以实现的，所以更加通用的办法就是如wiki上说的，挑选多个 hash 函数来处理，下面是一段伪代码，计算某集合的 &lt;em&gt;Minhashing&lt;/em&gt; 向量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FOREACH hash_func_family:
  CALCULATE hi(r)

FOREACH columes:
  IF val(c) == 1:
    # Init value for SIG(i, c) is inf
    SIG(i, c) = min( SIG(i, c), hi(r) )
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;locality-sensitive-hash:98fc395032abb3e5c8d913488aa084ef&#34;&gt;Locality-Sensitive Hash&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;By Me: 此处的概念还有些模糊，需要再啃啃。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;经过了前面的 &lt;em&gt;Shingling&lt;/em&gt; 和 &lt;em&gt;Minhashing&lt;/em&gt; ，需要处理的数据已经减少许多了，但是对于大文档集来说还不够。如果是需要找到任意两个集合之间的相似度，那么除了计算它们每两对之间的相似度以外没有其它任何办法。但是如果只是需要找到超过某个相似度阈值的集合对，则可以使用LSH，又叫 &lt;em&gt;Nearest Neighbor search&lt;/em&gt; 。&lt;/p&gt;

&lt;p&gt;LSH的一般做法是对元素使用多次Hash，让相似的元素落入同一个bucket中（即Hash冲突），而不相似的不在。对于上面生成的 &lt;em&gt;signatures matrix&lt;/em&gt; ，一个有效的办法是把矩阵按&lt;code&gt;r&lt;/code&gt;行分成&lt;code&gt;b&lt;/code&gt;个brand，对每一个brand中的每一小块长度为&lt;code&gt;r&lt;/code&gt;的特征值做hash，下面是分析（这块还是有些地方没想清楚，先记录下来）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;设矩阵分成了&lt;code&gt;b&lt;/code&gt;个 &lt;em&gt;brand&lt;/em&gt; ，每个 &lt;em&gt;brand&lt;/em&gt; 中有 &lt;code&gt;r&lt;/code&gt; 行。某特定两个文档的 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 是 &lt;code&gt;s&lt;/code&gt; 。即在 matrix 中某个Minhashing字符串与其它有&lt;code&gt;s&lt;/code&gt;的概率相似。&lt;/li&gt;
&lt;li&gt;某个brand中选定的特征列和其它所有列相似的概率是 &lt;code&gt;$s^{r}$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;某个brand中选定的特征列和至少一个其它列不相似的概率是&lt;code&gt;$1-s^{r}$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;一个特征列和每一个brand中都有至少一个不相似列的概率是&lt;code&gt;$(1-s^{r})^{b}$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;一个特征列和至少一个brand中所有列都相似，从而成为 &lt;em&gt;candidate pair&lt;/em&gt; 的概率为 &lt;code&gt;$1-(1-s^{r})^{b}$&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个曲线是一个S型的连续曲线，我们需要做的就是通过挑选&lt;code&gt;b&lt;/code&gt;和&lt;code&gt;r&lt;/code&gt;，让这条曲线在两端尽量的平缓，而在中间部分尽可能的陡峭。这样就不会有过多的 &lt;em&gt;False Positive&lt;/em&gt; 或者 &lt;em&gt;False Negative&lt;/em&gt; 。&lt;/p&gt;

&lt;h2 id=&#34;具体使用流程:98fc395032abb3e5c8d913488aa084ef&#34;&gt;具体使用流程&lt;/h2&gt;

&lt;p&gt;综上所述，在实际应用中会有下面几步工作（文档比较）：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;选择整数 &lt;code&gt;$k$&lt;/code&gt; ，将输入文档转换为 &lt;em&gt;k-shingling&lt;/em&gt; 集合。此处可以通过Hash将 &lt;em&gt;k-shingles&lt;/em&gt; 转换为较短的 &lt;em&gt;bucket序号&lt;/em&gt; 。&lt;/li&gt;
&lt;li&gt;以 &lt;em&gt;shingle&lt;/em&gt; 排序 &lt;code&gt;&amp;lt;document, shingle&amp;gt;&lt;/code&gt; 对。&lt;/li&gt;
&lt;li&gt;选择长度 &lt;code&gt;$n$&lt;/code&gt; 用于 &lt;em&gt;Minhashing Signature&lt;/em&gt; ，并为所有文档计算特征值。&lt;/li&gt;
&lt;li&gt;确定一个概率 &lt;code&gt;$t$&lt;/code&gt; 作为文档相似度的阈值，选择 &lt;code&gt;$b$&lt;/code&gt; 和 &lt;code&gt;$r$&lt;/code&gt; 并保证 &lt;code&gt;$br=n$&lt;/code&gt; ，而且阈值&lt;code&gt;$t$&lt;/code&gt;接近&lt;code&gt;$(1/b)^{1/r}$&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;如果需要最大程度的避免 &lt;em&gt;False Negative&lt;/em&gt; ，那么选择 &lt;code&gt;$b$&lt;/code&gt; 和 &lt;code&gt;$r$&lt;/code&gt; 时要注意计算出来的值要小于 &lt;code&gt;$t$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;如果需要保证速度而且避免 &lt;em&gt;False Positive&lt;/em&gt; ，那么选择 &lt;code&gt;$b$&lt;/code&gt; 和 &lt;code&gt;$r$&lt;/code&gt; 时注意计算出一个高的阈值。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;使用 &lt;em&gt;LSH&lt;/em&gt; 找到所有的 *candidate pairs*。&lt;/li&gt;
&lt;li&gt;检查选择出来的特征对，确定它们的相似度都大于 &lt;code&gt;$t$&lt;/code&gt; 。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;实例:98fc395032abb3e5c8d913488aa084ef&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;注：此处只是列出课程中出现的示例，后续会尝试使用程序完成，再补齐说明。&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;entity-resolution:98fc395032abb3e5c8d913488aa084ef&#34;&gt;Entity Resolution&lt;/h3&gt;

&lt;h3 id=&#34;fingerprints:98fc395032abb3e5c8d913488aa084ef&#34;&gt;Fingerprints&lt;/h3&gt;

&lt;h3 id=&#34;similar-news-articles:98fc395032abb3e5c8d913488aa084ef&#34;&gt;Similar News Articles&lt;/h3&gt;

&lt;h2 id=&#34;距离计算:98fc395032abb3e5c8d913488aa084ef&#34;&gt;距离计算&lt;/h2&gt;

&lt;p&gt;此块知识的最后提到了距离的计算。从某种意义上说，计算LSH即是计算某两个点之间的距离，越相似的点距离越近。上面提到的 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 并不是距离，用&lt;code&gt;1&lt;/code&gt;减去它才是。一般说来，有两种类型的距离，它们是 &lt;em&gt;欧氏距离&lt;/em&gt; 和 &lt;em&gt;非欧距离&lt;/em&gt; 。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;欧氏距离是指欧氏空间的距离，欧氏空间包含有实实在在维度，和密集的点。&lt;/li&gt;
&lt;li&gt;欧空中，可以在两个点之间找到中点。&lt;/li&gt;
&lt;li&gt;欧氏距离是基于欧空中点的位置来确定的&lt;/li&gt;
&lt;li&gt;其它的空间即被称为非欧空间。在非欧空间中，距离的计算是通过点的其它一些特性来完成的，因为非欧空间并没有位置这个概念。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;假设 &lt;code&gt;$x$&lt;/code&gt; ， &lt;code&gt;$y$&lt;/code&gt; 和 &lt;code&gt;$z$&lt;/code&gt; 是某个空间中的点，而 &lt;code&gt;$d$&lt;/code&gt; 是计算距离的函数，那么它有如下特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;$d(x,y) \geq 0$&lt;/code&gt; ：所有距离都是非负值。&lt;/li&gt;
&lt;li&gt;仅在 &lt;code&gt;$x$&lt;/code&gt; ， &lt;code&gt;$y$&lt;/code&gt; 是同一点时，才有 &lt;code&gt;$d(x,y) = 0$&lt;/code&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$d(x,y) = d(y,x)$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$d(x,z) \leq d(x,y) + d(y,z)$&lt;/code&gt; ：三角定理。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;欧氏距离:98fc395032abb3e5c8d913488aa084ef&#34;&gt;欧氏距离&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;$L_{r}-norm = (\sum_{i=1}^{n} |x_{i} - y_{i}|^{r})^{1/r}$&lt;/code&gt; 。在&lt;code&gt;$r=2$&lt;/code&gt;时，即变成平方各开根号，即熟悉的距离计算。此外还有&lt;code&gt;$L_{1}-norm$&lt;/code&gt; 。&lt;/p&gt;

&lt;h3 id=&#34;非欧距离:98fc395032abb3e5c8d913488aa084ef&#34;&gt;非欧距离&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Jaccard Distance&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;余弦距离&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Edit Distance&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Hamming Distance&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W1 - Link Analysis
</title>
      <link>http://hzmangel.github.io/post/mmds-w1-link-analysis/</link>
      <pubDate>Sun, 14 Jun 2015 17:10:58 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w1-link-analysis/</guid>
      <description>

&lt;p&gt;第一周的后半部分讲的是Link Analysis，主要讲的是&lt;strong&gt;PageRank&lt;/strong&gt;的计算。&lt;/p&gt;

&lt;p&gt;互联网在某种意义上是一个有向图，每个页面是图上的节点，而页面间的链接就是图的边。在经历了早期的目录式页面分类后，web现在进入了以Search为主的的组织方式。下面问题来了：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;怎么确定找到的信息是可以信赖的（或者相对可以信赖的）。&lt;/li&gt;
&lt;li&gt;当我们查找某个词时，哪个才是最好的结果。&lt;/li&gt;
&lt;li&gt;&lt;del&gt;搜索技术哪家强？&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以这里引入了给页面排序的做法，以确定页面的重要性。&lt;/p&gt;

&lt;h2 id=&#34;pagerank:b45bcfea51201690689ab7d1025dd751&#34;&gt;PageRank&lt;/h2&gt;

&lt;p&gt;PageRank，Google家的看家算法。核心思路是，如果一个页面是重要的，那么它指向的那个页面应该也是重要的。也就是用其它页面来证明这个页面的重要性。&lt;/p&gt;

&lt;h3 id=&#34;flow-formulation:b45bcfea51201690689ab7d1025dd751&#34;&gt;&lt;em&gt;Flow&lt;/em&gt; Formulation&lt;/h3&gt;

&lt;p&gt;在一张图中，假设有节点&lt;code&gt;$i$&lt;/code&gt;，它的PR值是&lt;code&gt;$r_{i}$&lt;/code&gt;，它有&lt;code&gt;$d_{i}$&lt;/code&gt;条出链，其中一条指向节点&lt;code&gt;$j$&lt;/code&gt;。那么&lt;code&gt;$j$&lt;/code&gt;上由&lt;code&gt;$i$&lt;/code&gt;带来的PR值即为&lt;code&gt;$\frac{r_{i}}{d_{i}}$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;根据这个需求，可以列出来一个方程组。显然，图中有几个节点，这个方程组就会有几个方程，为了给这个方程求得一个固定解，我们会人为的加上一个条件 &lt;code&gt;$\sum{r_{i}} = 1$&lt;/code&gt;，这样就可以求解出每个节点的PR值了。&lt;/p&gt;

&lt;p&gt;这个方法比较易于理解，但是对于大规模的页面集不适用，所以引入了&lt;em&gt;Matrix&lt;/em&gt; Formulation。&lt;/p&gt;

&lt;h3 id=&#34;matrix-formulation:b45bcfea51201690689ab7d1025dd751&#34;&gt;&lt;em&gt;Matrix&lt;/em&gt; Formulation&lt;/h3&gt;

&lt;p&gt;首先，把所有页面之间的跳转都用一个&lt;strong&gt;列随机矩阵(column stochastic matrix)&lt;/strong&gt;来表示，记为&lt;code&gt;$M$&lt;/code&gt;。对于每条链路&lt;code&gt;$i\rightarrow j$&lt;/code&gt;，都有相应的&lt;code&gt;$M_{ji} = 1/d_{i}$&lt;/code&gt;，其中&lt;code&gt;$d$&lt;/code&gt;是&lt;code&gt;$i$&lt;/code&gt;的出链路条数。下一步是&lt;strong&gt;Rank向量&lt;/strong&gt;，记为&lt;code&gt;$r$&lt;/code&gt;它是一个1维的列向量，每一个元素的值就是表示此节点的Rank值，记为&lt;code&gt;$r_{i}$&lt;/code&gt;，此向量满足&lt;code&gt;$\sum_{i}r_{i} = 1$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;有此定义后，上面介绍的*Flow*方程可以转换成这样： &lt;code&gt;$r = M \cdot r$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;由矩阵的定义，这个&lt;code&gt;$r$&lt;/code&gt;是矩阵&lt;code&gt;$M$&lt;/code&gt;的&lt;strong&gt;单位向量(eigenvector)&lt;/strong&gt;。即，页面的PageRank值，就是这些页面之间转移矩阵的单位向量，解出了这个向量，也就确定了这些页面的PageRank值。&lt;/p&gt;

&lt;p&gt;求解特征向量使用的是被称为*Power Iteration*的方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;初始化： &lt;code&gt;$r^{(0)} = [1/N, \ldots , 1/N]^{T}$&lt;/code&gt; ，其中&lt;code&gt;$N$&lt;/code&gt;是图中的节点数，也即所有页面个数。&lt;/li&gt;
&lt;li&gt;迭代： &lt;code&gt;$r^{(t+1)} = M \cdot r^{(t)}$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;停止条件： &lt;code&gt;$|r^{(t+1) = r^{(t)}}|_{1} &amp;lt; \epsilon$&lt;/code&gt;。其中&lt;code&gt;$|x_{i}|_{1} ＝ \sum_{i}|x_{i}|$&lt;/code&gt;是向量&lt;code&gt;$i$&lt;/code&gt;的 &lt;em&gt;L1范数&lt;/em&gt; （其实就是绝对值相加，范数的定义就是 &lt;code&gt;$|x_{i}|_{p} = (\sum_{i}|x_{i}|^{p})^{\frac{1}{p}}$&lt;/code&gt;）。此处可以使用其它的范数，如L2范数，即欧氏距离。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;teleports:b45bcfea51201690689ab7d1025dd751&#34;&gt;&lt;em&gt;Teleports&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;如果图中有 &lt;em&gt;dead end&lt;/em&gt; 节点，即只有进链没有出链；或者是在多个节点之间存在环，那么上面的迭代就会收敛到一个错误的结果上。解决方案就是，对于每次跳转，有&lt;code&gt;$\beta$&lt;/code&gt;的概率跟随链路去跳，而有&lt;code&gt;$1-\beta$&lt;/code&gt;的概率是一次随机传送 (&lt;em&gt;Teleports&lt;/em&gt;)，这样就解决上面提到的两个问题了。在实际使用中，一般&lt;code&gt;$\beta$&lt;/code&gt;取值为&lt;code&gt;0.8&lt;/code&gt;或&lt;code&gt;0.9&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;在计算上，就是引入一个Teleports矩阵，其中的每个值都是&lt;code&gt;$1/N$&lt;/code&gt;，&lt;code&gt;$N$&lt;/code&gt;为节点数目。而之前的转移矩阵&lt;code&gt;$M$&lt;/code&gt;则变为&lt;code&gt;$\beta M + (1-\beta)\frac{1}{N}e \cdot e^{T}$&lt;/code&gt;，记为&lt;code&gt;$A$&lt;/code&gt;。同样，状态跳转的迭代公式也变为&lt;code&gt;$r = A \cdot r$&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&#34;实际使用的样子:b45bcfea51201690689ab7d1025dd751&#34;&gt;实际使用的样子&lt;/h3&gt;

&lt;p&gt;以上说的都是理论，在那个神奇的世界里，计算机的内存都是无限大的，计算速度都是无限NB的，一句话，TA们都是超限界的。在回到了位于爬行界的真实世界后，还有其它需要考虑的东西。假设节点数目是 &lt;strong&gt;1 billion&lt;/strong&gt; ，那么计算前和后的向量各需要&lt;strong&gt;1 billion&lt;/strong&gt;，但是对于那个矩阵，它可是&lt;strong&gt;1 billion * 1 billion&lt;/strong&gt;，也就是&lt;strong&gt;10^18&lt;/strong&gt;。这个内存，有点贵哈～&lt;/p&gt;

&lt;p&gt;一般来说，转移矩阵都是稀疏的，这样在存储的时候可以不用存多少东西，但是加了那个Teleports后，它变成每个位置都有值了，这内存使用就duang的一下上来了。还好经过计算，发现公式&lt;code&gt;$r = A \cdot r$&lt;/code&gt; 可以改写成：&lt;code&gt;$r = \beta M \cdot r + [ \frac{1-\beta}{N} ]_{N}$&lt;/code&gt;。这就是说，矩阵还是那个稀疏的矩阵，但是在每次算完后，需要在向量上加上Teleports的结果。这样一来，占用的内存又回去了吧。&lt;/p&gt;

&lt;p&gt;基本上就是这样了~&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W1 - HDFS &amp; MR
</title>
      <link>http://hzmangel.github.io/post/mmds-w1-hdfs-mr/</link>
      <pubDate>Sat, 13 Jun 2015 23:13:32 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w1-hdfs-mr/</guid>
      <description>

&lt;p&gt;前段时间在Cousera上各种挤时间跟完了一门 &lt;a href=&#34;https://class.coursera.org/mmds-002&#34;&gt;MMDS&lt;/a&gt; ，手上留下了一堆笔记，整理下，顺便给新blog开光吧。&lt;/p&gt;

&lt;p&gt;课程总共7周，这篇整理的第一周的 &lt;code&gt;HDFS&lt;/code&gt; 和 &lt;code&gt;MR&lt;/code&gt; 部分。&lt;/p&gt;

&lt;h2 id=&#34;dfs:328f502a6f9088599afe3ec330de3e9a&#34;&gt;DFS&lt;/h2&gt;

&lt;p&gt;课程开始是从分布式存储DFS讲起，介绍性的东西居多。在大规模的集群中，硬件故障是个很容易发生的问题。解决方案要么就是堆大把的钱上NB的机器，要么就是多做备份。这里的DFS就是后一种解决方案。&lt;/p&gt;

&lt;p&gt;现在较为通用的分布式架构就是使用不那么NB的Linux机器组建Cluster，然后用不那么NB的网络把它们连接起来。而在分布式存储的情况下，把数据在不同节点之间复制是需要时间的，所以让程序去找数据就是一个比较正确的解决方案了。&lt;/p&gt;

&lt;p&gt;DFS中的节点有两类，&lt;code&gt;Chunk Server&lt;/code&gt;用来存放数据，&lt;code&gt;Master Node&lt;/code&gt;用来存放文件和&lt;code&gt;Chunk Server&lt;/code&gt;的对应关系。一个文件会被分成多处连续的&lt;code&gt;chunks&lt;/code&gt;，典型的大小是16~64MB。每一个&lt;code&gt;chunk&lt;/code&gt;都会复制2到3份，分别存储在不同的机器上（最好是在不同rack上）。而&lt;code&gt;Master Node&lt;/code&gt;就会存储这些机器和文件的映射关系。当需要查找这个文件时，先从&lt;code&gt;Master Node&lt;/code&gt;处取到&lt;code&gt;Chunk Server&lt;/code&gt;的信息，再从相应的server上获取文件内容。&lt;/p&gt;

&lt;h2 id=&#34;map-reduce:328f502a6f9088599afe3ec330de3e9a&#34;&gt;Map Reduce&lt;/h2&gt;

&lt;p&gt;MR是一种编程模型，典型的应用场景就是 &lt;strong&gt;Word Cound&lt;/strong&gt; 。将MR应用到 WordCount 的先决条件为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;文件过大，不能完整的放到内存中去&lt;/li&gt;
&lt;li&gt;但是所有的&lt;code&gt;&amp;lt;key, value&amp;gt;&lt;/code&gt;对可以完全放到内存中&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;算法流程:328f502a6f9088599afe3ec330de3e9a&#34;&gt;算法流程&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;将数据分块读入&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Map&lt;/code&gt; ：创建&lt;code&gt;&amp;lt;k, v&amp;gt;&lt;/code&gt;数据对。在本例中，即创建出一系列的&lt;code&gt;&amp;lt;word, 1&amp;gt;&lt;/code&gt;对。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Group by Key&lt;/code&gt; ：对&lt;code&gt;&amp;lt;k, v&amp;gt;&lt;/code&gt;进行排序。本例中即将同样key的&lt;code&gt;&amp;lt;key, value&amp;gt;&lt;/code&gt;对放在一起。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Reduce&lt;/code&gt; ：将同一个&lt;code&gt;key&lt;/code&gt;的&lt;code&gt;&amp;lt;k, v&amp;gt;&lt;/code&gt;对合并到一起，并对&lt;code&gt;v&lt;/code&gt;做相应处理，在本例中，就是把所有的值相加。&lt;/li&gt;
&lt;li&gt;输出结果&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在具体实现上，程序需要指定&lt;code&gt;Map&lt;/code&gt;和&lt;code&gt;Reduce&lt;/code&gt;两个方法，这两个方法的参数都是&lt;code&gt;&amp;lt;key, value&amp;gt;&lt;/code&gt;对。例如&lt;code&gt;Map&lt;/code&gt;函数的&lt;code&gt;key&lt;/code&gt;可以是文件名，&lt;code&gt;value&lt;/code&gt;是对应的某一行或者某一块文字。而在&lt;code&gt;Reduce&lt;/code&gt;函数中，输入的&lt;code&gt;key&lt;/code&gt;是某个单词，而&lt;code&gt;value&lt;/code&gt;是1。&lt;code&gt;Reduce&lt;/code&gt;负责归并结果，并输出。&lt;/p&gt;

&lt;h3 id=&#34;map-reduce的环境:328f502a6f9088599afe3ec330de3e9a&#34;&gt;Map Reduce的环境&lt;/h3&gt;

&lt;p&gt;除了根据逻辑实现&lt;code&gt;Map&lt;/code&gt;和&lt;code&gt;Reduce&lt;/code&gt;两个函数外，还需要一个运行Map Reduce的环境，它需要提供下面几项功能：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;把输入数据分块&lt;/li&gt;
&lt;li&gt;在机器前调度程序运行&lt;/li&gt;
&lt;li&gt;处理 &lt;strong&gt;Group by key&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;处理机器故障&lt;/li&gt;
&lt;li&gt;管理机器间通信&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;出错处理:328f502a6f9088599afe3ec330de3e9a&#34;&gt;出错处理&lt;/h3&gt;

&lt;p&gt;MR出错分几种，处理方式也不同：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Map&lt;/code&gt; Worker出错：MasterNode会将此块数据标为未完成，并等待下一轮调度&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Reduce&lt;/code&gt; Worker出错：MasterNode会将正在运行中的任务置为无效，并等待下一轮调度&lt;/li&gt;
&lt;li&gt;MasterNode：任务直接退出。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;map-和-reduce-的数目:328f502a6f9088599afe3ec330de3e9a&#34;&gt;&lt;code&gt;Map&lt;/code&gt;和&lt;code&gt;Reduce&lt;/code&gt;的数目&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;Mapper&lt;/code&gt;的数目会比节点数目多许多，这样就能保证一台节点上会被会到多个任务，即使节点出错，也只会影响到正在运行中的小块任务，对于其它被分配到此节点但是还没有运行的任务来说，可以很方便的调度到其它节点上。如果任务很大，而又有一个节点在任务运行时故障的话，就需要回滚较多的部分。而且大任务也不方便充分利用空闲的机器资源。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Reduce&lt;/code&gt;的数目没有具体要求，但是一般会比&lt;code&gt;Mapper&lt;/code&gt;的数目要少。&lt;/p&gt;

&lt;h3 id=&#34;改良:328f502a6f9088599afe3ec330de3e9a&#34;&gt;改良&lt;/h3&gt;

&lt;p&gt;还是拿WC作为例子。在&lt;code&gt;Reduce&lt;/code&gt;函数处，原始的办法需要传一堆&lt;code&gt;&amp;lt;word, 1&amp;gt;&lt;/code&gt;对到处理端，这会占用较多的带宽和传输时间，所以一个改良就是在传给&lt;code&gt;Reduce&lt;/code&gt;之前先归并一下，相当于多级&lt;code&gt;Reduce&lt;/code&gt;，而这一中间处理是在本地完成的，这样就可以减少对网络带宽的占用，以及乱序Mapper结果时的计算量。&lt;/p&gt;

&lt;p&gt;注意，此种方式并不适用所有计算，例如对多个数取平均值就不可以使用。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>