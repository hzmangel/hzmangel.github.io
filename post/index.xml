<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 湖间小筑</title>
    <link>http://hzmangel.github.io/post/</link>
    <description>Recent content in Posts on 湖间小筑</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>Copyright - ©2015 - hzmangel</copyright>
    <lastBuildDate>Mon, 07 Dec 2015 21:15:45 +0800</lastBuildDate>
    <atom:link href="http://hzmangel.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Go generate study
</title>
      <link>http://hzmangel.github.io/post/go-generate-struct-to-schema/</link>
      <pubDate>Mon, 07 Dec 2015 21:15:45 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/go-generate-struct-to-schema/</guid>
      <description>

&lt;p&gt;最近在折腾用Golang弄DB，定义完了 &lt;code&gt;struct&lt;/code&gt; 后发现好像没有 ORM 可以把这个 &lt;code&gt;struct&lt;/code&gt; 给映射到某张表上，所以需要：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;手动创建表结构，包括折腾表名和数据结构&lt;/li&gt;
&lt;li&gt;同步代码中的字段和表结构&lt;/li&gt;
&lt;li&gt;如果要换DB还得再来一次&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;于是，开始找有没有像 Rails 中一样的生成器。&lt;/p&gt;

&lt;p&gt;最开始是想用一个脚本解析struct，生成相应的代码，但是在查文档的时候发现golang在1.4版本中引入了 &lt;a href=&#34;https://golang.org/doc/go1.4#gogenerate&#34;&gt;generate&lt;/a&gt; 命令，它可以解析一个文件并生成另一个文件。官方给的说明里面是从yacc的语法生成go文件。想了想觉得它可以完成生成SQL命令，同步struct和SQL的功能，而且如果做了处理的话还可以方便的在DB间切换，那就试试吧。&lt;/p&gt;

&lt;h2 id=&#34;语法:ccc7399e1d95744ff35922b65c36cc73&#34;&gt;语法&lt;/h2&gt;

&lt;p&gt;在介绍如何编写前，先介绍下调用的方式。要成功调用一次generate需要两个条件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;已经安装的generator&lt;/li&gt;
&lt;li&gt;在需要处理的代码中加上 &lt;code&gt;//go:generate&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样在文件所在的目录下运行 &lt;code&gt;go generate&lt;/code&gt; 即可根据注释中所指示的方法，调用相应的生成器了。&lt;/p&gt;

&lt;h2 id=&#34;编写:ccc7399e1d95744ff35922b65c36cc73&#34;&gt;编写&lt;/h2&gt;

&lt;p&gt;Golang给出的sample是一个字符串相关的东西，具体可以见 &lt;a href=&#34;https://godoc.org/golang.org/x/tools/cmd/stringer&#34;&gt;文档页&lt;/a&gt;。&lt;a href=&#34;http://www.onebigfluke.com/2014/12/generic-programming-go-generate.html&#34;&gt;另一个页面&lt;/a&gt; 也有一份具体的实现可以参考。它主要的思路就是用 &lt;code&gt;generate&lt;/code&gt; 去处理某个 &lt;code&gt;.go&lt;/code&gt; 文件，然后生成一些新的东西。主要是用内置的 &lt;em&gt;AST&lt;/em&gt; 解析文件，并生成相应的东西。&lt;/p&gt;

&lt;h2 id=&#34;我的实现:ccc7399e1d95744ff35922b65c36cc73&#34;&gt;我的实现&lt;/h2&gt;

&lt;p&gt;回到最开始的问题，我有一个定义好的struct，需要去生成SQL命令。所以还是需要用到内置的AST解析器的。除此之外也就是一点点的区分类型型，做转换了。代码放在 &lt;a href=&#34;https://github.com/hzmangel/struct2schema&#34;&gt;https://github.com/hzmangel/struct2schema&lt;/a&gt; 这里，README还在编写中。目前代码支持将大部分Go内置的类型转换为SQL类型，不过可能还是有一些不支持的需要手动操作。在DB支持上，目前只看了sqlite3和MySQL。下一步要做的事情估计有这些：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持更多的DB类型，如PostgreSQL， MSSQL 等（Mongo这种无结构的就不需要了&amp;hellip;&amp;hellip;）&lt;/li&gt;
&lt;li&gt;现在生成的SQL命令中，字段名和Go中的变量名一致，都是Camel String，拟在解析的时候将其变为下划线小写的方式来生成SQL命令（考虑从后面的json字段获取小写名）&lt;/li&gt;
&lt;li&gt;支持更多的Go数据类型&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前生成出来的代码会直接打印到屏幕上，格式啥的，据说1.6上对模板会有一个更新， &lt;a href=&#34;https://github.com/golang/go/issues/9969&#34;&gt;讨论在此&lt;/a&gt; ，拭目以待。&lt;/p&gt;

&lt;h2 id=&#34;后记:ccc7399e1d95744ff35922b65c36cc73&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;在写这篇文章的时候，突然反应过来这个 &lt;code&gt;go generate&lt;/code&gt; 就是会调用在注释中写的命令，根据给定的参数处理。所以理论上说可以给定任何文件，所以还是有不少可玩性的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W3 - Communities in Social Network (Basic)
</title>
      <link>http://hzmangel.github.io/post/mmds-w3-communities-in-social-networks-basic/</link>
      <pubDate>Wed, 11 Nov 2015 23:53:33 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w3-communities-in-social-networks-basic/</guid>
      <description>

&lt;p&gt;第三周分两部分，第一部分是 &lt;em&gt;Communities in Social Network&lt;/em&gt; 。是介绍如何在社交网络中给用户分组的。这一部分的课也分为基础和高级，这一篇是基础， 高级的课程另开一篇吧（主要是基础中还有些东西没完全弄明白&amp;hellip;）。&lt;/p&gt;

&lt;p&gt;社交网络包含有用户和用户之间的联系。把用户看成顶点，把用户之间的联系看成边，就可以得到一个图 (&lt;em&gt;social graph&lt;/em&gt;)。像 Facebook 中的图就是无向图，而 Twitter/G+ 中的就是有向图。在 &lt;em&gt;Network&lt;/em&gt; 和 &lt;em&gt;Communities&lt;/em&gt; 这块，主要的任务有两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;给定一个模型，怎么去生成网络&lt;/li&gt;
&lt;li&gt;给定一个网络，怎么找到最好的 &lt;em&gt;Communities&lt;/em&gt; 模型&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;从模型生成网络:6f5ccd4dcd8169ad30902566c7400598&#34;&gt;从模型生成网络&lt;/h2&gt;

&lt;p&gt;从模型生成网络是指定义一个模型，它接受一系列的参数，并最终生成一张网络（主要是边的生成）。&lt;/p&gt;

&lt;h3 id=&#34;agm-affiliation-graph-model:6f5ccd4dcd8169ad30902566c7400598&#34;&gt;AGM: Affiliation Graph Model&lt;/h3&gt;

&lt;h4 id=&#34;参数:6f5ccd4dcd8169ad30902566c7400598&#34;&gt;参数&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;$V$&lt;/code&gt;: 节点&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$C$&lt;/code&gt;: &lt;em&gt;Community&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$M$&lt;/code&gt;: 节点与 &lt;em&gt;Community&lt;/em&gt; 之间的关系&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$P_{c}$&lt;/code&gt;: 某个 &lt;em&gt;Community&lt;/em&gt; 的联通率，即都属于某个 &lt;em&gt;Community&lt;/em&gt; 的节点之间联通的概率。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;生成过程:6f5ccd4dcd8169ad30902566c7400598&#34;&gt;生成过程&lt;/h4&gt;

&lt;p&gt;遍历每个 &lt;em&gt;Community&lt;/em&gt; &lt;code&gt;$A$&lt;/code&gt; 中的节点对，以 &lt;code&gt;$P_{A}$&lt;/code&gt; 的概率连接它们（边的生成）。&lt;/p&gt;

&lt;p&gt;任意两点间的连接概率为&lt;code&gt;$P(u,v)=1-\prod_{c \in M_{u} \bigcap M_{v}}(1-p_{c}) $&lt;/code&gt; 。其中，&lt;code&gt;$M_{u}$&lt;/code&gt; 表示包含有节点 &lt;code&gt;$u$&lt;/code&gt; 的 &lt;em&gt;Community&lt;/em&gt; 集合。如果 &lt;code&gt;$u$&lt;/code&gt; 和 &lt;code&gt;$v$&lt;/code&gt; 没有共用的 &lt;em&gt;Community&lt;/em&gt; ，则概率 &lt;code&gt;$P(u,v) = \epsilon $&lt;/code&gt; 。&lt;/p&gt;

&lt;h4 id=&#34;适应性强:6f5ccd4dcd8169ad30902566c7400598&#34;&gt;适应性强&lt;/h4&gt;

&lt;p&gt;可以用于生成多种 &lt;em&gt;Community&lt;/em&gt; ：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;无交集 (Non-overlapping)&lt;/li&gt;
&lt;li&gt;有交集 (Overlapping)&lt;/li&gt;
&lt;li&gt;内嵌 (Nested)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;从网络生成模型:6f5ccd4dcd8169ad30902566c7400598&#34;&gt;从网络生成模型&lt;/h2&gt;

&lt;h3 id=&#34;agm-affiliation-graph-model-1:6f5ccd4dcd8169ad30902566c7400598&#34;&gt;AGM: Affiliation Graph Model&lt;/h3&gt;

&lt;p&gt;这个是mmds书本&lt;a href=&#34;http://www.mmds.org/mmds/v2.1/ch10-graphs2.pdf&#34;&gt;配套幻灯片&lt;/a&gt;上的内容，占坑。&lt;/p&gt;

&lt;h3 id=&#34;bigclam:6f5ccd4dcd8169ad30902566c7400598&#34;&gt;BigCLAM&lt;/h3&gt;

&lt;h4 id=&#34;membership-strength:6f5ccd4dcd8169ad30902566c7400598&#34;&gt;Membership Strength&lt;/h4&gt;

&lt;p&gt;引入一个新概念，某个节点 &lt;code&gt;$u$&lt;/code&gt; 对某个 &lt;em&gt;Community&lt;/em&gt; 的 &lt;em&gt;membership strength&lt;/em&gt; ，记为 &lt;code&gt;$F_{u,A}$&lt;/code&gt;，同时定义如果此值为0，则表明节点不在 &lt;em&gt;Community&lt;/em&gt; 中。所以在某个 &lt;em&gt;Community&lt;/em&gt; 中，两个节点的联通概率为 &lt;code&gt;$P_{A}(u,v)=1-\exp (-F_{uA} \cdot F_{vA})$&lt;/code&gt; 。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;此处不是很明白为什么同 &lt;em&gt;Community&lt;/em&gt; 中节点的连通概率可以写成上面的方式，需要去书中相应章节找答案。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面介绍的是在同一个 &lt;em&gt;Community&lt;/em&gt; 中两个节点的联通概率，现在将要说的是在不同的 &lt;em&gt;Community&lt;/em&gt; 中如何确定联通概率。首先需要定义一个 &lt;em&gt;Community membership strength matrix&lt;/em&gt; ，它的列是 &lt;em&gt;Community&lt;/em&gt; ，行是节点。将矩阵记为 &lt;code&gt;$F$&lt;/code&gt;，某个位置 &lt;code&gt;$F_{vA}$&lt;/code&gt; 的值表示的是节点在这个 &lt;em&gt;Community&lt;/em&gt; 中的 &lt;em&gt;strength&lt;/em&gt; ，所以任意一行 &lt;code&gt;$F_{u}$&lt;/code&gt; 则表示的是节点在多个 &lt;em&gt;Community&lt;/em&gt; 中的 &lt;em&gt;membership strength&lt;/em&gt; 向量。&lt;/p&gt;

&lt;p&gt;上面的公式计算的是两个节点在单 &lt;em&gt;Community&lt;/em&gt; 中的联通概率，两个节点之间存在至少相同的 &lt;em&gt;Community&lt;/em&gt; ，那么它们的联通概率即为 &lt;code&gt;$P(u,v)=1-\prod_{C}(1-P_{C}(u,v))$&lt;/code&gt; ，经过和上面的公式整合，简化，可以得到 &lt;code&gt;$P(u,v)=1-\exp (-F_{v} \cdot F_{v}^{T} )$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;所以问题就从如何从一个网络生成 &lt;em&gt;Community&lt;/em&gt; 变成了如何从给定的网络中找到可以最大化 &lt;em&gt;likelihood&lt;/em&gt; 的矩阵 &lt;code&gt;$F$&lt;/code&gt; ，即 &lt;code&gt;$argmax_{F} \prod_{u,v \in E} p(u,v) \prod_{(u,v \notin E)}(1-p(u,v))$&lt;/code&gt; 。通常这个 &lt;em&gt;likelihood&lt;/em&gt; 会使用对数表示，记为 &lt;code&gt;$l(F)=\log P(G|F)$&lt;/code&gt; 。最后问题就演变为找到可以使 &lt;code&gt;$l(F)$&lt;/code&gt; 最大化的 &lt;code&gt;$F$&lt;/code&gt;。而 &lt;code&gt;$$l(F) = \sum_{(u,v) \in E} \log (1-\exp (-F_{u}F_{v}^{T})) - \sum_{(u,v) \notin E}(F_{u}F_{v}^{T})$$&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;计算:6f5ccd4dcd8169ad30902566c7400598&#34;&gt;计算&lt;/h4&gt;

&lt;p&gt;考虑到需要求 &lt;em&gt;likelihood&lt;/em&gt; 的极大值，所以考虑使用导数（ &lt;em&gt;gradient&lt;/em&gt; ）来计算。将上式中的&lt;code&gt;$F_{u}$&lt;/code&gt;看作变量，即某节点 &lt;code&gt;$u$&lt;/code&gt; 的最大 &lt;em&gt;likelihood&lt;/em&gt; 值，则对上面公式的求导后可变为&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$ \nabla l(F_{u})=\sum_{v \in \mathcal{N}(u)} F_{v} \frac{\exp (-F_{u}F_{v}^{T})}{1- \exp (-F_{u}F_{v}^{T})} - \sum_{v \notin \mathcal{N}(u)} F_{v} $$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;其中，&lt;code&gt;$\mathcal{N}(u)$&lt;/code&gt;表示节点 &lt;code&gt;$u$&lt;/code&gt; 的邻接节点。&lt;/p&gt;

&lt;p&gt;这个公式的问题在于，后面的一个求和操作是线性时间的，而且是和所有节点数目相关，这样会拖慢运算速度。式子的后一项是计算所有不在节点 &lt;code&gt;$u$&lt;/code&gt; 邻接节点中的节点的&lt;code&gt;$F$&lt;/code&gt;值的和，它可以替换为 &lt;code&gt;$\sum_{v} F_{v} - F_{u} - \sum_{v \in \mathcal{N}(u) F_{v}}$&lt;/code&gt; ，而这个式子的第一项 &lt;code&gt;$\sum_{v} F_{v}$&lt;/code&gt; 是可以预先计算得到的，而 &lt;code&gt;$\sum_{v \in \mathcal{N}(u) F_{v}}$&lt;/code&gt; 虽然也是线性时间，但是只和 &lt;code&gt;$u$&lt;/code&gt; 的邻接节点数目想关，这个数值在真实网络中远小于整个网络的节点数的。所以在速度上有很大的改进。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;有点不明白，&lt;code&gt;$\sum_{v} F_{v}$&lt;/code&gt; 在开始的时候是怎么可以事先计算的，要求的不就是某个 &lt;code&gt;$F$&lt;/code&gt; 吗。后期实现的时候再看如何处理吧。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;其它阅读-graph-and-social-network:6f5ccd4dcd8169ad30902566c7400598&#34;&gt;其它阅读: Graph and Social Network&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：此处的内容并没有在课程讲义中，而对应书本的 &lt;em&gt;10.1.2 Social Networks as Graphs&lt;/em&gt; 节。主要是觉得在课程上说的有些东西不是很清楚来龙去脉，所以去书本上找一找，就看到了这个东西。目前还有一些问题不是很明了，也一起列在最后。&lt;/p&gt;

&lt;p&gt;一般认为，一个 &lt;em&gt;SN&lt;/em&gt; 可以看成一张图，但这并不是表示任意一个 &lt;em&gt;graph&lt;/em&gt; ，就能表示一个 &lt;em&gt;social network&lt;/em&gt; ，这个是由这张图的 &lt;em&gt;locality of relationships&lt;/em&gt; 确定的。&lt;/p&gt;

&lt;p&gt;检查一个 &lt;em&gt;graph&lt;/em&gt; 是不是 &lt;em&gt;SN&lt;/em&gt; ，则需要计算这张图的 &lt;em&gt;locality of relationships&lt;/em&gt; （找不到这个词怎么翻译，先原样放这吧）。给定一个有 &lt;code&gt;$N$&lt;/code&gt; 节点， &lt;code&gt;$E$&lt;/code&gt; 条边的图，它的联通率是指，有三个点&lt;code&gt;$X$&lt;/code&gt;，&lt;code&gt;$Y$&lt;/code&gt;和&lt;code&gt;$Z$&lt;/code&gt;，在确定&lt;code&gt;$(X,Y)$&lt;/code&gt;和&lt;code&gt;$(X,Z)$&lt;/code&gt;联通的情况下，&lt;code&gt;$(Y,Z)$&lt;/code&gt;联通的概率。&lt;/p&gt;

&lt;p&gt;此概率的计算有以下几步：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;理论联通率&lt;/strong&gt;：完全图会有&lt;code&gt;K=$\binom{N}{2}$&lt;/code&gt;条边，所以理论联通率应为 &lt;code&gt;$E/K$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;单条边的联通率&lt;/strong&gt;：在大规模的图中，一般认为理论联通率就是图的联通率，但是在小规模的图中，这个数值偏差有些大，所以需要重新计算。考虑给定条件，在已经确定有两条边的情况下，计算第三条边出现的条件概率，即为&lt;code&gt;$(E-2)/(K-2)$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实际联通率&lt;/strong&gt;：实际概率的计算需要遍历图中的每个节点，假设为&lt;code&gt;$X$&lt;/code&gt;，再找到邻接节点 &lt;code&gt;$Y$&lt;/code&gt; 和 &lt;code&gt;$Z$&lt;/code&gt; ，最后计算 &lt;code&gt;$(Y,Z)$&lt;/code&gt; 出现的概率。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;书中表示，最后算出来的实际概率大于单边理论概率，所以这张图可以看成是某 &lt;em&gt;SN&lt;/em&gt; 网络（可以表示 &lt;em&gt;SN&lt;/em&gt; 网络的 &lt;em&gt;locality&lt;/em&gt; ）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;是不是说大规格图的联通率直接认为是理论联通率，所以就肯定可以表示 &lt;em&gt;SN&lt;/em&gt; 而不用再计算？&lt;/li&gt;
&lt;li&gt;如果计算出来的联通率比单边联通率还要低，那是不是就说明这个图不能表示 &lt;em&gt;SN&lt;/em&gt; ？&lt;/li&gt;
&lt;li&gt;是不是说实际联通率只要超过单边联通率就可以看成是 &lt;em&gt;SN&lt;/em&gt; ？还是说需要超过某个阈值才可以算？&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W2 - Frequent Itemsets - Part II
</title>
      <link>http://hzmangel.github.io/post/mmds-w2-frequent-itemsets-p2/</link>
      <pubDate>Wed, 01 Jul 2015 18:20:21 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w2-frequent-itemsets-p2/</guid>
      <description>

&lt;p&gt;这一部分介绍 &lt;em&gt;A-Priori&lt;/em&gt; 算法。&lt;/p&gt;

&lt;p&gt;前一篇中所说的两种计算数据对次数的办法，都是 &lt;em&gt;one-pass&lt;/em&gt; 的，程序在读入数据的同时，生成数据对，然后在数据对对应的计数上加1。这样数据读取完成后，直接查找计数的数即可得知结果。但是这个方法在在数据量过大时会超过主存的大小，从而在计算时引发换页，降低效率。而本篇要介绍的 &lt;em&gt;A-Priori&lt;/em&gt; 算法，通过减少需要计算的数据对个数，从而减少对内存的需求。&lt;/p&gt;

&lt;p&gt;这个算法的核心思想是 &lt;em&gt;monotonicity&lt;/em&gt; ，即如果 &lt;code&gt;{I}&lt;/code&gt; 是 &lt;em&gt;frequent itemset&lt;/em&gt; ，那么它的所有子集都是 &lt;em&gt;frequent itemset&lt;/em&gt; 。反过来说，如果一个元素不是 &lt;em&gt;frequent itemset&lt;/em&gt; ，那么任何包含它的集合都不可能是。&lt;/p&gt;

&lt;h2 id=&#34;基本算法:f42a6776e590235dbcf00cc877498b66&#34;&gt;基本算法&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;A-Priori&lt;/em&gt; 是一个 &lt;em&gt;two-pass&lt;/em&gt; 算法，&lt;/p&gt;

&lt;h3 id=&#34;first-pass:f42a6776e590235dbcf00cc877498b66&#34;&gt;First Pass&lt;/h3&gt;

&lt;p&gt;在读取的过程中创建两张Hash表，一张表是将所有 &lt;code&gt;item&lt;/code&gt; 映射为 &lt;code&gt;1&lt;/code&gt; 到 &lt;code&gt;n&lt;/code&gt; 的索引，减少后面引用时占用的内存。第二张表是将 &lt;code&gt;item&lt;/code&gt; 的索引与 &lt;code&gt;item&lt;/code&gt; 出现的次数对应起来。&lt;/p&gt;

&lt;h3 id=&#34;between-passes:f42a6776e590235dbcf00cc877498b66&#34;&gt;Between Passes&lt;/h3&gt;

&lt;p&gt;检查索引和出现次数映射的表，找出其中所有的 &lt;em&gt;frequent itemset&lt;/em&gt; ，并将它们重新索引为 &lt;code&gt;1&lt;/code&gt; 到 &lt;code&gt;m&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;second-pass:f42a6776e590235dbcf00cc877498b66&#34;&gt;Second Pass&lt;/h3&gt;

&lt;p&gt;计算所有使用 &lt;em&gt;frequent itemset&lt;/em&gt; 表中元素组成的数据对的 &lt;em&gt;support&lt;/em&gt; 值。具体流程如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;对每一个 &lt;em&gt;basket&lt;/em&gt; ，找出其中属于 &lt;em&gt;frequent itemset&lt;/em&gt; 表中的元素。&lt;/li&gt;
&lt;li&gt;为找到的元素构建数据对。&lt;/li&gt;
&lt;li&gt;找到数据对对应的计数值，加上它出现的次数。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;最后，只需要检查第二次生成的数据就好。另外，如果需要求多于2个元素的 &lt;em&gt;itemset&lt;/em&gt; ，可以按上述办法将算法级联计算。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;基本算法介绍完成，下面就是改进了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Mutation: it is the key to our evolution.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;pcy:f42a6776e590235dbcf00cc877498b66&#34;&gt;PCY&lt;/h2&gt;

&lt;p&gt;在 &lt;em&gt;first pass&lt;/em&gt; 的时候，内存中有很多空闲的地方，所以在给 &lt;em&gt;item&lt;/em&gt; 计数的同时，生成出所有的 &lt;em&gt;pair&lt;/em&gt; ，取 Hash 后，给对应的 &lt;em&gt;bucket&lt;/em&gt; 加1 。在 &lt;em&gt;second pass&lt;/em&gt; 的时候，只处理如下的数据对：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;i&lt;/code&gt; 和 &lt;code&gt;j&lt;/code&gt; 都是 &lt;em&gt;frequent items&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{i, j}&lt;/code&gt; 被 Hash 的 &lt;em&gt;bucket&lt;/em&gt; 是 &lt;em&gt;frequent bucket&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这样 &lt;em&gt;second pass&lt;/em&gt; 中需要处理的数据又能少一些了。&lt;/p&gt;

&lt;p&gt;这个算法的原理是，如果一个 &lt;em&gt;pair&lt;/em&gt; 是 &lt;em&gt;frequent pair&lt;/em&gt; ，那么它对应的 &lt;em&gt;bucket&lt;/em&gt; 一定超过 &lt;em&gt;support&lt;/em&gt; 的阈值。而如果一个 &lt;em&gt;bucket&lt;/em&gt; 没有超过阈值，那么里面的所有 &lt;em&gt;pair&lt;/em&gt; 一定不是 &lt;em&gt;frequent pair&lt;/em&gt; 。 注意：一个 &lt;em&gt;bucket&lt;/em&gt; 是 &lt;em&gt;frequent bucket&lt;/em&gt; 并不能保证其中的 &lt;em&gt;pair&lt;/em&gt; 都是 &lt;em&gt;frequent pair&lt;/em&gt; 。&lt;/p&gt;

&lt;h3 id=&#34;first-pass-1:f42a6776e590235dbcf00cc877498b66&#34;&gt;First Pass&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;for b in buckets
  for item in b:
    count[item]++
  end

  for pair in b:
    bucket(hash(pair))++
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;between-passes-1:f42a6776e590235dbcf00cc877498b66&#34;&gt;Between Passes&lt;/h3&gt;

&lt;p&gt;除了找出 &lt;em&gt;frequent item&lt;/em&gt; 外，还会把上一步中的 &lt;em&gt;hash bucket&lt;/em&gt; 替换为 &lt;em&gt;bitmap&lt;/em&gt; ，&lt;code&gt;1&lt;/code&gt; 表示 &lt;em&gt;frequent bucket&lt;/em&gt; ， &lt;code&gt;0&lt;/code&gt; 表示不是。这样可以进一步减少内存空间的占用（从 &lt;code&gt;32bit&lt;/code&gt; 降到 &lt;code&gt;1bit&lt;/code&gt;）。&lt;/p&gt;

&lt;h3 id=&#34;second-pass-1:f42a6776e590235dbcf00cc877498b66&#34;&gt;Second Pass&lt;/h3&gt;

&lt;p&gt;根据上面列出的条件，找到 &lt;em&gt;canidate pair&lt;/em&gt; ，计算。&lt;/p&gt;

&lt;p&gt;PCY算法还有两个变种：&lt;/p&gt;

&lt;h2 id=&#34;multistage:f42a6776e590235dbcf00cc877498b66&#34;&gt;Multistage&lt;/h2&gt;

&lt;p&gt;这个算法的核心在于，在PCY算法的 &lt;em&gt;first pass&lt;/em&gt; 后，并不开始对 &lt;em&gt;bucket&lt;/em&gt; 做筛选，而是将其中属于 &lt;em&gt;frequent bucket&lt;/em&gt; 的 &lt;em&gt;pair&lt;/em&gt; 挑出来，再做一次 Hash ，从而进一步减少 &lt;em&gt;second pass&lt;/em&gt; 中需要处理的数据量。从定义上来看，这个算法可能会有多个 &lt;em&gt;pass&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;multihash:f42a6776e590235dbcf00cc877498b66&#34;&gt;Multihash&lt;/h2&gt;

&lt;p&gt;在 &lt;em&gt;first pass&lt;/em&gt; 中，使用多个独立的Hash函数来计算 &lt;em&gt;bucket&lt;/em&gt; 。相比物 Multistage ，它还是 &lt;em&gt;two pass&lt;/em&gt; 的算法，但是它的风险在于在多个 Hash 的作用下，可能会有更多的 &lt;em&gt;frequent bucket&lt;/em&gt; 。如果它能保证 &lt;em&gt;frequent bucket&lt;/em&gt; 的数目足够小，那么我们就能在 &lt;em&gt;two pass&lt;/em&gt; 获得 Multistage 的优点。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;算法的另一个发展趋势是获取到大部分的 &lt;em&gt;frequent itemsets&lt;/em&gt; ，使用降低精确性的办法来换取效率。&lt;/p&gt;

&lt;h2 id=&#34;基本想法:f42a6776e590235dbcf00cc877498b66&#34;&gt;基本想法&lt;/h2&gt;

&lt;p&gt;对样本 &lt;em&gt;basket&lt;/em&gt; 进行随机抽样，并在内存中对样本进行 &lt;em&gt;A-Priori&lt;/em&gt; 或其改进版本的计算，此处需要计算所有的 &lt;em&gt;frequent itemsets&lt;/em&gt; ，而不仅仅是 &lt;em&gt;pair&lt;/em&gt; 。计算时使用的 &lt;em&gt;support threshold&lt;/em&gt; 需要根据样本容量处理，例如抽样率为 &lt;code&gt;1/100&lt;/code&gt; ，那么阈值也相应的变成 &lt;code&gt;s/100&lt;/code&gt; 。在 &lt;em&gt;second pass&lt;/em&gt; 的时候，可以使用整体数据来验证是否找到的真的是 &lt;em&gt;frequent pair&lt;/em&gt; 。&lt;/p&gt;

&lt;p&gt;这个算法有个问题有些 &lt;em&gt;set&lt;/em&gt; 可能在样本中不是 &lt;em&gt;frequent&lt;/em&gt; 但在全局中是，这里有一个稍微改进的办法是适当降低样本计算中的 &lt;em&gt;support&lt;/em&gt; 阈值，例如从 &lt;code&gt;s/100&lt;/code&gt; 降为 &lt;code&gt;s/125&lt;/code&gt; ，不过这会需要更大的内存空间。（而且还是有可能会漏掉的吧&amp;hellip;&amp;hellip;）&lt;/p&gt;

&lt;h2 id=&#34;son-算法:f42a6776e590235dbcf00cc877498b66&#34;&gt;SON 算法&lt;/h2&gt;

&lt;p&gt;这个算法是将大的数据分成小集合分别读入内存进行计算，并将在任意一次计算中得到的 &lt;em&gt;frequent pair&lt;/em&gt; 置于候选的位置。而在 &lt;em&gt;second pass&lt;/em&gt; ，计算这些候选 &lt;em&gt;pair&lt;/em&gt; 并得出最后结果。这个算法的核心是，任何一个 &lt;em&gt;pair&lt;/em&gt; 如果是 &lt;em&gt;frequent pair&lt;/em&gt; ，那么它一定会在某个子集中是 &lt;em&gt;frequent pair&lt;/em&gt; 。&lt;/p&gt;

&lt;p&gt;这个算法可以让计算并行化，以提高处理效率。&lt;/p&gt;

&lt;h2 id=&#34;toivonen-算法:f42a6776e590235dbcf00cc877498b66&#34;&gt;Toivonen 算法&lt;/h2&gt;

&lt;p&gt;在 &lt;em&gt;first pass&lt;/em&gt; 中，使用子集基本算法一样的方法，抽样并计算，但是在取阈值的时候设置的稍低，例如 &lt;code&gt;s/125&lt;/code&gt;，以尽可能的不要遗漏在全集中是 &lt;em&gt;frequent set&lt;/em&gt; 的数据。此后设置一个 &lt;em&gt;negative border&lt;/em&gt; 区间，以存放所有子集是 &lt;em&gt;frequent set&lt;/em&gt; 但是本身不是的集合。在 &lt;em&gt;second pass&lt;/em&gt; 的时候，计算所有候选 &lt;em&gt;frequent itemsets&lt;/em&gt; 和 &lt;em&gt;negative border&lt;/em&gt; 中的数据。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果 &lt;em&gt;negative border&lt;/em&gt; 中没有数据是 &lt;em&gt;frequent itemsets&lt;/em&gt; ，那么所得就是所需的 &lt;em&gt;frequent itemsets&lt;/em&gt; .&lt;/li&gt;
&lt;li&gt;如果在 &lt;em&gt;negative border&lt;/em&gt; 中找到了 &lt;em&gt;frequent itesmsets&lt;/em&gt; ，那么就需要重新取样进行计算，直到满足前一条的条件为止。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;证明：假设 &lt;code&gt;S&lt;/code&gt; 是一个在全集中的 &lt;em&gt;frequent itemset&lt;/em&gt; ，但是在抽样样本中，它既不属于 &lt;em&gt;frequent itemsets&lt;/em&gt; 也不是 &lt;em&gt;negative border&lt;/em&gt; 。那么可以找到一个 &lt;code&gt;T&lt;/code&gt; ，它是在样本中非 &lt;em&gt;frequent&lt;/em&gt; 的 &lt;code&gt;S&lt;/code&gt; 最小子集。即比 &lt;code&gt;T&lt;/code&gt; 小的子集在样本中都是 &lt;em&gt;frequent itemset&lt;/em&gt; ，那根据 &lt;em&gt;negative border&lt;/em&gt; 的定义，&lt;code&gt;T&lt;/code&gt; 一定会在 &lt;em&gt;negative border&lt;/em&gt; 中，否则它就不是是最小子集。由于空集总是 &lt;em&gt;frequent itemset&lt;/em&gt; ，所以 &lt;code&gt;T&lt;/code&gt; 一定存在。这样就有一个 &lt;code&gt;T&lt;/code&gt; ，它在全集中是 &lt;em&gt;frequent itemset&lt;/em&gt; ，在样本中是 &lt;em&gt;negative border&lt;/em&gt; ，于是，重新抽样吧。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W2 - Frequent Itemsets - Part I
</title>
      <link>http://hzmangel.github.io/post/mmds-w2-frequent-itemsets-p1/</link>
      <pubDate>Fri, 26 Jun 2015 19:35:21 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w2-frequent-itemsets-p1/</guid>
      <description>

&lt;p&gt;第二周的最后一块内容是 &lt;em&gt;Frequent Itemsets&lt;/em&gt; 。主要介绍了 &lt;em&gt;Frequent Itemsets&lt;/em&gt; ， &lt;em&gt;Association Rule&lt;/em&gt; 以及算法。这一部分介绍前面的，后面一篇会介绍算法和优化。&lt;/p&gt;

&lt;h2 id=&#34;market-basket-模型:5f0cf4b257bf00ac7ad2347da74e001b&#34;&gt;&lt;em&gt;Market-Basket&lt;/em&gt; 模型&lt;/h2&gt;

&lt;p&gt;这个模型一般用来描述两种数据之间的 &lt;em&gt;m2m&lt;/em&gt; 关系。其中的一个数据是 &lt;em&gt;item&lt;/em&gt; ，而另一个是 &lt;em&gt;baskets&lt;/em&gt; 。每一个 &lt;em&gt;basket&lt;/em&gt; 中包含有多个 &lt;em&gt;items&lt;/em&gt; ，即为 &lt;em&gt;itemset&lt;/em&gt; 。 通常认为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;basket&lt;/em&gt; 中的 &lt;em&gt;items&lt;/em&gt; 数量较少，远小于整体 &lt;em&gt;items&lt;/em&gt; 的数量。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;basket&lt;/em&gt; 的数量较大，不能完全的放于内存中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这是一个模型的例子：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Market&lt;/em&gt; ：一个超级市场。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Item&lt;/em&gt; ：市场中售卖的所有东西。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Basket&lt;/em&gt; ：用户的订单。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;frequent-itemsets:5f0cf4b257bf00ac7ad2347da74e001b&#34;&gt;&lt;em&gt;Frequent Itemsets&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Frequent Itemsets&lt;/em&gt; 即指在 &lt;em&gt;baskets&lt;/em&gt; 中经常出现的 &lt;em&gt;itemset&lt;/em&gt; ，它的定义为：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;假设 &lt;em&gt;I&lt;/em&gt; 是一个 &lt;em&gt;itemset&lt;/em&gt; ，定义 &lt;em&gt;I&lt;/em&gt; 的 &lt;em&gt;support&lt;/em&gt; 为包含 &lt;em&gt;I&lt;/em&gt; 的 &lt;em&gt;basket&lt;/em&gt; 个数。定义数字 &lt;em&gt;s&lt;/em&gt; 为 &lt;em&gt;support threshold&lt;/em&gt; ，所有 &lt;em&gt;support&lt;/em&gt; 数超过 &lt;em&gt;s&lt;/em&gt; 的 &lt;em&gt;I&lt;/em&gt; 即为 &lt;em&gt;frequent itemset&lt;/em&gt; 。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt; ：一个 &lt;em&gt;basket&lt;/em&gt; 可能包含有多个 &lt;em&gt;itemset&lt;/em&gt; ，并不是说一个 &lt;em&gt;basket&lt;/em&gt; 中包含的就是一个 &lt;em&gt;itemset&lt;/em&gt; 。&lt;/p&gt;

&lt;p&gt;一个 &lt;em&gt;itemset&lt;/em&gt; 可以包含有不定个元素。如果一个 &lt;em&gt;itemset&lt;/em&gt; 是 &lt;em&gt;frequent itemset&lt;/em&gt; ，那么它的的任何子集都是 &lt;em&gt;frequent itemset&lt;/em&gt; 。&lt;/p&gt;

&lt;h4 id=&#34;应用场景:5f0cf4b257bf00ac7ad2347da74e001b&#34;&gt;应用场景&lt;/h4&gt;

&lt;h5 id=&#34;超市商品:5f0cf4b257bf00ac7ad2347da74e001b&#34;&gt;超市商品&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;item&lt;/em&gt; ：超市商品。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;basket&lt;/em&gt; ：用户订单，上面包含有一个或多个商品。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过分析订单中的 &lt;em&gt;frequent itemsets&lt;/em&gt; ，可以得出哪些商品会经常被同时购买。&lt;/p&gt;

&lt;h5 id=&#34;相关内容查找:5f0cf4b257bf00ac7ad2347da74e001b&#34;&gt;相关内容查找&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;item&lt;/em&gt; ：单词&lt;/li&gt;
&lt;li&gt;&lt;em&gt;basket&lt;/em&gt; ：文档，如网页，blog，tweets等。包含一系列的单词。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;找出的 &lt;em&gt;frequent itemset&lt;/em&gt; 中，肯定包含有一些 common word，除去这些词后，可以揭示出一些单词之间的关系。&lt;/p&gt;

&lt;h5 id=&#34;剽窃检测:5f0cf4b257bf00ac7ad2347da74e001b&#34;&gt;剽窃检测&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;itme&lt;/em&gt; ：文档&lt;/li&gt;
&lt;li&gt;&lt;em&gt;basket&lt;/em&gt; ：句子。如果一个文档包含这个句子，那么它在这个 &lt;em&gt;basket&lt;/em&gt; 中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;找出的 &lt;em&gt;frequent itemset&lt;/em&gt; 表示这些文档中有大量的句子相似，可以检测抄袭的方法。&lt;/p&gt;

&lt;h5 id=&#34;生物标记:5f0cf4b257bf00ac7ad2347da74e001b&#34;&gt;生物标记&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;item&lt;/em&gt; ：生物标记，如基因，血蛋白，疾病等。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;basket&lt;/em&gt; ：病人数据，如基因检测，生化指标等。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;找到的包含疾病和生物标记的 &lt;em&gt;frequent itemset&lt;/em&gt; 可以用于疾病检测。&lt;/p&gt;

&lt;h3 id=&#34;association-rule:5f0cf4b257bf00ac7ad2347da74e001b&#34;&gt;&lt;em&gt;Association Rule&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;定义 &lt;code&gt;$I \rightarrow j$&lt;/code&gt; ，表示如果一个 &lt;em&gt;basket&lt;/em&gt; 包含有 &lt;code&gt;{I}&lt;/code&gt; 中所有的元素，那么它就有可能包含 &lt;code&gt;j&lt;/code&gt; 。即 &lt;code&gt;{I}&lt;/code&gt; 是一组元素，而 &lt;code&gt;j&lt;/code&gt; 是一个元素。这个推断，就是一个 &lt;em&gt;association rule&lt;/em&gt; 。 &lt;em&gt;association rule&lt;/em&gt; 有一个属性为 &lt;em&gt;Confidence&lt;/em&gt; ，它表示给了 &lt;code&gt;{I}&lt;/code&gt; 后出现 &lt;code&gt;j&lt;/code&gt; 的概率，即在所有包含&lt;code&gt;{I}&lt;/code&gt;的 &lt;em&gt;basket&lt;/em&gt; 中同时包含&lt;code&gt;j&lt;/code&gt;的概率。&lt;/p&gt;

&lt;p&gt;现在问题来了：找到所有 &lt;em&gt;support &amp;gt;= s&lt;/em&gt; 且 &lt;em&gt;confidence &amp;gt;= c&lt;/em&gt; 的 &lt;em&gt;association rule&lt;/em&gt; 。此处的 &lt;em&gt;support&lt;/em&gt; 是指 &lt;code&gt;{I}&lt;/code&gt; 的 &lt;em&gt;support&lt;/em&gt; ，而不是 &lt;code&gt;{I, j}&lt;/code&gt; 的。&lt;/p&gt;

&lt;p&gt;计算过程的描述如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;找到所有 &lt;em&gt;support &amp;gt;= sc&lt;/em&gt; 的集合&lt;/li&gt;
&lt;li&gt;找到所有 &lt;em&gt;support &amp;gt;= s&lt;/em&gt; 的集合&lt;/li&gt;
&lt;li&gt;如果 &lt;code&gt;{I, j}&lt;/code&gt; 的 &lt;em&gt;support &amp;gt;= cs&lt;/em&gt; ，那么找到它少一个元素，而且 &lt;em&gt;support &amp;gt;= s&lt;/em&gt; 的子集。&lt;/li&gt;
&lt;li&gt;当且仅当下面条件都满足时， &lt;code&gt;$I \rightarrow j$&lt;/code&gt; 才是一个被接受的 &lt;em&gt;rule&lt;/em&gt; 。

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;{I}&lt;/code&gt; 的 &lt;em&gt;support s1 &amp;gt;= s&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{I, j}&lt;/code&gt; 的 &lt;em&gt;support s2 &amp;gt;= sc&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;s2/s1 &amp;gt;= c&lt;/em&gt; （此比值即为 &lt;em&gt;confidence&lt;/em&gt; 的定义）.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在实现上，数据通常是以普通文件方式存储于磁盘上的，存储的方式是按 &lt;em&gt;basket&lt;/em&gt; 排列。在读入数据时，即将数据切分成不同长度的集合。不难看出，磁盘IO是数据读取过程中主要的开销。在实践中，数据是多次读取的。所以在计算时，读取次数也是一个需要考虑的部分。&lt;/p&gt;

&lt;p&gt;在计算过程中，由于设置的阈值比较高，所以通常能找到的就是 &lt;em&gt;frequent pair&lt;/em&gt; ，所以这也是需要面对的最大问题。&lt;/p&gt;

&lt;h4 id=&#34;一般算法:5f0cf4b257bf00ac7ad2347da74e001b&#34;&gt;一般算法&lt;/h4&gt;

&lt;p&gt;一般的算法就是读一次文件，在内存中计算出所有 &lt;em&gt;pair&lt;/em&gt; 出现的次数。对于一个有 &lt;em&gt;n&lt;/em&gt; 个元素的 &lt;em&gt;basket&lt;/em&gt; ，最后会生成 &lt;em&gt;n(n-1)/2&lt;/em&gt; 个 &lt;em&gt;pair&lt;/em&gt; 。所以如果 &lt;em&gt;n**2&lt;/em&gt; 的大小超过内存，这个算法就会失败。&lt;/p&gt;

&lt;p&gt;在计算上，有两种方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;计算所有数据对出现的次数，并存放于三角矩阵中。&lt;/li&gt;
&lt;li&gt;使用类似稀疏矩阵的存储方法，使用坐标+次数的方式存储。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;第一种方式每个数据对需要 &lt;code&gt;4&lt;/code&gt; 个字节（假设一个整型数使用4个字节）。第二种方式每对需要12字节（坐标及计数），但是只有存在的数据对才会占用空间。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W2 - Nearest Neighbor Learning
</title>
      <link>http://hzmangel.github.io/post/mmds-w2-nearest-neighbor-learning/</link>
      <pubDate>Wed, 24 Jun 2015 02:20:33 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w2-nearest-neighbor-learning/</guid>
      <description>

&lt;p&gt;第二周的 &lt;em&gt;Nearest Neighbor Learning&lt;/em&gt; 只是一个大概的介绍。这是一个通过在训练集中找到离待查询数据最近的点从而做出预测的方法。&lt;/p&gt;

&lt;h2 id=&#34;supervised-learning:e415793b7fb7c6a9af71ea301a4a0120&#34;&gt;Supervised Learning&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Supervised learning&lt;/em&gt; 是机器学习中的一种方法（还有两种分别是 &lt;a href=&#34;https://en.wikipedia.org/wiki/Unsupervised_learning&#34;&gt;Unsupervised learning&lt;/a&gt; 和 &lt;a href=&#34;https://en.wikipedia.org/wiki/Reinforcement_learning&#34;&gt;Reinforcement learning&lt;/a&gt; , via &lt;a href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&gt;wikipedia&lt;/a&gt; 。这个如果将来看了相应的课再写。）,它通过训练数据集建立一种模式或叫函数，再通过此模式去匹配待查询数据集从而得出预测结果。&lt;/p&gt;

&lt;h2 id=&#34;instance-based-learning:e415793b7fb7c6a9af71ea301a4a0120&#34;&gt;Instance Based Learning&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Instance Based Learning&lt;/em&gt; 是机器学习的一种方法，也叫 &lt;em&gt;Memory Based Learning&lt;/em&gt; 。它不生成完整的匹配模式，而是在从训练数据中找到和待查询数据相近的数据（这些数据通常存放于内存中），并输出结果。将要说到的 &lt;em&gt;Nearest Neighbor Learning&lt;/em&gt; 即是其中的一种。此方法也分为 &lt;em&gt;1-Nearest Neighbor&lt;/em&gt; 和 &lt;em&gt;k-Nearest Neighbor&lt;/em&gt; 。进行计算需要了解如下4个元素：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如何衡量距离？&lt;/li&gt;
&lt;li&gt;找几个 Neighbor ？&lt;/li&gt;
&lt;li&gt;各点是否有不同的权重？&lt;/li&gt;
&lt;li&gt;如何确定输出？&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;1-nearest-neighbor-和-k-nearest-neighbor:e415793b7fb7c6a9af71ea301a4a0120&#34;&gt;1-Nearest Neighbor 和 k-Nearest Neighbor&lt;/h3&gt;

&lt;p&gt;在 &lt;em&gt;1-Nearest Neighbor&lt;/em&gt; 的情况下，距离使用欧氏距离，寻找最近的&lt;code&gt;1&lt;/code&gt;个 &lt;em&gt;Neighbor&lt;/em&gt; ，各点权重相同，直接以最近 &lt;em&gt;Neighbor&lt;/em&gt; 的输出为查询数据的输出。而在 &lt;em&gt;k-Nearest Neighbor&lt;/em&gt; 中，查找的节点变成了 &lt;em&gt;k&lt;/em&gt; 个，而输出出变成了 &lt;em&gt;k&lt;/em&gt; 个节点的平均值。&lt;/p&gt;

&lt;h3 id=&#34;kernel-regression:e415793b7fb7c6a9af71ea301a4a0120&#34;&gt;Kernel Regression&lt;/h3&gt;

&lt;p&gt;核回归，统计中的一种方法，具体的算法及使用场景没看太明白，此处暂时仅为记录而用。对于这个算法，它所涉及到的4个参量是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;欧氏距离&lt;/li&gt;
&lt;li&gt;查找所有训练集中的点&lt;/li&gt;
&lt;li&gt;权重函数： &lt;code&gt;$w_{i} = exp(-\frac{d(x_{i}, q)^{2}}{K_{w}})$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;输出：&lt;code&gt;$\frac{\sum_{i}w_{i}y_{i}}{\sum_{i}w_{i}}$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W2 - Locality-Sensitive Hashing
</title>
      <link>http://hzmangel.github.io/post/mmds-w2-locality-sensitive-hashing/</link>
      <pubDate>Wed, 17 Jun 2015 08:02:10 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w2-locality-sensitive-hashing/</guid>
      <description>

&lt;p&gt;Locality-Sensitive Hashing，LSH，局部敏感hash或叫位置敏感hash。它的想法是在对原始数据空间的数据做Hash后，让位置相邻的数据有很大概率被放到同一个或者相近的bucket中，而不相邻的点放在一起的概率要很小。这样就会减少后期数据处理的数据集，从而简化后续的工作。&lt;/p&gt;

&lt;h2 id=&#34;相似数据集:98fc395032abb3e5c8d913488aa084ef&#34;&gt;相似数据集&lt;/h2&gt;

&lt;p&gt;许多数据挖掘的问题都能简化为查找相似数据集的问题，如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;查找含有相似单词的页面，用以给页面分类，或者找出页面的镜像站，或者检查剽窃等。&lt;/li&gt;
&lt;li&gt;NetFlix，理解成豆瓣就行，哪些用户有相似的爱好。&lt;/li&gt;
&lt;li&gt;以及，哪些电影有相似的粉丝。&lt;/li&gt;
&lt;li&gt;网上找到的个人信息，怎么才能确定哪些属于同一个人。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;先从相似文档找起，有三个关键技术：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Shingling：把文档，像页面啊，邮件啊，什么的，拆成sets。&lt;/li&gt;
&lt;li&gt;Minhashing：在保留相似性的基础上，把大的集合转化成短的标记。&lt;/li&gt;
&lt;li&gt;Locality-sensitive hashing：找出相似的对。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;文档相似性可以使用 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 来衡量。对于两个集合 &lt;code&gt;$S$&lt;/code&gt; 和 &lt;code&gt;$T$&lt;/code&gt; 来说，它们之间的 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 为 &lt;code&gt;$|S \cap T| / |S \cup T|$&lt;/code&gt;，记为 &lt;em&gt;SIM(S,T)&lt;/em&gt; 或 &lt;em&gt;J(S,T)&lt;/em&gt; 。不难看出，当此值为&lt;code&gt;0&lt;/code&gt;时表示两个集合没有交集，而为&lt;code&gt;1&lt;/code&gt;时则表示两个集合相等。&lt;/p&gt;

&lt;h3 id=&#34;k-shingling或叫k-gram:98fc395032abb3e5c8d913488aa084ef&#34;&gt;k-shingling或叫k-gram&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;k-shingling&lt;/em&gt; 是指把文档按连续的 &lt;em&gt;k&lt;/em&gt; 个字母拆成子集的方法。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;例如，给定文档&lt;code&gt;$D$&lt;/code&gt;的内容为&lt;code&gt;abcdabd&lt;/code&gt;，在&lt;code&gt;$k=2$&lt;/code&gt;的情况下，获得的 &lt;em&gt;2-shingling&lt;/em&gt; 集合为 &lt;code&gt;{ab, bc, cd, da, bd}&lt;/code&gt;。shingling有一个变种是生成一个bag而非set，此时重复的元素不会被归并，而是按照其本来出现的次数出现在最后结果中，如本例中的&lt;code&gt;ab&lt;/code&gt;将会出现2次。&lt;/p&gt;

&lt;p&gt;对于空格的处理有多种选项，较常见的是把所有空格类的东西都替换成一个空格，然后将其作为一个正常元素参与到shingling中去。即shingling后的元素可能会包含2个或多个单词。&lt;/p&gt;

&lt;p&gt;为了避免虚假的相似度， &lt;code&gt;$k$&lt;/code&gt; 的取值需要足够大。一般而言，对于短的如电子邮件之类的文件，取&lt;code&gt;$k=5$&lt;/code&gt;，而对于长的文档，如研究报告这种，取&lt;code&gt;$k=9$&lt;/code&gt;会比较好。&lt;/p&gt;

&lt;p&gt;对于shingling中的元素，可以直接使用字符串，但是更好的办法是把它通过hash变化映射到某个bucket中，而将这个bucket的编号作为shingling元素进行比较。这样可以在shingling元素空间不变的情况下，降低运行时占用的内存。而且在比较上，整数比字符串要更有优势。这一步叫做 &lt;strong&gt;Compressing Shinglings&lt;/strong&gt; 。&lt;/p&gt;

&lt;h3 id=&#34;minhashing:98fc395032abb3e5c8d913488aa084ef&#34;&gt;Minhashing&lt;/h3&gt;

&lt;p&gt;这一段是从 &lt;a href=&#34;https://en.wikipedia.org/wiki/MinHash&#34;&gt;wikipedia&lt;/a&gt; 上看到的定义：&lt;/p&gt;

&lt;p&gt;设有一个hash函数&lt;code&gt;$h$&lt;/code&gt;，可以将集合中的元素映射为不重复的整数值。这样对于任何集合&lt;code&gt;$S$&lt;/code&gt;，都能找到一个元素&lt;code&gt;$x$&lt;/code&gt;让&lt;code&gt;$h(S)$&lt;/code&gt;取到最小值&lt;code&gt;$h_{min}(S)$&lt;/code&gt;。这样就把对字符串比较，存储转换成了对整数的计算和存储。由于&lt;code&gt;$h_{min}(S)$&lt;/code&gt;只能得到一个值，所以需要使用 &lt;em&gt;Hash Function Family&lt;/em&gt; 去处理集合，以得到一个最小值的向量。在向量长度足够的情况下，两个集合的相似度等于最小值相等的概率。计算向量有两种办法，一种是选取足够多的hash函数，另一种是对一个hash得出的值作多次变换。&lt;/p&gt;

&lt;p&gt;在课程中，首先介绍了怎么抽取多个集合的 &lt;em&gt;Characteristic Matrix&lt;/em&gt; ，这个矩阵的每一列都是一个需要计算相似度的集合，记为&lt;code&gt;$S_{1}$&lt;/code&gt;到&lt;code&gt;$S_{N}$&lt;/code&gt;，而行是所有元素的集合，记为&lt;code&gt;$e_{1}$&lt;/code&gt;到&lt;code&gt;$e_{M}$&lt;/code&gt;。如果某个元素&lt;code&gt;$e_{i}$&lt;/code&gt;包含于集合&lt;code&gt;$S_{j}$&lt;/code&gt;中，则在矩阵相应的位置&lt;code&gt;$(i,j)$&lt;/code&gt;标上&lt;code&gt;1&lt;/code&gt;，反之则为&lt;code&gt;0&lt;/code&gt;。典型情况下这个矩阵是稀疏的。&lt;/p&gt;

&lt;p&gt;此后直接介绍了一个 &lt;em&gt;Minhashing&lt;/em&gt; 的函数簇。假设前述的 &lt;em&gt;Characteristic Matrix&lt;/em&gt; 的行排列是随机的，我们定义一个 &lt;em&gt;Minhashing&lt;/em&gt; 函数 &lt;code&gt;h(S)&lt;/code&gt; ，它的值是在特定排列下，列 &lt;code&gt;S&lt;/code&gt; 中第一次出现 &lt;code&gt;1&lt;/code&gt; 的行数。使用多个独立的哈希函数（如100个），即可为每一个集合创建一个 &lt;em&gt;signatures&lt;/em&gt; ，而多个集合的结果合并后可以生成一个新的矩阵， &lt;em&gt;signatures matrix&lt;/em&gt; 。这个矩阵的列是各个集合，而行是某一次计算 &lt;em&gt;Minhashing&lt;/em&gt; 时的结果。&lt;/p&gt;

&lt;p&gt;下面来分析下 &lt;em&gt;Jaccard Similarity*。首先看 *Characteristic Matrix&lt;/em&gt; 。设有两个需要比较的集合 &lt;code&gt;$S_{1}$&lt;/code&gt; 和 &lt;code&gt;$S_{2}$&lt;/code&gt; ，假设它们的 &lt;em&gt;Characteristic Matrix&lt;/em&gt; 为 &lt;code&gt;$M$&lt;/code&gt;，那么在矩阵 &lt;code&gt;$M$&lt;/code&gt; 中，每一行的元素只有4种组合： &lt;code&gt;(0,0)&lt;/code&gt; ，&lt;code&gt;(0,1)&lt;/code&gt; ， &lt;code&gt;(1,0)&lt;/code&gt; 和 &lt;code&gt;(1,1)&lt;/code&gt;。我们把这4种关系在M中的数量分别记为ABCD，不难看出，两个集合的相似度可以表示为 &lt;code&gt;$J(S_{1}, S_{2}) = D/(A+B+C)$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;然后再来看 &lt;em&gt;signatures matrix&lt;/em&gt; 。在某个特定的排列下，如果两个集合的 &lt;em&gt;Minhashing&lt;/em&gt; 值相同，那第它们一定是 &lt;code&gt;(1,1)&lt;/code&gt; 形式的，而其它三种形式不会有此结果）注意，此处只能保证， &lt;em&gt;Minhashing&lt;/em&gt; 值相同时，能保证这一行是 &lt;code&gt;(1,1)&lt;/code&gt;，但是一行是&lt;code&gt;(1,1)&lt;/code&gt;并不能说明这一行是 &lt;em&gt;Minhashing&lt;/em&gt; 值）。所以可以得知，两个集合 &lt;em&gt;Minhashing&lt;/em&gt; 值相等的概率，也就是两个集合的 &lt;em&gt;Jaccard&lt;/em&gt; 相似度，都是 &lt;code&gt;$J(S_{1}, S_{2}) = D/(A+B+C)$&lt;/code&gt; 。&lt;/p&gt;

&lt;p&gt;在实际实现上，给大量的数据做随机排列是比较难以实现的，所以更加通用的办法就是如wiki上说的，挑选多个 hash 函数来处理，下面是一段伪代码，计算某集合的 &lt;em&gt;Minhashing&lt;/em&gt; 向量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FOREACH hash_func_family:
  CALCULATE hi(r)

FOREACH columes:
  IF val(c) == 1:
    # Init value for SIG(i, c) is inf
    SIG(i, c) = min( SIG(i, c), hi(r) )
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;locality-sensitive-hash:98fc395032abb3e5c8d913488aa084ef&#34;&gt;Locality-Sensitive Hash&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;By Me: 此处的概念还有些模糊，需要再啃啃。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;经过了前面的 &lt;em&gt;Shingling&lt;/em&gt; 和 &lt;em&gt;Minhashing&lt;/em&gt; ，需要处理的数据已经减少许多了，但是对于大文档集来说还不够。如果是需要找到任意两个集合之间的相似度，那么除了计算它们每两对之间的相似度以外没有其它任何办法。但是如果只是需要找到超过某个相似度阈值的集合对，则可以使用LSH，又叫 &lt;em&gt;Nearest Neighbor search&lt;/em&gt; 。&lt;/p&gt;

&lt;p&gt;LSH的一般做法是对元素使用多次Hash，让相似的元素落入同一个bucket中（即Hash冲突），而不相似的不在。对于上面生成的 &lt;em&gt;signatures matrix&lt;/em&gt; ，一个有效的办法是把矩阵按&lt;code&gt;r&lt;/code&gt;行分成&lt;code&gt;b&lt;/code&gt;个brand，对每一个brand中的每一小块长度为&lt;code&gt;r&lt;/code&gt;的特征值做hash，下面是分析（这块还是有些地方没想清楚，先记录下来）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;设矩阵分成了&lt;code&gt;b&lt;/code&gt;个 &lt;em&gt;brand&lt;/em&gt; ，每个 &lt;em&gt;brand&lt;/em&gt; 中有 &lt;code&gt;r&lt;/code&gt; 行。某特定两个文档的 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 是 &lt;code&gt;s&lt;/code&gt; 。即在 matrix 中某个Minhashing字符串与其它有&lt;code&gt;s&lt;/code&gt;的概率相似。&lt;/li&gt;
&lt;li&gt;某个brand中选定的特征列和其它所有列相似的概率是 &lt;code&gt;$s^{r}$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;某个brand中选定的特征列和至少一个其它列不相似的概率是&lt;code&gt;$1-s^{r}$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;一个特征列和每一个brand中都有至少一个不相似列的概率是&lt;code&gt;$(1-s^{r})^{b}$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;一个特征列和至少一个brand中所有列都相似，从而成为 &lt;em&gt;candidate pair&lt;/em&gt; 的概率为 &lt;code&gt;$1-(1-s^{r})^{b}$&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个曲线是一个S型的连续曲线，我们需要做的就是通过挑选&lt;code&gt;b&lt;/code&gt;和&lt;code&gt;r&lt;/code&gt;，让这条曲线在两端尽量的平缓，而在中间部分尽可能的陡峭。这样就不会有过多的 &lt;em&gt;False Positive&lt;/em&gt; 或者 &lt;em&gt;False Negative&lt;/em&gt; 。&lt;/p&gt;

&lt;h2 id=&#34;具体使用流程:98fc395032abb3e5c8d913488aa084ef&#34;&gt;具体使用流程&lt;/h2&gt;

&lt;p&gt;综上所述，在实际应用中会有下面几步工作（文档比较）：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;选择整数 &lt;code&gt;$k$&lt;/code&gt; ，将输入文档转换为 &lt;em&gt;k-shingling&lt;/em&gt; 集合。此处可以通过Hash将 &lt;em&gt;k-shingles&lt;/em&gt; 转换为较短的 &lt;em&gt;bucket序号&lt;/em&gt; 。&lt;/li&gt;
&lt;li&gt;以 &lt;em&gt;shingle&lt;/em&gt; 排序 &lt;code&gt;&amp;lt;document, shingle&amp;gt;&lt;/code&gt; 对。&lt;/li&gt;
&lt;li&gt;选择长度 &lt;code&gt;$n$&lt;/code&gt; 用于 &lt;em&gt;Minhashing Signature&lt;/em&gt; ，并为所有文档计算特征值。&lt;/li&gt;
&lt;li&gt;确定一个概率 &lt;code&gt;$t$&lt;/code&gt; 作为文档相似度的阈值，选择 &lt;code&gt;$b$&lt;/code&gt; 和 &lt;code&gt;$r$&lt;/code&gt; 并保证 &lt;code&gt;$br=n$&lt;/code&gt; ，而且阈值&lt;code&gt;$t$&lt;/code&gt;接近&lt;code&gt;$(1/b)^{1/r}$&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;如果需要最大程度的避免 &lt;em&gt;False Negative&lt;/em&gt; ，那么选择 &lt;code&gt;$b$&lt;/code&gt; 和 &lt;code&gt;$r$&lt;/code&gt; 时要注意计算出来的值要小于 &lt;code&gt;$t$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;如果需要保证速度而且避免 &lt;em&gt;False Positive&lt;/em&gt; ，那么选择 &lt;code&gt;$b$&lt;/code&gt; 和 &lt;code&gt;$r$&lt;/code&gt; 时注意计算出一个高的阈值。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;使用 &lt;em&gt;LSH&lt;/em&gt; 找到所有的 *candidate pairs*。&lt;/li&gt;
&lt;li&gt;检查选择出来的特征对，确定它们的相似度都大于 &lt;code&gt;$t$&lt;/code&gt; 。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;实例:98fc395032abb3e5c8d913488aa084ef&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;注：此处只是列出课程中出现的示例，后续会尝试使用程序完成，再补齐说明。&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;entity-resolution:98fc395032abb3e5c8d913488aa084ef&#34;&gt;Entity Resolution&lt;/h3&gt;

&lt;h3 id=&#34;fingerprints:98fc395032abb3e5c8d913488aa084ef&#34;&gt;Fingerprints&lt;/h3&gt;

&lt;h3 id=&#34;similar-news-articles:98fc395032abb3e5c8d913488aa084ef&#34;&gt;Similar News Articles&lt;/h3&gt;

&lt;h2 id=&#34;距离计算:98fc395032abb3e5c8d913488aa084ef&#34;&gt;距离计算&lt;/h2&gt;

&lt;p&gt;此块知识的最后提到了距离的计算。从某种意义上说，计算LSH即是计算某两个点之间的距离，越相似的点距离越近。上面提到的 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 并不是距离，用&lt;code&gt;1&lt;/code&gt;减去它才是。一般说来，有两种类型的距离，它们是 &lt;em&gt;欧氏距离&lt;/em&gt; 和 &lt;em&gt;非欧距离&lt;/em&gt; 。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;欧氏距离是指欧氏空间的距离，欧氏空间包含有实实在在维度，和密集的点。&lt;/li&gt;
&lt;li&gt;欧空中，可以在两个点之间找到中点。&lt;/li&gt;
&lt;li&gt;欧氏距离是基于欧空中点的位置来确定的&lt;/li&gt;
&lt;li&gt;其它的空间即被称为非欧空间。在非欧空间中，距离的计算是通过点的其它一些特性来完成的，因为非欧空间并没有位置这个概念。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;假设 &lt;code&gt;$x$&lt;/code&gt; ， &lt;code&gt;$y$&lt;/code&gt; 和 &lt;code&gt;$z$&lt;/code&gt; 是某个空间中的点，而 &lt;code&gt;$d$&lt;/code&gt; 是计算距离的函数，那么它有如下特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;$d(x,y) \geq 0$&lt;/code&gt; ：所有距离都是非负值。&lt;/li&gt;
&lt;li&gt;仅在 &lt;code&gt;$x$&lt;/code&gt; ， &lt;code&gt;$y$&lt;/code&gt; 是同一点时，才有 &lt;code&gt;$d(x,y) = 0$&lt;/code&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$d(x,y) = d(y,x)$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$d(x,z) \leq d(x,y) + d(y,z)$&lt;/code&gt; ：三角定理。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;欧氏距离:98fc395032abb3e5c8d913488aa084ef&#34;&gt;欧氏距离&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;$L_{r}-norm = (\sum_{i=1}^{n} |x_{i} - y_{i}|^{r})^{1/r}$&lt;/code&gt; 。在&lt;code&gt;$r=2$&lt;/code&gt;时，即变成平方各开根号，即熟悉的距离计算。此外还有&lt;code&gt;$L_{1}-norm$&lt;/code&gt; 。&lt;/p&gt;

&lt;h3 id=&#34;非欧距离:98fc395032abb3e5c8d913488aa084ef&#34;&gt;非欧距离&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Jaccard Distance&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;余弦距离&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Edit Distance&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Hamming Distance&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Set timezone in Python
</title>
      <link>http://hzmangel.github.io/post/timezone-in-python/</link>
      <pubDate>Tue, 16 Jun 2015 23:40:10 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/timezone-in-python/</guid>
      <description>&lt;p&gt;今天在写一个脚本的时候，发现使用&lt;code&gt;datetime.datetime.now()&lt;/code&gt;输出的是UTC时间，而同样的命令在ipython中输入的就是本地的时间。找了好久才找到不用&lt;code&gt;pytz&lt;/code&gt;的解决方案：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import datetime

print(datetime.datetime.now())
os.environ[&#39;TZ&#39;] = &#39;Asia/Shanghai&#39;
print(datetime.datetime.now())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;TZ&lt;/code&gt;的环境变量让datetime输出了指定时区的时间。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W1 - Link Analysis
</title>
      <link>http://hzmangel.github.io/post/mmds-w1-link-analysis/</link>
      <pubDate>Sun, 14 Jun 2015 17:10:58 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w1-link-analysis/</guid>
      <description>

&lt;p&gt;第一周的后半部分讲的是Link Analysis，主要讲的是&lt;strong&gt;PageRank&lt;/strong&gt;的计算。&lt;/p&gt;

&lt;p&gt;互联网在某种意义上是一个有向图，每个页面是图上的节点，而页面间的链接就是图的边。在经历了早期的目录式页面分类后，web现在进入了以Search为主的的组织方式。下面问题来了：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;怎么确定找到的信息是可以信赖的（或者相对可以信赖的）。&lt;/li&gt;
&lt;li&gt;当我们查找某个词时，哪个才是最好的结果。&lt;/li&gt;
&lt;li&gt;&lt;del&gt;搜索技术哪家强？&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以这里引入了给页面排序的做法，以确定页面的重要性。&lt;/p&gt;

&lt;h2 id=&#34;pagerank:b45bcfea51201690689ab7d1025dd751&#34;&gt;PageRank&lt;/h2&gt;

&lt;p&gt;PageRank，Google家的看家算法。核心思路是，如果一个页面是重要的，那么它指向的那个页面应该也是重要的。也就是用其它页面来证明这个页面的重要性。&lt;/p&gt;

&lt;h3 id=&#34;flow-formulation:b45bcfea51201690689ab7d1025dd751&#34;&gt;&lt;em&gt;Flow&lt;/em&gt; Formulation&lt;/h3&gt;

&lt;p&gt;在一张图中，假设有节点&lt;code&gt;$i$&lt;/code&gt;，它的PR值是&lt;code&gt;$r_{i}$&lt;/code&gt;，它有&lt;code&gt;$d_{i}$&lt;/code&gt;条出链，其中一条指向节点&lt;code&gt;$j$&lt;/code&gt;。那么&lt;code&gt;$j$&lt;/code&gt;上由&lt;code&gt;$i$&lt;/code&gt;带来的PR值即为&lt;code&gt;$\frac{r_{i}}{d_{i}}$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;根据这个需求，可以列出来一个方程组。显然，图中有几个节点，这个方程组就会有几个方程，为了给这个方程求得一个固定解，我们会人为的加上一个条件 &lt;code&gt;$\sum{r_{i}} = 1$&lt;/code&gt;，这样就可以求解出每个节点的PR值了。&lt;/p&gt;

&lt;p&gt;这个方法比较易于理解，但是对于大规模的页面集不适用，所以引入了&lt;em&gt;Matrix&lt;/em&gt; Formulation。&lt;/p&gt;

&lt;h3 id=&#34;matrix-formulation:b45bcfea51201690689ab7d1025dd751&#34;&gt;&lt;em&gt;Matrix&lt;/em&gt; Formulation&lt;/h3&gt;

&lt;p&gt;首先，把所有页面之间的跳转都用一个&lt;strong&gt;列随机矩阵(column stochastic matrix)&lt;/strong&gt;来表示，记为&lt;code&gt;$M$&lt;/code&gt;。对于每条链路&lt;code&gt;$i\rightarrow j$&lt;/code&gt;，都有相应的&lt;code&gt;$M_{ji} = 1/d_{i}$&lt;/code&gt;，其中&lt;code&gt;$d$&lt;/code&gt;是&lt;code&gt;$i$&lt;/code&gt;的出链路条数。下一步是&lt;strong&gt;Rank向量&lt;/strong&gt;，记为&lt;code&gt;$r$&lt;/code&gt;它是一个1维的列向量，每一个元素的值就是表示此节点的Rank值，记为&lt;code&gt;$r_{i}$&lt;/code&gt;，此向量满足&lt;code&gt;$\sum_{i}r_{i} = 1$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;有此定义后，上面介绍的*Flow*方程可以转换成这样： &lt;code&gt;$r = M \cdot r$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;由矩阵的定义，这个&lt;code&gt;$r$&lt;/code&gt;是矩阵&lt;code&gt;$M$&lt;/code&gt;的&lt;strong&gt;单位向量(eigenvector)&lt;/strong&gt;。即，页面的PageRank值，就是这些页面之间转移矩阵的单位向量，解出了这个向量，也就确定了这些页面的PageRank值。&lt;/p&gt;

&lt;p&gt;求解特征向量使用的是被称为*Power Iteration*的方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;初始化： &lt;code&gt;$r^{(0)} = [1/N, \ldots , 1/N]^{T}$&lt;/code&gt; ，其中&lt;code&gt;$N$&lt;/code&gt;是图中的节点数，也即所有页面个数。&lt;/li&gt;
&lt;li&gt;迭代： &lt;code&gt;$r^{(t+1)} = M \cdot r^{(t)}$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;停止条件： &lt;code&gt;$|r^{(t+1) = r^{(t)}}|_{1} &amp;lt; \epsilon$&lt;/code&gt;。其中&lt;code&gt;$|x_{i}|_{1} ＝ \sum_{i}|x_{i}|$&lt;/code&gt;是向量&lt;code&gt;$i$&lt;/code&gt;的 &lt;em&gt;L1范数&lt;/em&gt; （其实就是绝对值相加，范数的定义就是 &lt;code&gt;$|x_{i}|_{p} = (\sum_{i}|x_{i}|^{p})^{\frac{1}{p}}$&lt;/code&gt;）。此处可以使用其它的范数，如L2范数，即欧氏距离。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;teleports:b45bcfea51201690689ab7d1025dd751&#34;&gt;&lt;em&gt;Teleports&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;如果图中有 &lt;em&gt;dead end&lt;/em&gt; 节点，即只有进链没有出链；或者是在多个节点之间存在环，那么上面的迭代就会收敛到一个错误的结果上。解决方案就是，对于每次跳转，有&lt;code&gt;$\beta$&lt;/code&gt;的概率跟随链路去跳，而有&lt;code&gt;$1-\beta$&lt;/code&gt;的概率是一次随机传送 (&lt;em&gt;Teleports&lt;/em&gt;)，这样就解决上面提到的两个问题了。在实际使用中，一般&lt;code&gt;$\beta$&lt;/code&gt;取值为&lt;code&gt;0.8&lt;/code&gt;或&lt;code&gt;0.9&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;在计算上，就是引入一个Teleports矩阵，其中的每个值都是&lt;code&gt;$1/N$&lt;/code&gt;，&lt;code&gt;$N$&lt;/code&gt;为节点数目。而之前的转移矩阵&lt;code&gt;$M$&lt;/code&gt;则变为&lt;code&gt;$\beta M + (1-\beta)\frac{1}{N}e \cdot e^{T}$&lt;/code&gt;，记为&lt;code&gt;$A$&lt;/code&gt;。同样，状态跳转的迭代公式也变为&lt;code&gt;$r = A \cdot r$&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&#34;实际使用的样子:b45bcfea51201690689ab7d1025dd751&#34;&gt;实际使用的样子&lt;/h3&gt;

&lt;p&gt;以上说的都是理论，在那个神奇的世界里，计算机的内存都是无限大的，计算速度都是无限NB的，一句话，TA们都是超限界的。在回到了位于爬行界的真实世界后，还有其它需要考虑的东西。假设节点数目是 &lt;strong&gt;1 billion&lt;/strong&gt; ，那么计算前和后的向量各需要&lt;strong&gt;1 billion&lt;/strong&gt;，但是对于那个矩阵，它可是&lt;strong&gt;1 billion * 1 billion&lt;/strong&gt;，也就是&lt;strong&gt;10^18&lt;/strong&gt;。这个内存，有点贵哈～&lt;/p&gt;

&lt;p&gt;一般来说，转移矩阵都是稀疏的，这样在存储的时候可以不用存多少东西，但是加了那个Teleports后，它变成每个位置都有值了，这内存使用就duang的一下上来了。还好经过计算，发现公式&lt;code&gt;$r = A \cdot r$&lt;/code&gt; 可以改写成：&lt;code&gt;$r = \beta M \cdot r + [ \frac{1-\beta}{N} ]_{N}$&lt;/code&gt;。这就是说，矩阵还是那个稀疏的矩阵，但是在每次算完后，需要在向量上加上Teleports的结果。这样一来，占用的内存又回去了吧。&lt;/p&gt;

&lt;p&gt;基本上就是这样了~&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W1 - HDFS &amp; MR
</title>
      <link>http://hzmangel.github.io/post/mmds-w1-hdfs-mr/</link>
      <pubDate>Sat, 13 Jun 2015 23:13:32 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w1-hdfs-mr/</guid>
      <description>

&lt;p&gt;前段时间在Cousera上各种挤时间跟完了一门 &lt;a href=&#34;https://class.coursera.org/mmds-002&#34;&gt;MMDS&lt;/a&gt; ，手上留下了一堆笔记，整理下，顺便给新blog开光吧。&lt;/p&gt;

&lt;p&gt;课程总共7周，这篇整理的第一周的 &lt;code&gt;HDFS&lt;/code&gt; 和 &lt;code&gt;MR&lt;/code&gt; 部分。&lt;/p&gt;

&lt;h2 id=&#34;dfs:328f502a6f9088599afe3ec330de3e9a&#34;&gt;DFS&lt;/h2&gt;

&lt;p&gt;课程开始是从分布式存储DFS讲起，介绍性的东西居多。在大规模的集群中，硬件故障是个很容易发生的问题。解决方案要么就是堆大把的钱上NB的机器，要么就是多做备份。这里的DFS就是后一种解决方案。&lt;/p&gt;

&lt;p&gt;现在较为通用的分布式架构就是使用不那么NB的Linux机器组建Cluster，然后用不那么NB的网络把它们连接起来。而在分布式存储的情况下，把数据在不同节点之间复制是需要时间的，所以让程序去找数据就是一个比较正确的解决方案了。&lt;/p&gt;

&lt;p&gt;DFS中的节点有两类，&lt;code&gt;Chunk Server&lt;/code&gt;用来存放数据，&lt;code&gt;Master Node&lt;/code&gt;用来存放文件和&lt;code&gt;Chunk Server&lt;/code&gt;的对应关系。一个文件会被分成多处连续的&lt;code&gt;chunks&lt;/code&gt;，典型的大小是16~64MB。每一个&lt;code&gt;chunk&lt;/code&gt;都会复制2到3份，分别存储在不同的机器上（最好是在不同rack上）。而&lt;code&gt;Master Node&lt;/code&gt;就会存储这些机器和文件的映射关系。当需要查找这个文件时，先从&lt;code&gt;Master Node&lt;/code&gt;处取到&lt;code&gt;Chunk Server&lt;/code&gt;的信息，再从相应的server上获取文件内容。&lt;/p&gt;

&lt;h2 id=&#34;map-reduce:328f502a6f9088599afe3ec330de3e9a&#34;&gt;Map Reduce&lt;/h2&gt;

&lt;p&gt;MR是一种编程模型，典型的应用场景就是 &lt;strong&gt;Word Cound&lt;/strong&gt; 。将MR应用到 WordCount 的先决条件为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;文件过大，不能完整的放到内存中去&lt;/li&gt;
&lt;li&gt;但是所有的&lt;code&gt;&amp;lt;key, value&amp;gt;&lt;/code&gt;对可以完全放到内存中&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;算法流程:328f502a6f9088599afe3ec330de3e9a&#34;&gt;算法流程&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;将数据分块读入&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Map&lt;/code&gt; ：创建&lt;code&gt;&amp;lt;k, v&amp;gt;&lt;/code&gt;数据对。在本例中，即创建出一系列的&lt;code&gt;&amp;lt;word, 1&amp;gt;&lt;/code&gt;对。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Group by Key&lt;/code&gt; ：对&lt;code&gt;&amp;lt;k, v&amp;gt;&lt;/code&gt;进行排序。本例中即将同样key的&lt;code&gt;&amp;lt;key, value&amp;gt;&lt;/code&gt;对放在一起。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Reduce&lt;/code&gt; ：将同一个&lt;code&gt;key&lt;/code&gt;的&lt;code&gt;&amp;lt;k, v&amp;gt;&lt;/code&gt;对合并到一起，并对&lt;code&gt;v&lt;/code&gt;做相应处理，在本例中，就是把所有的值相加。&lt;/li&gt;
&lt;li&gt;输出结果&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在具体实现上，程序需要指定&lt;code&gt;Map&lt;/code&gt;和&lt;code&gt;Reduce&lt;/code&gt;两个方法，这两个方法的参数都是&lt;code&gt;&amp;lt;key, value&amp;gt;&lt;/code&gt;对。例如&lt;code&gt;Map&lt;/code&gt;函数的&lt;code&gt;key&lt;/code&gt;可以是文件名，&lt;code&gt;value&lt;/code&gt;是对应的某一行或者某一块文字。而在&lt;code&gt;Reduce&lt;/code&gt;函数中，输入的&lt;code&gt;key&lt;/code&gt;是某个单词，而&lt;code&gt;value&lt;/code&gt;是1。&lt;code&gt;Reduce&lt;/code&gt;负责归并结果，并输出。&lt;/p&gt;

&lt;h3 id=&#34;map-reduce的环境:328f502a6f9088599afe3ec330de3e9a&#34;&gt;Map Reduce的环境&lt;/h3&gt;

&lt;p&gt;除了根据逻辑实现&lt;code&gt;Map&lt;/code&gt;和&lt;code&gt;Reduce&lt;/code&gt;两个函数外，还需要一个运行Map Reduce的环境，它需要提供下面几项功能：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;把输入数据分块&lt;/li&gt;
&lt;li&gt;在机器前调度程序运行&lt;/li&gt;
&lt;li&gt;处理 &lt;strong&gt;Group by key&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;处理机器故障&lt;/li&gt;
&lt;li&gt;管理机器间通信&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;出错处理:328f502a6f9088599afe3ec330de3e9a&#34;&gt;出错处理&lt;/h3&gt;

&lt;p&gt;MR出错分几种，处理方式也不同：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Map&lt;/code&gt; Worker出错：MasterNode会将此块数据标为未完成，并等待下一轮调度&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Reduce&lt;/code&gt; Worker出错：MasterNode会将正在运行中的任务置为无效，并等待下一轮调度&lt;/li&gt;
&lt;li&gt;MasterNode：任务直接退出。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;map-和-reduce-的数目:328f502a6f9088599afe3ec330de3e9a&#34;&gt;&lt;code&gt;Map&lt;/code&gt;和&lt;code&gt;Reduce&lt;/code&gt;的数目&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;Mapper&lt;/code&gt;的数目会比节点数目多许多，这样就能保证一台节点上会被会到多个任务，即使节点出错，也只会影响到正在运行中的小块任务，对于其它被分配到此节点但是还没有运行的任务来说，可以很方便的调度到其它节点上。如果任务很大，而又有一个节点在任务运行时故障的话，就需要回滚较多的部分。而且大任务也不方便充分利用空闲的机器资源。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Reduce&lt;/code&gt;的数目没有具体要求，但是一般会比&lt;code&gt;Mapper&lt;/code&gt;的数目要少。&lt;/p&gt;

&lt;h3 id=&#34;改良:328f502a6f9088599afe3ec330de3e9a&#34;&gt;改良&lt;/h3&gt;

&lt;p&gt;还是拿WC作为例子。在&lt;code&gt;Reduce&lt;/code&gt;函数处，原始的办法需要传一堆&lt;code&gt;&amp;lt;word, 1&amp;gt;&lt;/code&gt;对到处理端，这会占用较多的带宽和传输时间，所以一个改良就是在传给&lt;code&gt;Reduce&lt;/code&gt;之前先归并一下，相当于多级&lt;code&gt;Reduce&lt;/code&gt;，而这一中间处理是在本地完成的，这样就可以减少对网络带宽的占用，以及乱序Mapper结果时的计算量。&lt;/p&gt;

&lt;p&gt;注意，此种方式并不适用所有计算，例如对多个数取平均值就不可以使用。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blog搬(zhe)家(teng)记</title>
      <link>http://hzmangel.github.io/post/new-github-blog/</link>
      <pubDate>Sat, 13 Jun 2015 11:36:18 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/new-github-blog/</guid>
      <description>

&lt;p&gt;其实把blog从WP挪出来的想法很早前就有了，只是由于拖延症的原因一直没去弄。不过最近可能是处于病情的低谷期，所以就动手了。&lt;/p&gt;

&lt;p&gt;当初想把blog搬家的主要需求也就下面这些：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;能用Markdown写。除了写着舒服外，这不是还能装13嘛～&lt;/li&gt;
&lt;li&gt;之前那些内容能弄过来，折腾来折腾去弄了这么久，之前的东西还是想保留下的。&lt;/li&gt;
&lt;li&gt;模板稍微好看点（不过这个最后证实了还是需要自己弄，诶）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;hugo:1b3cc112b24de2a8fac8b7e8858afbfb&#34;&gt;Hugo&lt;/h2&gt;

&lt;p&gt;最开始的时候也想着用 &lt;a href=&#34;http://jekyllrb.com/&#34;&gt;Jekyll&lt;/a&gt; 或者 &lt;a href=&#34;http://octopress.org/&#34;&gt;Octopress&lt;/a&gt; ，但是在转换的时候，发现有些&lt;code&gt;Latex&lt;/code&gt;的代码中的行末注释&lt;code&gt;{%&lt;/code&gt;会被认成模板字符串的开始字符，然后就悲剧的说我找不到另一半了啊啊啊我要去死啊啊啊，然后就没有然后了&amp;hellip;&amp;hellip; 想着如果自己写一个的话那又不知道要拖到什么时候了，所以在网上找到了 &lt;a href=&#34;http://gohugo.io/&#34;&gt;Hugo&lt;/a&gt;。所以，其实我选择它的原因也就是它能把我原来所有的东西都显示出来不出错（格式什么的再说哈）。&lt;/p&gt;

&lt;h2 id=&#34;wp2hugo:1b3cc112b24de2a8fac8b7e8858afbfb&#34;&gt;wp2hugo&lt;/h2&gt;

&lt;p&gt;敲定了用的框架，下一步就是挪东西了。Jekyll提供了一个挪东西的Gem， &lt;a href=&#34;https://github.com/jekyll/jekyll-import&#34;&gt;Jekyll Import&lt;/a&gt; ，在WP上也有导出的插件，但是拿到的导出文件多少有些问题，有的是没有时间，有的是没有转成Markdown，当然有一个最不能忍的就是导出文件的文件名（我的blog里面的标题是中文，所以也就明白是咋回事了吧&amp;hellip;）。本来想去看那些代码自己弄，再一想算了，自己写吧，这种小工具应该不会引发拖延症的。所以就有了这么个玩意： &lt;a href=&#34;https://github.com/hzmangel/wp2hugo&#34;&gt;wp2hugo&lt;/a&gt; 。这货主要干的事情是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;解析Wordpress导出的XML文件。&lt;/li&gt;
&lt;li&gt;将站点信息存到&lt;code&gt;config.yaml&lt;/code&gt;中。&lt;/li&gt;
&lt;li&gt;将文章存放到带有元信息的Markdown文件中（使用 &lt;a href=&#34;https://pypi.python.org/pypi/html2text&#34;&gt;html2text&lt;/a&gt; 将原来的HTML内容转成Markdown ），使用 &lt;strong&gt;post id&lt;/strong&gt; 作为文件名。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基本上我目前的需求是可以被满足了，不过有一个问题是现在页面中引用的图像文件还是放在老站上的，考虑是不是在脚本中加一个选项把它给放到某个assets目录中去。&lt;/p&gt;

&lt;h2 id=&#34;twenty-ten:1b3cc112b24de2a8fac8b7e8858afbfb&#34;&gt;Twenty Ten&lt;/h2&gt;

&lt;p&gt;原来的Blog用的模板是WP自带的 &lt;a href=&#34;https://wordpress.org/themes/twentyten/&#34;&gt;Twenty Ten&lt;/a&gt; ，试了几个Hugo的模板后不喜欢，于是着手把这货给弄过来。由于有一些Hugo的变量，还有就是在port到后面有点烦了所以直接把bootstrap弄进来了，所以也新开了一个 &lt;a href=&#34;https://github.com/hzmangel/hugo-twenty-ten&#34;&gt;GitHub repo&lt;/a&gt; 放这东西。不过有两个东西想吐槽，一是分页，二是标签云。&lt;/p&gt;

&lt;h3 id=&#34;分页:1b3cc112b24de2a8fac8b7e8858afbfb&#34;&gt;分页&lt;/h3&gt;

&lt;p&gt;Hugo内置了分页，就是在页面中加入 &lt;code&gt;{{ template &amp;quot;_internal/pagination.html&amp;quot; . }}&lt;/code&gt; ，但是它是把所有页面都列出来的啊，然后我就发现我的页面下面放了2行页码。虽然说增加每页的文章数可以解决这个问题，但这也不是啥解决办法啊。提供的少的可怜的计算功能还有犯晕的模板语法也没法很快弄出来那种显示第一页最后一页的链接，高亮当前页并显示当前页前后各X页，其它页用&lt;code&gt;...&lt;/code&gt;代替的效果，所以最后就直接用上一页下一页了，回头有空的话考虑用那种页面底部加上loading按钮的做法吧。&lt;/p&gt;

&lt;h3 id=&#34;标签云:1b3cc112b24de2a8fac8b7e8858afbfb&#34;&gt;标签云&lt;/h3&gt;

&lt;p&gt;Hugo中提供获取所有标签和标签下对应文章数的函数，但是对于生成字体大小不同的标签云来说，它没有提供对应的数学函数。最后选择的做法就是用Hugo把标签名称，数目，以及链接地址生成到某个div的data属性中，然后再用javascript取到其中信息，计算，并生成标签云的代码。Hugo输出的模板是这样的： &lt;a href=&#34;https://github.com/hzmangel/hugo-twenty-ten/blob/master/layouts/partials/sidebar.html&#34;&gt;sidebar.html&lt;/a&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;div class=&amp;quot;sidebar-block&amp;quot;&amp;gt;
  &amp;lt;h5&amp;gt;标签&amp;lt;/h5&amp;gt;
  &amp;lt;div id=&amp;quot;tag_cloud&amp;quot; data-tags=&amp;quot;{{ range $key, $value := .Site.Taxonomies.tags }} [&#39;{{$key}}&#39;, {{len $value}}, &#39;/tags/{{ $key | urlize }}&#39;]; {{ end }}&amp;quot; /&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后生成标签云的js是这样的：&lt;a href=&#34;https://github.com/hzmangel/hugo-twenty-ten/blob/master/static/js/index.js&#34;&gt;index.js&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;$( document ).ready(function() {
    var tag_list = eval($(&amp;quot;#tag_cloud&amp;quot;).data(&#39;tags&#39;).split(&#39;;&#39;));
    tag_list.pop();

    var tag_json = [];
    var total_cnt = 0;
    $.each(tag_list, function( idx, val ) {
        foo = eval(val);
        tag_json.push(foo);
        total_cnt += foo[1];
    })
    generate_tag_cloud(tag_json, total_cnt);
});

var generate_tag_cloud = function(tag_json, total_cnt) {
    tag_cloud_str = &amp;quot;&amp;lt;div id=&#39;tag_cloud_canvas&#39;&amp;gt;&amp;quot;;

    $.each(tag_json, function(idx, val) {
        font_size = 8 + Math.log(val[1])/Math.log(total_cnt) * 18;
        tag_cloud_str += &amp;quot;&amp;lt;a style=\&amp;quot;font-size:&amp;quot; + font_size +&amp;quot;pt\&amp;quot; href=&amp;quot; + val[2] + &amp;quot;&amp;gt;&amp;quot; + val[0] + &amp;quot;&amp;lt;/a&amp;gt;&amp;amp;nbsp; &amp;quot;;
    });

    tag_cloud_str += &amp;quot;&amp;lt;/div&amp;gt;&amp;quot;;

    $(&amp;quot;#tag_cloud&amp;quot;).append(tag_cloud_str);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前差不多就是这样了，偶尔想到啥新的东西再往上加吧，嘿嘿。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Control Goroutines amount via bufferred channel</title>
      <link>http://hzmangel.github.io/post/1239/</link>
      <pubDate>Sat, 19 Apr 2014 23:23:51 +0000</pubDate>
      
      <guid>http://hzmangel.github.io/post/1239/</guid>
      <description>&lt;p&gt;最近还是在写爬虫，然后发现用goroutine是很快，但是很容易就碰到并发数过多被服务器限制的问题。虽然说让goroutine在起来前睡一小会能解决一些问题
，但是终归感觉这样的办法不靠谱。继续翻文档发现&lt;code&gt;bufferred channel&lt;/code&gt;用在这不错。###  Bufferred Channel&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Bufferred channel&lt;/code&gt;和&lt;a href=&#34;http://www.hzmangel.info/blog/archives/1230&#34;&gt;前一篇文章&lt;/a&gt;中说的东西没
有太大差别，只是那篇文章中说的channel是不带缓存的，也就是相当于Semaphore的用法，而加了缓存的就是管道。不多说了，直接上代码吧：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import &amp;quot;fmt&amp;quot;
import &amp;quot;time&amp;quot;

func main() {

    channel_cnt := 10
    concurrency_chan := make(chan bool, 2)
    msg_chan := make(chan string)

    fmt.Println(&amp;quot;Start launching goroutines&amp;quot;)
    for i := 0; i &amp;lt; channel_cnt; i++ {
        go foo(i, concurrency_chan, msg_chan)
    }
    fmt.Println(&amp;quot;Finish launching goroutines&amp;quot;)

    for i := 0; i &amp;lt; channel_cnt; i++ {
        fmt.Println(&amp;lt;-msg_chan)
    }

}

func foo(i int, concurrency_chan chan bool, msg_chan chan string) {
    concurrency_chan &amp;lt;- true
    s := fmt.Sprintf(&amp;quot;%s: Call index %d&amp;quot;, time.Now(), i)
    msg_chan &amp;lt;- s
    time.Sleep(1 * time.Second)
    &amp;lt;-concurrency_chan
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段程序创建了两个&lt;code&gt;channel&lt;/code&gt;，而&lt;code&gt;concurrency_chan&lt;/code&gt;就是一个用来控制goroutine数量的&lt;code&gt;channel&lt;/code&gt;。在每个&lt;code&gt;gorout
ine&lt;/code&gt;开始的时候，会往这个&lt;code&gt;channel&lt;/code&gt;中写入一个值，而在函数结束的时候从&lt;code&gt;channel&lt;/code&gt;中把东西取出来。当&lt;code&gt;channel&lt;/code&gt;中的缓存被占满的时候，
后续的写入请求就会被阻塞，从而达到限制&lt;code&gt;goroutine&lt;/code&gt;个数的目的。代码和输出结果可以看&lt;a href=&#34;http://play.golang.org/p/8-
qUxtH0gp&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Golang and JSON API</title>
      <link>http://hzmangel.github.io/post/1233/</link>
      <pubDate>Thu, 20 Feb 2014 01:44:55 +0000</pubDate>
      
      <guid>http://hzmangel.github.io/post/1233/</guid>
      <description>

&lt;p&gt;最近在尝试用golang做爬虫类的东西，避免不了需要处理JSON API。其间碰到了些问题，记在这里以便下次查阅。### 生成URL&lt;/p&gt;

&lt;p&gt;嗯，反正我的blog已经被墙了，所以这次就用不存在的网站来作为示例吧。这个网站的名字叫Facebook（其实是不想为国内的账号上传身份证，所以只有用国外的A
PI了）。找了一圈，决定拿这个不存在的网站的CEO来测试。&lt;/p&gt;

&lt;p&gt;用的是Facebook提供的&lt;a href=&#34;https://developers.facebook.com/docs/graph-
api&#34;&gt;Graph API&lt;/a&gt;，下面演示的是如何用golang拼出来一个URL：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;net/url&amp;quot;
)

func main() {
    fmt.Println(userinfo_api(&amp;quot;4&amp;quot;, &amp;quot;&amp;quot;))
}

func userinfo_api(account_id string, access_token string) string {
    base_url := &amp;quot;http://graph.facebook.com/&amp;quot;

    params := make(url.Values)
    params.Set(&amp;quot;fields&amp;quot;, &amp;quot;id,name,picture&amp;quot;)
    if access_token != &amp;quot;&amp;quot; {
        params.Set(&amp;quot;access_token&amp;quot;, access_token)
    }

    api_url := base_url + account_id + &amp;quot;?&amp;quot; + params.Encode()

    return api_url
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;程序会在终端上打印出一个编码过的URL地址。这个地址直接贴到浏览器里面是会返回错误信息的，所以下一步我们需要在golang中去访问这个地址。多提一句，到目前
为止还不需要科学上网的技能。还是一样，如果不想在本机测试结果，可以到&lt;a href=&#34;http://play.golang.org/p/mhBQxRwTmF&#34;&gt;这个地址&lt;/a&gt;去
瞧一瞧看一看。&lt;/p&gt;

&lt;h3 id=&#34;调用api:2e6ed15c900da4872231994fbb6e0e50&#34;&gt;调用API&lt;/h3&gt;

&lt;p&gt;在网上查了查发现大家对golang自带的http库表示比较满意，所以本着少用第三方库的原则，就直接使用了内置的&lt;code&gt;http&lt;/code&gt;库来获取JSON。程序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;net/http&amp;quot;
    &amp;quot;net/url&amp;quot;
)

func main() {
    info_api := userinfo_api(&amp;quot;4&amp;quot;, &amp;quot;&amp;quot;)
    fmt.Println(info_api)
    get_api_resp(info_api)
}

func userinfo_api(account_id string, access_token string) string {
    base_url := &amp;quot;http://graph.facebook.com/&amp;quot;

    params := make(url.Values)
    params.Set(&amp;quot;fields&amp;quot;, &amp;quot;id,name,picture&amp;quot;)
    if access_token != &amp;quot;&amp;quot; {
        params.Set(&amp;quot;access_token&amp;quot;, access_token)
    }

    api_url := base_url + account_id + &amp;quot;?&amp;quot; + params.Encode()

    return api_url
}

func get_api_resp(api_url string) {
    resp, err := http.Get(api_url)
    if err != nil {
        panic(err)
    }
    defer resp.Body.Close()

    fmt.Println(resp)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到，直接使用&lt;code&gt;http.Get&lt;/code&gt;即可向服务器端发送GET请求。这个程序最后打印出来的&lt;code&gt;resp&lt;/code&gt;是一个&lt;code&gt;http.Response&lt;/code&gt;的类型，而我们真正
需要的是这个响应所带来的内容，所以需要用下面的调用来取得真正的响应内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import &amp;quot;io/ioutil&amp;quot;

...

content, err := ioutil.ReadAll(resp.Body)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时得到的&lt;code&gt;content&lt;/code&gt;是&lt;code&gt;[]byte&lt;/code&gt;类型的数据，即&lt;code&gt;byte&lt;/code&gt;类型的数组，如果直接用&lt;code&gt;fmt.Println(content)&lt;/code&gt;打印出来则会在屏幕
上显示数据的Ascii码，所以如果想以字符串的形式打印到屏幕上需要将其转换为string类型：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(string(content))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时屏幕上显示的即是正常的JSON响应了。&lt;/p&gt;

&lt;h3 id=&#34;处理json数据:2e6ed15c900da4872231994fbb6e0e50&#34;&gt;处理JSON数据&lt;/h3&gt;

&lt;p&gt;此时的JSON数据还只是一个字符串，程序并不认为它和其它的字符串有什么不同，此时就需要使用内置的JSON库去解码，解码所得的结果放于&lt;code&gt;map&lt;/code&gt;结构中。&lt;/p&gt;

&lt;p&gt;golang的解码有两种方式。第一种方式是明确知道返回JSON的数据格式，包括返回数据的结构，每一项的名称，以及值的类型，这种类型的返回数据可以直接解码到预
先定义的&lt;code&gt;struct&lt;/code&gt;中，然后使用点操作符调用获取其中的值，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type userinfo_api_resp struct {
    Id      string
    Name    string
    Picture userinfo_picture_data_wrapper
}

type userinfo_picture_data_wrapper struct {
    Data userinfo_picture_data_detail
}

type userinfo_picture_data_detail struct {
    Url           string
    Is_silhouette bool
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在定义好这些内容后，使用下面的代码负责把JSON内容填入其中：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// content saves json repsonse in []byte formated
err = json.Unmarshal(content, &amp;amp;json_rslt)
if err != nil {
    panic(err)
}
fmt.Println(json_rslt)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时即可以像普通的struct那样调用JSON中的值了，如使用&lt;code&gt;json_rslt.Name&lt;/code&gt;可以获取到名字，用&lt;code&gt;json_rslt.Picture.Dat
a.Url&lt;/code&gt;可以获取到图片的地址等。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：&lt;code&gt;struct&lt;/code&gt;中字段名的首字母都需要大写，猜测是由于golang中非首字母大写的变量不会被导出。（话说，真的好想好想吐槽这个设定啊，实在不习惯看大小写混杂的变量名，诶。）&lt;/p&gt;

&lt;p&gt;当JSON返回的格式未知，或者不想定义过多的struct时，可以使用第二种方法。&lt;code&gt;interface{}&lt;/code&gt;。这个东西是一个空的&lt;code&gt;interface&lt;/code&gt;，表示的
是一个没有任何方法的&lt;code&gt;interface&lt;/code&gt;，因为任何golang的类型都至少实现了0个方法（唔，直接翻译过来的，读着有点拗口，回头揣摩明白了再回来改），所以
这个类型实际可以用于任何类型的变量。第二种方法就是使用&lt;code&gt;interface{}&lt;/code&gt;来表示所有未定的类型，然后由程序去处理相应的数据。还是上面那段JSON数据，
想取到图片地址，则需要这么处理：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;json_rslt := map[string]interface{}{}

...

for k1, v1 := range json_rslt {
    if k1 == &amp;quot;picture&amp;quot; {
        for k2, v2 := range v1.(map[string]interface{}) {
            if k2 == &amp;quot;data&amp;quot; {
                for k3, v3 := range v2.(map[string]interface{}) {
                    if k3 == &amp;quot;url&amp;quot; {
                        fmt.Println(v3)
                    }
                }
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;唔，其实也没简单到哪去的感觉&amp;hellip;&amp;hellip;&lt;/p&gt;

&lt;p&gt;在&lt;code&gt;range&lt;/code&gt;后调用的诸如&lt;code&gt;v1.(map[string]interface{})&lt;/code&gt;被称为&lt;a href=&#34;http://golan
g.org/ref/spec#Type_assertions&#34;&gt;&lt;em&gt;type assertions&lt;/em&gt;&lt;/a&gt;，它的意思是断言&lt;code&gt;v1&lt;/code&gt;非空而且&lt;code&gt;v1&lt;/code&gt;的类型是&lt;code&gt;(map[string]interface{
})&lt;/code&gt;。文档中给出的更加精确的说法是&lt;code&gt;x.(T)&lt;/code&gt;表示&lt;code&gt;x&lt;/code&gt;的动态类型和&lt;code&gt;T&lt;/code&gt;一致，&lt;code&gt;T&lt;/code&gt;必需实现&lt;code&gt;x&lt;/code&gt;的所有接口。更加具体的代码请参考文档&lt;/p&gt;

&lt;p&gt;到这，差不多就完成了，POST请求的事情还没用到，等用到后发现有什么需要写的再写吧。另外需要注意的一点是，&lt;a href=&#34;http://play.golang.org/&#34;&gt;http://play.golang.org/&lt;/a&gt;
网站上是不允许使用&lt;code&gt;net/http&lt;/code&gt;库的，所以我后面的程序才没有给出代码链接，有兴趣的可以在自己机器上试验，记得要科学上网哦，要不然就会返回奇怪的结果了～&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>goroutines &#43; channel</title>
      <link>http://hzmangel.github.io/post/1230/</link>
      <pubDate>Mon, 17 Feb 2014 01:40:16 +0000</pubDate>
      
      <guid>http://hzmangel.github.io/post/1230/</guid>
      <description>

&lt;p&gt;&lt;code&gt;channel&lt;/code&gt;是golang里面一个比较有意思的东西，可以把它看成是一个semaphone（无缓存版队列）或者FIFO（有缓存版队列）。这篇文章只是把最
近用到的一些东西归纳了一下，就算是给自己留份存档吧。&lt;code&gt;channel&lt;/code&gt;是需要和&lt;code&gt;goroutines&lt;/code&gt;一起使用的。&lt;/p&gt;

&lt;h3 id=&#34;goroutines:c6debf44b3ea0c5c4c8edcf6064ff030&#34;&gt;goroutines&lt;/h3&gt;

&lt;p&gt;先说&lt;code&gt;goroutines&lt;/code&gt;吧。golang的并行模型使用的是&lt;strong&gt;CSP&lt;/strong&gt;（官方的说法是Newsqueak-Alef-Limbo，和原始的CSP有一些区别
。具体的差别还没有细查。参见&lt;a href=&#34;http://talks.golang.org/2012/concurrency.slide#10&#34;&gt;此张幻灯片&lt;/a&gt; ），&lt;code&gt;go
routines&lt;/code&gt;是一个独立运行的函数，不是进程也不是线程。它在被调用或者说被启动后直接返回，而不用等待函数运行结束。这是官方对它的一段&lt;a href=&#34;htt
p://talks.golang.org/2012/concurrency.slide#17&#34;&gt;简要介绍&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What is a goroutine? It&amp;rsquo;s an independently executing function, launched by a go statement.&lt;/li&gt;
&lt;li&gt;It has its own call stack, which grows and shrinks as required.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s very cheap. It&amp;rsquo;s practical to have thousands, even hundreds of thousands of goroutines.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s not a thread.&lt;/li&gt;
&lt;li&gt;There might be only one thread in a program with thousands of goroutines.&lt;/li&gt;
&lt;li&gt;Instead, goroutines are multiplexed dynamically onto threads as needed to keep all the goroutines running.&lt;/li&gt;
&lt;li&gt;But if you think of it as a very cheap thread, you won&amp;rsquo;t be far off.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;goroutines&lt;/code&gt;使用&lt;code&gt;go&lt;/code&gt;关键字来创建，创建完后就自己蹲一边运行去了，主程序也不用理它，自己往下走就行。光看文字太枯燥，读程序看输出吧。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import &amp;quot;fmt&amp;quot;
import &amp;quot;time&amp;quot;

func main() {
     fmt.Println(&amp;quot;START 1&amp;quot;)
     for i := 0; i &amp;lt; 3; i++ {
          foo(i)
     }
     fmt.Println(&amp;quot;END 1&amp;quot;)

     fmt.Println(&amp;quot;START 2&amp;quot;)
     for i := 0; i &amp;lt; 3; i++ {
          go foo(i)
     }
     fmt.Println(&amp;quot;END 2&amp;quot;)

     time.Sleep(1 * time.Second)
}

func foo(i int) {
     time.Sleep(1)
     fmt.Printf(&amp;quot;Call index: %d\n&amp;quot;, i)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;程序的输出是这样的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;START 1
Call index: 0
Call index: 1
Call index: 2
END 1
START 2
END 2
Call index: 0
Call index: 2
Call index: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码中，main函数中用了两个循环，分别调用了3次foo()函数，但是两次调用的方式不同，第一次是直接调用，第二次是使用了&lt;code&gt;goroutine&lt;/code&gt;。从输出看，
前段输出是正常的顺序，但是从后一段输出可以看到，函数调用是立即返回的，而且最后输出的顺序也是不固定的（3次可能看到的效果比较一致，如果把循环次数涨到10次就
能看的比较明显了）。在第19行加上sleep的原因是因为main函数在执行完成后会直接退出，不会等待所有&lt;code&gt;goroutines&lt;/code&gt;执行完毕，所以此处需要让程序
等一等。不方便在本机运行的可以&lt;a href=&#34;http://play.golang.org/p/8BT6RVR7-w&#34;&gt;见这里&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;channel:c6debf44b3ea0c5c4c8edcf6064ff030&#34;&gt;channel&lt;/h3&gt;

&lt;p&gt;拿sleep来不让main函数退出也不是个事，毕竟时间不好控制，睡多了睡少了都不行，所以这里就引入了&lt;code&gt;channel&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;就像文章开头说的，&lt;code&gt;channel&lt;/code&gt;就像是一个Semaphore或者FIFO的东西，更通俗的理解就是一条传送带或者管道，用于在&lt;code&gt;goroutines&lt;/code&gt;之间传
递消息。把上面的程序改一下（顺序调用的那个就先去掉了，减少长度，嗯）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import &amp;quot;fmt&amp;quot;

func main() {

    foo_channel := make(chan string)
    channel_cnt := 3

    fmt.Println(&amp;quot;Start launching goroutines&amp;quot;)
    for i := 0; i &amp;lt; channel_cnt; i++ {
        go foo(i, foo_channel)
    }
    fmt.Println(&amp;quot;Finish launching goroutines&amp;quot;)

    for i := 0; i &amp;lt; channel_cnt; i++ {
        fmt.Println(&amp;lt;-foo_channel)
    }

}

func foo(i int, foo_channel chan string) {
    s := fmt.Sprintf(&amp;quot;Call index %d&amp;quot;, i)
    foo_channel &amp;lt;- s
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输入是介样的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Start launching goroutines
Finish launching goroutines
Call index 0
Call index 1
Call index 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个程序里面，没有使用&lt;code&gt;sleep&lt;/code&gt;，程序也做到了等待所有&lt;code&gt;goroutines&lt;/code&gt;运行完毕才退出这个需求，就是靠的main函数最后几行。blog上不知道为
啥没有行号了，程序&lt;a href=&#34;http://play.golang.org/p/FzHuVttKnB&#34;&gt;见这里吧&lt;/a&gt;。在第7行的时候创建了一个&lt;code&gt;channel&lt;/code&gt;，并把它
传入了&lt;code&gt;foo()&lt;/code&gt;函数中。而&lt;code&gt;foo()&lt;/code&gt;函数也不像上一次那样直接输出一个字符串，它把需要输出的字符串放到一个string中，然后通过&lt;code&gt;channel&lt;/code&gt;传
了回来。这条语句：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;foo_channel &amp;lt;- s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;做的就是把字符串放到channel中去，而这条语句：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fmt.Println(&amp;lt;-foo_channel)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;做的就是把字符串从channel中取出来，并把它输出。在执行&lt;code&gt;&amp;lt;-channel&lt;/code&gt;时，如果&lt;code&gt;channel&lt;/code&gt;上没有数据，这条语句是被blocked的，这样就
能保证main函数在所有goroutines执行完毕前不会退出。而且当channel中有值而且没有被读出的时候，对这个channel的写操作也是被block
ed的。这时的channel，可以用来做Semaphore。&lt;/p&gt;

&lt;p&gt;上面介绍的channel是没有缓存的，golang中还有一种channel是可以加上缓存的，&lt;a href=&#34;htt
p://talks.golang.org/2012/concurrency.slide#22A&#34;&gt;官方文档说这个就像是Erlang的mailboxes&lt;/a&gt;，因为不懂Erlang，所以只好把话贴在这以后两手一摊。目前
在项目中还没有用到带缓存的channel，暂时先不写了，估计在不远的将来还会有一篇关于&lt;code&gt;select&lt;/code&gt;的文章，目前时间未定，因为还没用到～&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Save base64 image with carrierwave and bootstrap-wysiwyg</title>
      <link>http://hzmangel.github.io/post/1224/</link>
      <pubDate>Sun, 01 Dec 2013 15:03:37 +0000</pubDate>
      
      <guid>http://hzmangel.github.io/post/1224/</guid>
      <description>

&lt;h3 id=&#34;tl-dr:5ab4430131db8e4e631503ef790e7c02&#34;&gt;tl;dr&lt;/h3&gt;

&lt;p&gt;The pasted image will be converted to base64 encoded format, which will hit
response size limitation of server. This article is talking about save image
to file with carrierwave.&lt;/p&gt;

&lt;p&gt;The source code is available at &lt;a href=&#34;https://github.com/hzmangel/base64_image_carrierwave&#34;&gt;github
repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Next is the full version.### The problem&lt;/p&gt;

&lt;p&gt;Recently I have faced a task to upload a image by a rich format text editor.
The web server is Rails, so I selected
&lt;a href=&#34;https://github.com/carrierwaveuploader/carrierwave&#34;&gt;carrierwave&lt;/a&gt; as the
upload gem, and &lt;a href=&#34;http://mindmup.github.io/bootstrap-
wysiwyg/&#34;&gt;bootstrap-wysiwyg&lt;/a&gt; as rich format text editor.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bootstrap-wysiwyg&lt;/code&gt; supports inserting image into edit area, and uploaded
image via base64. Every thing is okay in development enviromnent, but I have
met problem while deploying to production server.&lt;/p&gt;

&lt;p&gt;The root cause of the problem is response size exceed the max limitation. The
uploaded base64 encoded image are saved as string, and will be returned in
response body. I have tried increasing response body size limitation but take
no effect, so I switched to method that saving image to file.&lt;/p&gt;

&lt;h3 id=&#34;solution:5ab4430131db8e4e631503ef790e7c02&#34;&gt;Solution&lt;/h3&gt;

&lt;p&gt;This section only shows how to get image and save via carrierwave, please
refer to the source of the other contents.&lt;/p&gt;

&lt;p&gt;The sample project is a simple post manage system, each post contains &lt;code&gt;title&lt;/code&gt;
and &lt;code&gt;content&lt;/code&gt; field, and the &lt;code&gt;content&lt;/code&gt; field is rich format text.&lt;/p&gt;

&lt;p&gt;The passed in base64 encoded image is started with this string:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data:image/jpeg;base64,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then following the image data.&lt;/p&gt;

&lt;p&gt;The image uploaded is surrounded by &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tag, so I added a pre processing
to the content uploaded. The logic is simple: save found &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tag to a file
with carrierwave, and replace the base64 data to file path. The primary code
is here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def process_base64_content content
  return if content.nil?
  return content if not content.match /&amp;lt;/
  rslt = &#39;&#39;
  content.split(&amp;quot;&amp;lt;&amp;quot;).each do |elem_str|
    if elem_str[0..2] == &amp;quot;img&amp;quot;
      if elem_str.match(%r{data:(.*?);(.*?),(.*?)&amp;quot;&amp;gt;$})
        img_data = {
          :type =&amp;gt;      $1, # &amp;quot;image/png&amp;quot;
          :encoder =&amp;gt;   $2, # &amp;quot;base64&amp;quot;
          :data_str =&amp;gt;  $3, # data string
          :extension =&amp;gt; $1.split(&#39;/&#39;)[1] # &amp;quot;png&amp;quot;
        }

        other_img = PostImage.new
        img_data_str = img_data[:data_str]
        img_data_sio = CarrierStringIO.new(Base64.decode64(img_data_str))
        other_img.image = img_data_sio
        other_img.save
        rslt += view_context.image_tag(other_img.image.url)
      else
        rslt += &amp;quot;&amp;lt;#{elem_str}&amp;quot; if not elem_str.empty?
      end
    else
      rslt += &amp;quot;&amp;lt;#{elem_str}&amp;quot; if not elem_str.empty?
    end
  end

  rslt
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;PostImage&lt;/code&gt; is a model used for saving image. &lt;code&gt;CarrierStringIO&lt;/code&gt; is also a user
defined class to provide functions &lt;code&gt;original_filename&lt;/code&gt; and &lt;code&gt;content_type&lt;/code&gt;,
which are required by &lt;code&gt;carrierwave&lt;/code&gt;. Here is the definition of this class:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class CarrierStringIO &amp;lt; StringIO
  def original_filename
    # the real name does not matter
    &amp;quot;image.jpeg&amp;quot;
  end

  def content_type
    # this should reflect real content type, but for this example it&#39;s ok
    &amp;quot;image/jpeg&amp;quot;
  end
end

class PostImage
  include Mongoid::Document
  include Mongoid::Timestamps

  def image_data=(data)
    sio = CarrierStringIO.new(Base64.decode64(data))
    self.image = sio
  end

  mount_uploader :image, PostImageUploader
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last thing is the carrierwave uploader: &lt;code&gt;PostImageUploader&lt;/code&gt;. This is a
simple uploader that only save the image to file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# encoding: utf-8

class PostImageUploader &amp;lt; CarrierWave::Uploader::Base

  storage :file

  def store_dir
    &amp;quot;uploads/#{model.class.to_s.underscore}/#{mounted_as}/#{model.id}&amp;quot;
  end

end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the sample project, the file type and file name are hard coded in
&lt;code&gt;CarrierStringIO&lt;/code&gt;, please feel free to modify code as needed.&lt;/p&gt;

&lt;p&gt;Note: There is an bug of the code: The image can&amp;rsquo;t be extracted out if
inserted into a text paragraph. I will fix this once I have time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ajax loading multi series to jqPlot</title>
      <link>http://hzmangel.github.io/post/1220/</link>
      <pubDate>Sun, 14 Jul 2013 11:21:07 +0000</pubDate>
      
      <guid>http://hzmangel.github.io/post/1220/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;http://www.jqplot.com/tests/data-renderers.php&#34;&gt;Ajax example of jqPlot&lt;/a&gt;
only shows how to plot on series data, but the request I have met needs multi
series. The solution is easy, just record here for later reference.&lt;/p&gt;

&lt;p&gt;In the example, The function used to load ajax data is &lt;code&gt;ajaxDataRenderer&lt;/code&gt;,
which returns array of data. For multi series, just return more than one data
array. Here is a sample data set:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[
  [
    [1,1],[2,2],[3,3],[4,4],[5,5]
  ],
  [
    [5,1],[4,2],[3,3],[2,4],[1,5]
  ]
]
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>