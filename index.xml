<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>湖间小筑</title>
    <link>http://hzmangel.github.io/</link>
    <description>Recent content on 湖间小筑</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>Copyright - ©2015 - hzmangel</copyright>
    <lastBuildDate>Wed, 10 Aug 2016 00:07:40 +0800</lastBuildDate>
    <atom:link href="http://hzmangel.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Rails - Fix ActiveModel::ForbiddenAttributesError in grape
</title>
      <link>http://hzmangel.github.io/post/grape_n_strong_parameter/</link>
      <pubDate>Wed, 10 Aug 2016 00:07:40 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/grape_n_strong_parameter/</guid>
      <description>&lt;p&gt;最近尝试在 &lt;em&gt;Grape API&lt;/em&gt; 中创建一个 &lt;em&gt;ActiveModel&lt;/em&gt; ，但是在使用 &lt;code&gt;new&lt;/code&gt; 创建的时候发现会报 &lt;code&gt;ActiveModel::ForbiddenAttributesError&lt;/code&gt; 错误。想了想估计是碰上了 &lt;em&gt;stronng parameters&lt;/em&gt; 的问题。按照之前的经验，那就是给参数加上一个permit，但是发现在调用 &lt;code&gt;permit!&lt;/code&gt; 之后，grape的params变成了空字典，从而造成后续的创建出错。在网上找了一圈，结果发现 grape 自己的Github页面上就有说明，如果需要和 Rails 4 一起使用，需要加上 &lt;em&gt;hashie-forbidden_attributes&lt;/em&gt; gem。&lt;/p&gt;

&lt;p&gt;细看了眼原因，因为在Rails中，默认传递的params类型为 &lt;code&gt;ActionController::Parameters&lt;/code&gt; ，而在这个类下可以直接调用 &lt;code&gt;permit&lt;/code&gt; 或 &lt;code&gt;permit!&lt;/code&gt; 的方法来完成参数设置，同时会提供 &lt;code&gt;permitted?&lt;/code&gt; 函数来检查参数是否有效。而 grape 默认传入的参数类型是 &lt;code&gt;Hashie::Mash&lt;/code&gt; ，并没有提供 &lt;code&gt;permit&lt;/code&gt; 函数。好在 Ruby 支持 &lt;em&gt;Monkey Patch&lt;/em&gt; 所以新增的这个 gem 在 &lt;code&gt;Hashie::Mash&lt;/code&gt; 类中加上了对函数 &lt;code&gt;:permitted?&lt;/code&gt; 的响应，从而解决了这个问题。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rails - Reload Parent Page in iframe
</title>
      <link>http://hzmangel.github.io/post/rails_reload_parent_in_iframe/</link>
      <pubDate>Sun, 17 Apr 2016 12:59:42 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/rails_reload_parent_in_iframe/</guid>
      <description>&lt;p&gt;最近碰到的一个问题，弹出的 iframe 窗口在做完操作并把结果返回给 controller 后，调用 &lt;code&gt;render&lt;/code&gt; 或 &lt;code&gt;redirect_to&lt;/code&gt; 时都只会刷新 iframe 中的内容，而不会将整个页面都刷新。尝试在 iframe 中提交表单时关闭 iframe 窗口，但是依然无效。最后发现还是需要借助于 Javascript ，最后的解决方案如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class FooController &amp;lt; ApplicationController
  def parent_reload
    render text: &amp;quot;&amp;lt;script&amp;gt;window.parent.location.reload();&amp;lt;/script&amp;gt;&amp;quot;
  end

  def parent_redirect_to(url)
    render text: &amp;quot;&amp;lt;script&amp;gt;window.parent.location.href=&#39;#{url}&#39;;&amp;lt;/script&amp;gt;&amp;quot;
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是直接把 Javascript 以 HTML 的形式返回给浏览器端，浏览器端在收到内容后自动执行其中的脚本。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Integrate Rails with Elasticsearch - Indexing
</title>
      <link>http://hzmangel.github.io/post/rails_elasticsearch_indexing/</link>
      <pubDate>Tue, 23 Feb 2016 00:52:42 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/rails_elasticsearch_indexing/</guid>
      <description>

&lt;p&gt;之前写Rails在查找这块一般都是用DB内置的查询，不过上次试了下用 Elasticsearch ，比之前想像的要简单，记点东西在这吧。&lt;/p&gt;

&lt;p&gt;这篇东西会包含下面几项内容：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;安装 Elasticsearch&lt;/li&gt;
&lt;li&gt;关联 Rails 与 Elasticsearch&lt;/li&gt;
&lt;li&gt;配置索引内容&lt;/li&gt;
&lt;li&gt;Custom Analyzer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本文没有完整系统的介绍，更多的只是一些使用技巧。详细说明请参见[官方文档]()。&lt;/p&gt;

&lt;h2 id=&#34;安装elasticsearch&#34;&gt;安装Elasticsearch&lt;/h2&gt;

&lt;p&gt;两种方法，从官网下编译好的&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html&#34;&gt;二进制解压&lt;/a&gt;，或者用&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/master/setup-repositories.html&#34;&gt;操作系统的安装源&lt;/a&gt;。如果是OSX，可以使用 &lt;em&gt;homebrew&lt;/em&gt; 安装，命令为 &lt;code&gt;brew install elasticsearch&lt;/code&gt; ，安装后的启动可以使用 &lt;code&gt;brew info elasticsearch&lt;/code&gt; 查看。&lt;/p&gt;

&lt;p&gt;安装并启动服务后，可以用下面的命令查看系统是否成功启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -X GET &#39;http://localhost:9200&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果一切正常，会返回服务器的状态信息。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;关联-rails-与-elasticsearch&#34;&gt;关联 Rails 与 Elasticsearch&lt;/h2&gt;

&lt;h3 id=&#34;gem&#34;&gt;Gem&lt;/h3&gt;

&lt;p&gt;Elasticsearch提供了两个Gem用于和Rails集成，分别是 &lt;a href=&#34;https://github.com/elastic/elasticsearch-rails&#34;&gt;&lt;em&gt;elasticsearch-rails&lt;/em&gt;&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/elastic/elasticsearch-rails/tree/master/elasticsearch-model&#34;&gt;&lt;em&gt;elasticsearch-model&lt;/em&gt;&lt;/a&gt; 。其中， &lt;em&gt;elasticsearch-rails&lt;/em&gt; 为 Rails 的模块加入了 Elasticsearch 的功能，而 &lt;em&gt;elasticsearch-model&lt;/em&gt; 则为 Ruby 的类提供了一些简化的连接 &lt;em&gt;Elasticsearch&lt;/em&gt; 的方法，使得相应的类可以通过 &lt;code&gt;__elasticsearch__&lt;/code&gt; 直接访问 &lt;em&gt;Elasticsearch&lt;/em&gt; 服务器的资源。&lt;/p&gt;

&lt;h3 id=&#34;model-enable-indexes-in-elasticsearch&#34;&gt;Model - Enable indexes in Elasticsearch&lt;/h3&gt;

&lt;p&gt;由于 Elasticsearch 是一个单独的服务器，所以在使用前需要考虑以下几点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;有哪些Model需要被索引&lt;/li&gt;
&lt;li&gt;每个Model有哪些字段需要被索引&lt;/li&gt;
&lt;li&gt;如何在数据库中记录更新后同步 Elasticsearch 的索引&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果不使用 Elasticsearch 提供的 gem，需要手动去完成上面的几个步骤，但是上面提到的两个 gem 提供了一系列方便使用的函数。代码的东西对着说比较好弄，所以这里有一个 [Demo Project]()，对着来说吧。&lt;/p&gt;

&lt;p&gt;这个app就是一个简单的blog，由于只是为了介绍 Elasticsearch ，所以只引入了最简单的部分。&lt;/p&gt;

&lt;p&gt;在 &lt;code&gt;app/models/blog.rb&lt;/code&gt; 中可以看到，需要索引一个 model 需要三步：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在代码中加入 Elasticsearch 的模块。&lt;/li&gt;
&lt;li&gt;设置需要索引的模块。&lt;/li&gt;
&lt;li&gt;导入数据至 Elasticsearch 。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对于需要索引的Model，只需要在代码中加上几行即可以调用 Elasticsearch 的功能。可以在 &lt;code&gt;app/models/blog.rb&lt;/code&gt; 中看到下面的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;elasticsearch/model&#39;

class Blog &amp;lt; ActiveRecord::Base
  include Elasticsearch::Model
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在加入了这几行代码后，这个Model就可以通过 &lt;code&gt;__elasticsearch__&lt;/code&gt; 来调用Elasticsearch的API了。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;Blog.__elasticsearch__.client.cluster.health
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，只是在Blog中导入了一些Elasticsearch相关的配置，但是还没有将具体的数据导入到Elasticsearch服务器中。可以使用下面的命令导入数据：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;Blog.import force: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者逐步完成：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;Blog.__elasticsearch__.create_index! force: true
Blog.__elasticsearch__.refresh_index!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是由于 Elasticsearch 的索引是和model分开存放的，所以在每次数据有更新时，需要手动更新所更新文档的索引。如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;Blog.first.__elasticsearch__.index_document
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不过Elasticsearch提供了Callback的方式来处理这些操作，只需要在Model中加入如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;include Elasticsearch::Model::Callbacks
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;即可在每次数据有更新时，自动更新索引。在Gem的文档中还有一些更详细的用法，如异步索引，自定义索引等供查阅。&lt;/p&gt;

&lt;p&gt;在数据导入完成后即可尝试在 Elasticsearch 查找导入的数据了：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;Blog.search(&#39;*&#39;).records.records
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;model-choose-indexed-fields&#34;&gt;Model - Choose indexed fields&lt;/h3&gt;

&lt;p&gt;默认情况下，所有字段都会被加入 Elasticsearch 的索引。但是在实际使用情况中，可能会碰到只索引某些数据的场景。 Elasticsearch 提供了一个函数用来指定需要索引的字段以及对应的值。函数的名称是 &lt;code&gt;as_indexed_json&lt;/code&gt; 。这个函数是在 &lt;code&gt;Elasticsearch::Model::Serializing&lt;/code&gt; 定义的，如果需要定制，直接在Model中重新定义自己的函数就好。如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;elasticsearch/model&#39;

class Blog &amp;lt; ActiveRecord::Base
  include Elasticsearch::Model

  def as_indexed_json(options = {})
    as_json(
      only: [:title, :content]
    )
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上面的例子中，无论我的Blog记录中还有什么字段，只有 &lt;em&gt;title&lt;/em&gt; 和 &lt;em&gt;content&lt;/em&gt; 字段会被放入 Elasticsearch 的索引中，也只有这两个字段会返回在查询的结果中。除了内置字段外，还支持将函数中方法的返回值也加入索引中，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;elasticsearch/model&#39;

class Blog &amp;lt; ActiveRecord::Base
  include Elasticsearch::Model

  def as_indexed_json(options = {})
    as_json(
      only: [:title, :content],
      methods: [:author_name, :tag_count],
    )
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时在索引的字段和返回的结果中，会多出 &lt;em&gt;author_name&lt;/em&gt; 以及 &lt;em&gt;tag_count&lt;/em&gt; 的返回值。这种方式可以指定关联记录的索引信息。除此之外，还可以在 &lt;code&gt;include&lt;/code&gt; 中使用嵌套的字段来标明关系记录。例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;elasticsearch/model&#39;

class Blog &amp;lt; ActiveRecord::Base
  include Elasticsearch::Model

  def as_indexed_json(options = {})
    as_json(
      only: [:title, :content],
      include: { author: {only: :name}, tag: {methods: [:count]} }
    )
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，基本的索引构建应该差不多了，在调用过 &lt;em&gt;import&lt;/em&gt; 后，就可以用基本的查询功能去查找了。&lt;/p&gt;

&lt;h2 id=&#34;analyzer-和-mapping&#34;&gt;&lt;em&gt;Analyzer&lt;/em&gt; 和 &lt;em&gt;Mapping&lt;/em&gt;&lt;/h2&gt;

&lt;h3 id=&#34;analyzer&#34;&gt;&lt;em&gt;Analyzer&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;对每一条被索引的内容， ES 都会通过 &lt;em&gt;Analyzer&lt;/em&gt; 把内容分词后放入服务器（因为ES中是使用 &lt;a href=&#34;https://zh.wikipedia.org/wiki/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95&#34;&gt;反向索引&lt;/a&gt; 来查找的，而反向索引的 &lt;em&gt;token&lt;/em&gt; 就是输入文字经过 &lt;em&gt;Analyzer&lt;/em&gt; 处理的结果）。而在查询时，如果是全文检索，那么 ES 会将和分析内容时使用的相同的 &lt;em&gt;Analyzer&lt;/em&gt; 应用于检索词上，并将分词后的检索词用于查找以提高查找的准确性。而如果查询的类型是精确匹配， ES 将不会处理检索词。&lt;/p&gt;

&lt;p&gt;根据 &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/guide/current/analysis-intro.html&#34;&gt;文档&lt;/a&gt; 的介绍，每一个 &lt;em&gt;Analyzer&lt;/em&gt; 由 &lt;strong&gt;3&lt;/strong&gt; 步组成，分别是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Character filters&lt;/em&gt; : 对输入的内容按字符处理，包含有 &lt;em&gt;Mapping Char Filter&lt;/em&gt;, &lt;em&gt;HTML Strip Char Filter&lt;/em&gt; 和 &lt;em&gt;Pattern Replace Char Filter&lt;/em&gt; ，具体介绍请参阅 &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-charfilters.html&#34;&gt;文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Tokenizer&lt;/em&gt; : 分词器，根据不同的规则，将输入内容分为不同的 token ，&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html&#34;&gt;文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Token filters&lt;/em&gt; : 应用于上面分割开的 &lt;em&gt;token&lt;/em&gt; 上的 filter。 &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html&#34;&gt;文档&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ES 还提供了一些内置的 &lt;em&gt;Analyzer&lt;/em&gt; ，可以在 &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html&#34;&gt;这里&lt;/a&gt; 查到。由于 Rails 中的代码实例会牵涉到 &lt;em&gt;Mapping&lt;/em&gt; 相关的内容，所以会在下一节中贴代码。&lt;/p&gt;

&lt;h3 id=&#34;mapping&#34;&gt;&lt;em&gt;Mapping&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;对于输入的文本内容，ES会默认将此字段映射为 &lt;em&gt;string&lt;/em&gt; 类型，并使用 &lt;em&gt;standard analyzer&lt;/em&gt; ，如果需要一些特殊处理，就需要引入 &lt;em&gt;Mapping&lt;/em&gt; 了。 &lt;em&gt;Mapping&lt;/em&gt; 的作用是告诉 ES 某个字段需要按什么样的规则处理。例如一个手机号， ES 的默认处理是按长整形，但是可能需要按字符串处理，这时就需要指定 &lt;em&gt;Mapping&lt;/em&gt; 了。&lt;/p&gt;

&lt;p&gt;在 Rails 的 Model 中，可以通过下面的方法来设置字段的 &lt;em&gt;Analyzer&lt;/em&gt; 和 &lt;em&gt;Mapping Type&lt;/em&gt; ，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;settings do
  mappings do
    indexes :title, analyzer: &#39;english&#39;
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在上述的例子中，原本 &lt;em&gt;title&lt;/em&gt; 字段的 &lt;em&gt;analyzer&lt;/em&gt; 被指定为 &lt;em&gt;english&lt;/em&gt; 而不是默认的 &lt;em&gt;string&lt;/em&gt; 。&lt;/p&gt;

&lt;p&gt;Q: 为什么设定了type后，import返回值为1，而不设的时候返回是0？&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Use Different Auth Method In Rails API Controller
</title>
      <link>http://hzmangel.github.io/post/different_auth_method_in_rails_api/</link>
      <pubDate>Wed, 27 Jan 2016 23:21:42 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/different_auth_method_in_rails_api/</guid>
      <description>&lt;p&gt;开发的时候碰到一个问题，Rails的controller是用devise来提供认证的，如果用户在访问时没有有效的cookie就会被转到登录界面。但是在API的时候不能用cookie，所以需要分开做验证。&lt;/p&gt;

&lt;p&gt;基本想法是对于网页端的请求，继续使用devise进行认证，而对于某些同时提供API接口的Action，则使用token来验证。除此之外，对于POST请求，还需要跳过CSRF token的检查。基本上代码就是这样的（为了测试方便，使用了自己定义的&lt;code&gt;before_action&lt;/code&gt; ，同时设置所有的检查都基于请求中的 &lt;em&gt;token&lt;/em&gt; 字段。）&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class WelcomeController &amp;lt; ApplicationController
  before_action :auth_user!, only: [:foo], unless: -&amp;gt; { request.format.json? }
  before_action :auth_by_token!, if: -&amp;gt; { request.format.json? }

  def foo
    render json: params
  end

  def bar
    render json: params
  end

  private

  def auth_user!
    head :forbidden if params[:token] != &#39;24&#39;
  end

  def auth_by_token!
    head :forbidden if params[:token] != &#39;42&#39;
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时，如果使用浏览器访问 &lt;em&gt;foo&lt;/em&gt; , &lt;code&gt;request.format&lt;/code&gt; 为 &lt;em&gt;text/html&lt;/em&gt; ，此时就会调用 &lt;code&gt;auth_user!&lt;/code&gt; ，而访问 &lt;em&gt;bar&lt;/em&gt; 时，即使是使用浏览器，也不会调用 &lt;code&gt;auth_user!&lt;/code&gt; 。而对于API请求，统一都会调用 &lt;code&gt;auth_by_token!&lt;/code&gt; 进行邓处理。&lt;/p&gt;

&lt;p&gt;下面是一个简单的测试用例&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &amp;quot;rails_helper&amp;quot;

RSpec.describe WelcomeController, :type =&amp;gt; :controller do
  describe &#39;GET #foo.html&#39; do
    it &#39;responds forbidden if not given valid token&#39; do
      get :foo
      expect(response).to have_http_status(:forbidden)
    end

    it &#39;responds forbidden if given wrong token&#39; do
      get :foo, {token: 42}
      expect(response).to have_http_status(:forbidden)
    end

    # auth_user! is invoked
    it &#39;responds success if given correct token&#39; do
      get :foo, {token: 24}
      expect(response).to have_http_status(:ok)
    end
  end

  describe &#39;GET #foo.json&#39; do
    before :each do
      request.env[&amp;quot;HTTP_ACCEPT&amp;quot;] = &#39;application/json&#39;
    end

    it &#39;responds forbidden if not given valid token&#39; do
      get :foo
      expect(response).to have_http_status(:forbidden)
    end

    it &#39;responds forbidden if given wrong token&#39; do
      get :foo, {token: 24}
      expect(response).to have_http_status(:forbidden)
    end

    # auth_by_token! is invoked
    it &#39;responds success if given correct token&#39; do
      get :foo, {token: 42}
      expect(response).to have_http_status(:ok)
    end
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在老版本的Rails上，有一个&lt;a href=&#34;https://github.com/rails/rails/issues/9703&#34;&gt;bug#9703&lt;/a&gt;，就是它的判断会把 if/unless 和 only/except 分开处理。在这种情况下需要根据 &lt;a href=&#34;https://github.com/rails/rails/issues/9703#issuecomment-15830313&#34;&gt;此条评论&lt;/a&gt; 中的内容对代码做相应改动。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop Study Log 1: Env Setup
</title>
      <link>http://hzmangel.github.io/post/hadoop_study_log_1_env_setup/</link>
      <pubDate>Sun, 17 Jan 2016 07:22:32 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/hadoop_study_log_1_env_setup/</guid>
      <description>

&lt;p&gt;最近想折腾数据，于是决定从基础的Hadoop开始。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2016.01.22: 按照&lt;a href=&#34;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html&#34;&gt;Hadoop官方wiki&lt;/a&gt;配置的环境在尝试运行程序时会报连不上Node的错误，不知道是哪里的问题。后来在&lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-on-ubuntu-13-10&#34;&gt;DigitalOcean的文档&lt;/a&gt;中发现了需要改Namenode和Datanode的配置才可以。已经将文中的链接和所需修改的文件列表更新。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;基本概念&#34;&gt;基本概念&lt;/h2&gt;

&lt;p&gt;这里只是一些我之前可能弄混淆的概念，其它的一些东西可能需要去hadoop官网看了。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Hadoop是一个编程框架或者叫库，主要用于下面类型程序的编写

&lt;ul&gt;
&lt;li&gt;大规模数据集&lt;/li&gt;
&lt;li&gt;分布式&lt;/li&gt;
&lt;li&gt;简单任务模型&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Hadoop的核心项目&lt;a href=&#34;https://hadoop.apache.org/&#34;&gt;包含下面几个&lt;/a&gt;：

&lt;ul&gt;
&lt;li&gt;Common: 一些通用组件，工具类&lt;/li&gt;
&lt;li&gt;HDFS：分布式文件系统&lt;/li&gt;
&lt;li&gt;YARN：任务调度及资源管理&lt;/li&gt;
&lt;li&gt;MapReduce：并行编程模型&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;还有一些衍生项目，具体&lt;a href=&#34;https://hadoop.apache.org/&#34;&gt;参见网页&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;环境配置&#34;&gt;环境配置&lt;/h2&gt;

&lt;p&gt;鉴于之前在osx上装东西碰上过坑，所以选择了virtualbox+ubuntu的方案。系统的版本是Ubuntu 15.05，Hadoop选择的是2.6.3（截止写文章的时候它的Release Date最新）。具体步骤如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;下载及安装&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Download package from Hadoop site
$ wget http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.6.3/hadoop-2.6.3.tar.gz

# Decompress it
$ tar zxvf hadoop-2.6.3.tar.gz

# Move the package files to system directory, and change owner
$ sudo mv hadoop-2.6.3 /opt/hadoop
$ sudo chown -R vagrant:vagrant /opt/hadoop
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置服务（Single Node）&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Export JAVA_HOME env variable
$ export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并根据&lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-on-ubuntu-13-10&#34;&gt;此页面&lt;/a&gt;中的内容，修改相应文件并配置SSH服务。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;etc/hadoop/core-site.xml&lt;/p&gt;

&lt;p&gt;Hadoop启动时的设置，上面那个页面中给出的是 &lt;code&gt;fs.default.name&lt;/code&gt; ，不过在新的配置中，这个字段应该变成 &lt;code&gt;fs.defaultFS&lt;/code&gt; 。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;etc/hadoop/yarn-site.xml&lt;/p&gt;

&lt;p&gt;一些MapReduce的设置选项。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;etc/hadoop/mapred-site.xml&lt;/p&gt;

&lt;p&gt;配置使用Yarn作为ResManager&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;etc/hadoop/hdfs-site.xml&lt;/p&gt;

&lt;p&gt;配置Namenode和Datanode的路径信息&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;格式化namenode&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ bin/hdfs namenode -format
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;设置&lt;code&gt;etc/hadoop/hadoop-env.sh&lt;/code&gt;中的&lt;code&gt;JAVA_HOME&lt;/code&gt;变量&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;开启dfs服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sbin/start-dfs.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;至此，一个运行在单节点环境的Hadoop环境就ok了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Go generate study
</title>
      <link>http://hzmangel.github.io/post/go-generate-struct-to-schema/</link>
      <pubDate>Mon, 07 Dec 2015 21:15:45 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/go-generate-struct-to-schema/</guid>
      <description>

&lt;p&gt;最近在折腾用Golang弄DB，定义完了 &lt;code&gt;struct&lt;/code&gt; 后发现好像没有 ORM 可以把这个 &lt;code&gt;struct&lt;/code&gt; 给映射到某张表上，所以需要：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;手动创建表结构，包括折腾表名和数据结构&lt;/li&gt;
&lt;li&gt;同步代码中的字段和表结构&lt;/li&gt;
&lt;li&gt;如果要换DB还得再来一次&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;于是，开始找有没有像 Rails 中一样的生成器。&lt;/p&gt;

&lt;p&gt;最开始是想用一个脚本解析struct，生成相应的代码，但是在查文档的时候发现golang在1.4版本中引入了 &lt;a href=&#34;https://golang.org/doc/go1.4#gogenerate&#34;&gt;generate&lt;/a&gt; 命令，它可以解析一个文件并生成另一个文件。官方给的说明里面是从yacc的语法生成go文件。想了想觉得它可以完成生成SQL命令，同步struct和SQL的功能，而且如果做了处理的话还可以方便的在DB间切换，那就试试吧。&lt;/p&gt;

&lt;h2 id=&#34;语法&#34;&gt;语法&lt;/h2&gt;

&lt;p&gt;在介绍如何编写前，先介绍下调用的方式。要成功调用一次generate需要两个条件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;已经安装的generator&lt;/li&gt;
&lt;li&gt;在需要处理的代码中加上 &lt;code&gt;//go:generate&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样在文件所在的目录下运行 &lt;code&gt;go generate&lt;/code&gt; 即可根据注释中所指示的方法，调用相应的生成器了。&lt;/p&gt;

&lt;h2 id=&#34;编写&#34;&gt;编写&lt;/h2&gt;

&lt;p&gt;Golang给出的sample是一个字符串相关的东西，具体可以见 &lt;a href=&#34;https://godoc.org/golang.org/x/tools/cmd/stringer&#34;&gt;文档页&lt;/a&gt;。&lt;a href=&#34;http://www.onebigfluke.com/2014/12/generic-programming-go-generate.html&#34;&gt;另一个页面&lt;/a&gt; 也有一份具体的实现可以参考。它主要的思路就是用 &lt;code&gt;generate&lt;/code&gt; 去处理某个 &lt;code&gt;.go&lt;/code&gt; 文件，然后生成一些新的东西。主要是用内置的 &lt;em&gt;AST&lt;/em&gt; 解析文件，并生成相应的东西。&lt;/p&gt;

&lt;h2 id=&#34;我的实现&#34;&gt;我的实现&lt;/h2&gt;

&lt;p&gt;回到最开始的问题，我有一个定义好的struct，需要去生成SQL命令。所以还是需要用到内置的AST解析器的。除此之外也就是一点点的区分类型型，做转换了。代码放在 &lt;a href=&#34;https://github.com/hzmangel/struct2schema&#34;&gt;https://github.com/hzmangel/struct2schema&lt;/a&gt; 这里，README还在编写中。目前代码支持将大部分Go内置的类型转换为SQL类型，不过可能还是有一些不支持的需要手动操作。在DB支持上，目前只看了sqlite3和MySQL。下一步要做的事情估计有这些：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;支持更多的DB类型，如PostgreSQL， MSSQL 等（Mongo这种无结构的就不需要了&amp;hellip;&amp;hellip;）&lt;/li&gt;
&lt;li&gt;现在生成的SQL命令中，字段名和Go中的变量名一致，都是Camel String，拟在解析的时候将其变为下划线小写的方式来生成SQL命令（考虑从后面的json字段获取小写名）&lt;/li&gt;
&lt;li&gt;支持更多的Go数据类型&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前生成出来的代码会直接打印到屏幕上，格式啥的，据说1.6上对模板会有一个更新， &lt;a href=&#34;https://github.com/golang/go/issues/9969&#34;&gt;讨论在此&lt;/a&gt; ，拭目以待。&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;在写这篇文章的时候，突然反应过来这个 &lt;code&gt;go generate&lt;/code&gt; 就是会调用在注释中写的命令，根据给定的参数处理。所以理论上说可以给定任何文件，所以还是有不少可玩性的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W3 - Communities in Social Network (Basic)
</title>
      <link>http://hzmangel.github.io/post/mmds-w3-communities-in-social-networks-basic/</link>
      <pubDate>Wed, 11 Nov 2015 23:53:33 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w3-communities-in-social-networks-basic/</guid>
      <description>

&lt;p&gt;第三周分两部分，第一部分是 &lt;em&gt;Communities in Social Network&lt;/em&gt; 。是介绍如何在社交网络中给用户分组的。这一部分的课也分为基础和高级，这一篇是基础， 高级的课程另开一篇吧（主要是基础中还有些东西没完全弄明白&amp;hellip;）。&lt;/p&gt;

&lt;p&gt;社交网络包含有用户和用户之间的联系。把用户看成顶点，把用户之间的联系看成边，就可以得到一个图 (&lt;em&gt;social graph&lt;/em&gt;)。像 Facebook 中的图就是无向图，而 Twitter/G+ 中的就是有向图。在 &lt;em&gt;Network&lt;/em&gt; 和 &lt;em&gt;Communities&lt;/em&gt; 这块，主要的任务有两类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;给定一个模型，怎么去生成网络&lt;/li&gt;
&lt;li&gt;给定一个网络，怎么找到最好的 &lt;em&gt;Communities&lt;/em&gt; 模型&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;从模型生成网络&#34;&gt;从模型生成网络&lt;/h2&gt;

&lt;p&gt;从模型生成网络是指定义一个模型，它接受一系列的参数，并最终生成一张网络（主要是边的生成）。&lt;/p&gt;

&lt;h3 id=&#34;agm-affiliation-graph-model&#34;&gt;AGM: Affiliation Graph Model&lt;/h3&gt;

&lt;h4 id=&#34;参数&#34;&gt;参数&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;$V$&lt;/code&gt;: 节点&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$C$&lt;/code&gt;: &lt;em&gt;Community&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$M$&lt;/code&gt;: 节点与 &lt;em&gt;Community&lt;/em&gt; 之间的关系&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$P_{c}$&lt;/code&gt;: 某个 &lt;em&gt;Community&lt;/em&gt; 的联通率，即都属于某个 &lt;em&gt;Community&lt;/em&gt; 的节点之间联通的概率。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;生成过程&#34;&gt;生成过程&lt;/h4&gt;

&lt;p&gt;遍历每个 &lt;em&gt;Community&lt;/em&gt; &lt;code&gt;$A$&lt;/code&gt; 中的节点对，以 &lt;code&gt;$P_{A}$&lt;/code&gt; 的概率连接它们（边的生成）。&lt;/p&gt;

&lt;p&gt;任意两点间的连接概率为&lt;code&gt;$P(u,v)=1-\prod_{c \in M_{u} \bigcap M_{v}}(1-p_{c}) $&lt;/code&gt; 。其中，&lt;code&gt;$M_{u}$&lt;/code&gt; 表示包含有节点 &lt;code&gt;$u$&lt;/code&gt; 的 &lt;em&gt;Community&lt;/em&gt; 集合。如果 &lt;code&gt;$u$&lt;/code&gt; 和 &lt;code&gt;$v$&lt;/code&gt; 没有共用的 &lt;em&gt;Community&lt;/em&gt; ，则概率 &lt;code&gt;$P(u,v) = \epsilon $&lt;/code&gt; 。&lt;/p&gt;

&lt;h4 id=&#34;适应性强&#34;&gt;适应性强&lt;/h4&gt;

&lt;p&gt;可以用于生成多种 &lt;em&gt;Community&lt;/em&gt; ：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;无交集 (Non-overlapping)&lt;/li&gt;
&lt;li&gt;有交集 (Overlapping)&lt;/li&gt;
&lt;li&gt;内嵌 (Nested)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;从网络生成模型&#34;&gt;从网络生成模型&lt;/h2&gt;

&lt;h3 id=&#34;agm-affiliation-graph-model-1&#34;&gt;AGM: Affiliation Graph Model&lt;/h3&gt;

&lt;p&gt;这个是mmds书本&lt;a href=&#34;http://www.mmds.org/mmds/v2.1/ch10-graphs2.pdf&#34;&gt;配套幻灯片&lt;/a&gt;上的内容，占坑。&lt;/p&gt;

&lt;h3 id=&#34;bigclam&#34;&gt;BigCLAM&lt;/h3&gt;

&lt;h4 id=&#34;membership-strength&#34;&gt;Membership Strength&lt;/h4&gt;

&lt;p&gt;引入一个新概念，某个节点 &lt;code&gt;$u$&lt;/code&gt; 对某个 &lt;em&gt;Community&lt;/em&gt; 的 &lt;em&gt;membership strength&lt;/em&gt; ，记为 &lt;code&gt;$F_{u,A}$&lt;/code&gt;，同时定义如果此值为0，则表明节点不在 &lt;em&gt;Community&lt;/em&gt; 中。所以在某个 &lt;em&gt;Community&lt;/em&gt; 中，两个节点的联通概率为 &lt;code&gt;$P_{A}(u,v)=1-\exp (-F_{uA} \cdot F_{vA})$&lt;/code&gt; 。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;此处不是很明白为什么同 &lt;em&gt;Community&lt;/em&gt; 中节点的连通概率可以写成上面的方式，需要去书中相应章节找答案。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面介绍的是在同一个 &lt;em&gt;Community&lt;/em&gt; 中两个节点的联通概率，现在将要说的是在不同的 &lt;em&gt;Community&lt;/em&gt; 中如何确定联通概率。首先需要定义一个 &lt;em&gt;Community membership strength matrix&lt;/em&gt; ，它的列是 &lt;em&gt;Community&lt;/em&gt; ，行是节点。将矩阵记为 &lt;code&gt;$F$&lt;/code&gt;，某个位置 &lt;code&gt;$F_{vA}$&lt;/code&gt; 的值表示的是节点在这个 &lt;em&gt;Community&lt;/em&gt; 中的 &lt;em&gt;strength&lt;/em&gt; ，所以任意一行 &lt;code&gt;$F_{u}$&lt;/code&gt; 则表示的是节点在多个 &lt;em&gt;Community&lt;/em&gt; 中的 &lt;em&gt;membership strength&lt;/em&gt; 向量。&lt;/p&gt;

&lt;p&gt;上面的公式计算的是两个节点在单 &lt;em&gt;Community&lt;/em&gt; 中的联通概率，两个节点之间存在至少相同的 &lt;em&gt;Community&lt;/em&gt; ，那么它们的联通概率即为 &lt;code&gt;$P(u,v)=1-\prod_{C}(1-P_{C}(u,v))$&lt;/code&gt; ，经过和上面的公式整合，简化，可以得到 &lt;code&gt;$P(u,v)=1-\exp (-F_{v} \cdot F_{v}^{T} )$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;所以问题就从如何从一个网络生成 &lt;em&gt;Community&lt;/em&gt; 变成了如何从给定的网络中找到可以最大化 &lt;em&gt;likelihood&lt;/em&gt; 的矩阵 &lt;code&gt;$F$&lt;/code&gt; ，即 &lt;code&gt;$argmax_{F} \prod_{u,v \in E} p(u,v) \prod_{(u,v \notin E)}(1-p(u,v))$&lt;/code&gt; 。通常这个 &lt;em&gt;likelihood&lt;/em&gt; 会使用对数表示，记为 &lt;code&gt;$l(F)=\log P(G|F)$&lt;/code&gt; 。最后问题就演变为找到可以使 &lt;code&gt;$l(F)$&lt;/code&gt; 最大化的 &lt;code&gt;$F$&lt;/code&gt;。而 &lt;code&gt;$$l(F) = \sum_{(u,v) \in E} \log (1-\exp (-F_{u}F_{v}^{T})) - \sum_{(u,v) \notin E}(F_{u}F_{v}^{T})$$&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;计算&#34;&gt;计算&lt;/h4&gt;

&lt;p&gt;考虑到需要求 &lt;em&gt;likelihood&lt;/em&gt; 的极大值，所以考虑使用导数（ &lt;em&gt;gradient&lt;/em&gt; ）来计算。将上式中的&lt;code&gt;$F_{u}$&lt;/code&gt;看作变量，即某节点 &lt;code&gt;$u$&lt;/code&gt; 的最大 &lt;em&gt;likelihood&lt;/em&gt; 值，则对上面公式的求导后可变为&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$ \nabla l(F_{u})=\sum_{v \in \mathcal{N}(u)} F_{v} \frac{\exp (-F_{u}F_{v}^{T})}{1- \exp (-F_{u}F_{v}^{T})} - \sum_{v \notin \mathcal{N}(u)} F_{v} $$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;其中，&lt;code&gt;$\mathcal{N}(u)$&lt;/code&gt;表示节点 &lt;code&gt;$u$&lt;/code&gt; 的邻接节点。&lt;/p&gt;

&lt;p&gt;这个公式的问题在于，后面的一个求和操作是线性时间的，而且是和所有节点数目相关，这样会拖慢运算速度。式子的后一项是计算所有不在节点 &lt;code&gt;$u$&lt;/code&gt; 邻接节点中的节点的&lt;code&gt;$F$&lt;/code&gt;值的和，它可以替换为 &lt;code&gt;$\sum_{v} F_{v} - F_{u} - \sum_{v \in \mathcal{N}(u) F_{v}}$&lt;/code&gt; ，而这个式子的第一项 &lt;code&gt;$\sum_{v} F_{v}$&lt;/code&gt; 是可以预先计算得到的，而 &lt;code&gt;$\sum_{v \in \mathcal{N}(u) F_{v}}$&lt;/code&gt; 虽然也是线性时间，但是只和 &lt;code&gt;$u$&lt;/code&gt; 的邻接节点数目想关，这个数值在真实网络中远小于整个网络的节点数的。所以在速度上有很大的改进。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;有点不明白，&lt;code&gt;$\sum_{v} F_{v}$&lt;/code&gt; 在开始的时候是怎么可以事先计算的，要求的不就是某个 &lt;code&gt;$F$&lt;/code&gt; 吗。后期实现的时候再看如何处理吧。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;其它阅读-graph-and-social-network&#34;&gt;其它阅读: Graph and Social Network&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：此处的内容并没有在课程讲义中，而对应书本的 &lt;em&gt;10.1.2 Social Networks as Graphs&lt;/em&gt; 节。主要是觉得在课程上说的有些东西不是很清楚来龙去脉，所以去书本上找一找，就看到了这个东西。目前还有一些问题不是很明了，也一起列在最后。&lt;/p&gt;

&lt;p&gt;一般认为，一个 &lt;em&gt;SN&lt;/em&gt; 可以看成一张图，但这并不是表示任意一个 &lt;em&gt;graph&lt;/em&gt; ，就能表示一个 &lt;em&gt;social network&lt;/em&gt; ，这个是由这张图的 &lt;em&gt;locality of relationships&lt;/em&gt; 确定的。&lt;/p&gt;

&lt;p&gt;检查一个 &lt;em&gt;graph&lt;/em&gt; 是不是 &lt;em&gt;SN&lt;/em&gt; ，则需要计算这张图的 &lt;em&gt;locality of relationships&lt;/em&gt; （找不到这个词怎么翻译，先原样放这吧）。给定一个有 &lt;code&gt;$N$&lt;/code&gt; 节点， &lt;code&gt;$E$&lt;/code&gt; 条边的图，它的联通率是指，有三个点&lt;code&gt;$X$&lt;/code&gt;，&lt;code&gt;$Y$&lt;/code&gt;和&lt;code&gt;$Z$&lt;/code&gt;，在确定&lt;code&gt;$(X,Y)$&lt;/code&gt;和&lt;code&gt;$(X,Z)$&lt;/code&gt;联通的情况下，&lt;code&gt;$(Y,Z)$&lt;/code&gt;联通的概率。&lt;/p&gt;

&lt;p&gt;此概率的计算有以下几步：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;理论联通率&lt;/strong&gt;：完全图会有&lt;code&gt;K=$\binom{N}{2}$&lt;/code&gt;条边，所以理论联通率应为 &lt;code&gt;$E/K$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;单条边的联通率&lt;/strong&gt;：在大规模的图中，一般认为理论联通率就是图的联通率，但是在小规模的图中，这个数值偏差有些大，所以需要重新计算。考虑给定条件，在已经确定有两条边的情况下，计算第三条边出现的条件概率，即为&lt;code&gt;$(E-2)/(K-2)$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实际联通率&lt;/strong&gt;：实际概率的计算需要遍历图中的每个节点，假设为&lt;code&gt;$X$&lt;/code&gt;，再找到邻接节点 &lt;code&gt;$Y$&lt;/code&gt; 和 &lt;code&gt;$Z$&lt;/code&gt; ，最后计算 &lt;code&gt;$(Y,Z)$&lt;/code&gt; 出现的概率。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;书中表示，最后算出来的实际概率大于单边理论概率，所以这张图可以看成是某 &lt;em&gt;SN&lt;/em&gt; 网络（可以表示 &lt;em&gt;SN&lt;/em&gt; 网络的 &lt;em&gt;locality&lt;/em&gt; ）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;是不是说大规格图的联通率直接认为是理论联通率，所以就肯定可以表示 &lt;em&gt;SN&lt;/em&gt; 而不用再计算？&lt;/li&gt;
&lt;li&gt;如果计算出来的联通率比单边联通率还要低，那是不是就说明这个图不能表示 &lt;em&gt;SN&lt;/em&gt; ？&lt;/li&gt;
&lt;li&gt;是不是说实际联通率只要超过单边联通率就可以看成是 &lt;em&gt;SN&lt;/em&gt; ？还是说需要超过某个阈值才可以算？&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W2 - Frequent Itemsets - Part II
</title>
      <link>http://hzmangel.github.io/post/mmds-w2-frequent-itemsets-p2/</link>
      <pubDate>Wed, 01 Jul 2015 18:20:21 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w2-frequent-itemsets-p2/</guid>
      <description>

&lt;p&gt;这一部分介绍 &lt;em&gt;A-Priori&lt;/em&gt; 算法。&lt;/p&gt;

&lt;p&gt;前一篇中所说的两种计算数据对次数的办法，都是 &lt;em&gt;one-pass&lt;/em&gt; 的，程序在读入数据的同时，生成数据对，然后在数据对对应的计数上加1。这样数据读取完成后，直接查找计数的数即可得知结果。但是这个方法在在数据量过大时会超过主存的大小，从而在计算时引发换页，降低效率。而本篇要介绍的 &lt;em&gt;A-Priori&lt;/em&gt; 算法，通过减少需要计算的数据对个数，从而减少对内存的需求。&lt;/p&gt;

&lt;p&gt;这个算法的核心思想是 &lt;em&gt;monotonicity&lt;/em&gt; ，即如果 &lt;code&gt;{I}&lt;/code&gt; 是 &lt;em&gt;frequent itemset&lt;/em&gt; ，那么它的所有子集都是 &lt;em&gt;frequent itemset&lt;/em&gt; 。反过来说，如果一个元素不是 &lt;em&gt;frequent itemset&lt;/em&gt; ，那么任何包含它的集合都不可能是。&lt;/p&gt;

&lt;h2 id=&#34;基本算法&#34;&gt;基本算法&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;A-Priori&lt;/em&gt; 是一个 &lt;em&gt;two-pass&lt;/em&gt; 算法，&lt;/p&gt;

&lt;h3 id=&#34;first-pass&#34;&gt;First Pass&lt;/h3&gt;

&lt;p&gt;在读取的过程中创建两张Hash表，一张表是将所有 &lt;code&gt;item&lt;/code&gt; 映射为 &lt;code&gt;1&lt;/code&gt; 到 &lt;code&gt;n&lt;/code&gt; 的索引，减少后面引用时占用的内存。第二张表是将 &lt;code&gt;item&lt;/code&gt; 的索引与 &lt;code&gt;item&lt;/code&gt; 出现的次数对应起来。&lt;/p&gt;

&lt;h3 id=&#34;between-passes&#34;&gt;Between Passes&lt;/h3&gt;

&lt;p&gt;检查索引和出现次数映射的表，找出其中所有的 &lt;em&gt;frequent itemset&lt;/em&gt; ，并将它们重新索引为 &lt;code&gt;1&lt;/code&gt; 到 &lt;code&gt;m&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;second-pass&#34;&gt;Second Pass&lt;/h3&gt;

&lt;p&gt;计算所有使用 &lt;em&gt;frequent itemset&lt;/em&gt; 表中元素组成的数据对的 &lt;em&gt;support&lt;/em&gt; 值。具体流程如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;对每一个 &lt;em&gt;basket&lt;/em&gt; ，找出其中属于 &lt;em&gt;frequent itemset&lt;/em&gt; 表中的元素。&lt;/li&gt;
&lt;li&gt;为找到的元素构建数据对。&lt;/li&gt;
&lt;li&gt;找到数据对对应的计数值，加上它出现的次数。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;最后，只需要检查第二次生成的数据就好。另外，如果需要求多于2个元素的 &lt;em&gt;itemset&lt;/em&gt; ，可以按上述办法将算法级联计算。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;基本算法介绍完成，下面就是改进了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Mutation: it is the key to our evolution.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;pcy&#34;&gt;PCY&lt;/h2&gt;

&lt;p&gt;在 &lt;em&gt;first pass&lt;/em&gt; 的时候，内存中有很多空闲的地方，所以在给 &lt;em&gt;item&lt;/em&gt; 计数的同时，生成出所有的 &lt;em&gt;pair&lt;/em&gt; ，取 Hash 后，给对应的 &lt;em&gt;bucket&lt;/em&gt; 加1 。在 &lt;em&gt;second pass&lt;/em&gt; 的时候，只处理如下的数据对：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;i&lt;/code&gt; 和 &lt;code&gt;j&lt;/code&gt; 都是 &lt;em&gt;frequent items&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{i, j}&lt;/code&gt; 被 Hash 的 &lt;em&gt;bucket&lt;/em&gt; 是 &lt;em&gt;frequent bucket&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这样 &lt;em&gt;second pass&lt;/em&gt; 中需要处理的数据又能少一些了。&lt;/p&gt;

&lt;p&gt;这个算法的原理是，如果一个 &lt;em&gt;pair&lt;/em&gt; 是 &lt;em&gt;frequent pair&lt;/em&gt; ，那么它对应的 &lt;em&gt;bucket&lt;/em&gt; 一定超过 &lt;em&gt;support&lt;/em&gt; 的阈值。而如果一个 &lt;em&gt;bucket&lt;/em&gt; 没有超过阈值，那么里面的所有 &lt;em&gt;pair&lt;/em&gt; 一定不是 &lt;em&gt;frequent pair&lt;/em&gt; 。 注意：一个 &lt;em&gt;bucket&lt;/em&gt; 是 &lt;em&gt;frequent bucket&lt;/em&gt; 并不能保证其中的 &lt;em&gt;pair&lt;/em&gt; 都是 &lt;em&gt;frequent pair&lt;/em&gt; 。&lt;/p&gt;

&lt;h3 id=&#34;first-pass-1&#34;&gt;First Pass&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;for b in buckets
  for item in b:
    count[item]++
  end

  for pair in b:
    bucket(hash(pair))++
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;between-passes-1&#34;&gt;Between Passes&lt;/h3&gt;

&lt;p&gt;除了找出 &lt;em&gt;frequent item&lt;/em&gt; 外，还会把上一步中的 &lt;em&gt;hash bucket&lt;/em&gt; 替换为 &lt;em&gt;bitmap&lt;/em&gt; ，&lt;code&gt;1&lt;/code&gt; 表示 &lt;em&gt;frequent bucket&lt;/em&gt; ， &lt;code&gt;0&lt;/code&gt; 表示不是。这样可以进一步减少内存空间的占用（从 &lt;code&gt;32bit&lt;/code&gt; 降到 &lt;code&gt;1bit&lt;/code&gt;）。&lt;/p&gt;

&lt;h3 id=&#34;second-pass-1&#34;&gt;Second Pass&lt;/h3&gt;

&lt;p&gt;根据上面列出的条件，找到 &lt;em&gt;canidate pair&lt;/em&gt; ，计算。&lt;/p&gt;

&lt;p&gt;PCY算法还有两个变种：&lt;/p&gt;

&lt;h2 id=&#34;multistage&#34;&gt;Multistage&lt;/h2&gt;

&lt;p&gt;这个算法的核心在于，在PCY算法的 &lt;em&gt;first pass&lt;/em&gt; 后，并不开始对 &lt;em&gt;bucket&lt;/em&gt; 做筛选，而是将其中属于 &lt;em&gt;frequent bucket&lt;/em&gt; 的 &lt;em&gt;pair&lt;/em&gt; 挑出来，再做一次 Hash ，从而进一步减少 &lt;em&gt;second pass&lt;/em&gt; 中需要处理的数据量。从定义上来看，这个算法可能会有多个 &lt;em&gt;pass&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;multihash&#34;&gt;Multihash&lt;/h2&gt;

&lt;p&gt;在 &lt;em&gt;first pass&lt;/em&gt; 中，使用多个独立的Hash函数来计算 &lt;em&gt;bucket&lt;/em&gt; 。相比物 Multistage ，它还是 &lt;em&gt;two pass&lt;/em&gt; 的算法，但是它的风险在于在多个 Hash 的作用下，可能会有更多的 &lt;em&gt;frequent bucket&lt;/em&gt; 。如果它能保证 &lt;em&gt;frequent bucket&lt;/em&gt; 的数目足够小，那么我们就能在 &lt;em&gt;two pass&lt;/em&gt; 获得 Multistage 的优点。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;算法的另一个发展趋势是获取到大部分的 &lt;em&gt;frequent itemsets&lt;/em&gt; ，使用降低精确性的办法来换取效率。&lt;/p&gt;

&lt;h2 id=&#34;基本想法&#34;&gt;基本想法&lt;/h2&gt;

&lt;p&gt;对样本 &lt;em&gt;basket&lt;/em&gt; 进行随机抽样，并在内存中对样本进行 &lt;em&gt;A-Priori&lt;/em&gt; 或其改进版本的计算，此处需要计算所有的 &lt;em&gt;frequent itemsets&lt;/em&gt; ，而不仅仅是 &lt;em&gt;pair&lt;/em&gt; 。计算时使用的 &lt;em&gt;support threshold&lt;/em&gt; 需要根据样本容量处理，例如抽样率为 &lt;code&gt;1/100&lt;/code&gt; ，那么阈值也相应的变成 &lt;code&gt;s/100&lt;/code&gt; 。在 &lt;em&gt;second pass&lt;/em&gt; 的时候，可以使用整体数据来验证是否找到的真的是 &lt;em&gt;frequent pair&lt;/em&gt; 。&lt;/p&gt;

&lt;p&gt;这个算法有个问题有些 &lt;em&gt;set&lt;/em&gt; 可能在样本中不是 &lt;em&gt;frequent&lt;/em&gt; 但在全局中是，这里有一个稍微改进的办法是适当降低样本计算中的 &lt;em&gt;support&lt;/em&gt; 阈值，例如从 &lt;code&gt;s/100&lt;/code&gt; 降为 &lt;code&gt;s/125&lt;/code&gt; ，不过这会需要更大的内存空间。（而且还是有可能会漏掉的吧&amp;hellip;&amp;hellip;）&lt;/p&gt;

&lt;h2 id=&#34;son-算法&#34;&gt;SON 算法&lt;/h2&gt;

&lt;p&gt;这个算法是将大的数据分成小集合分别读入内存进行计算，并将在任意一次计算中得到的 &lt;em&gt;frequent pair&lt;/em&gt; 置于候选的位置。而在 &lt;em&gt;second pass&lt;/em&gt; ，计算这些候选 &lt;em&gt;pair&lt;/em&gt; 并得出最后结果。这个算法的核心是，任何一个 &lt;em&gt;pair&lt;/em&gt; 如果是 &lt;em&gt;frequent pair&lt;/em&gt; ，那么它一定会在某个子集中是 &lt;em&gt;frequent pair&lt;/em&gt; 。&lt;/p&gt;

&lt;p&gt;这个算法可以让计算并行化，以提高处理效率。&lt;/p&gt;

&lt;h2 id=&#34;toivonen-算法&#34;&gt;Toivonen 算法&lt;/h2&gt;

&lt;p&gt;在 &lt;em&gt;first pass&lt;/em&gt; 中，使用子集基本算法一样的方法，抽样并计算，但是在取阈值的时候设置的稍低，例如 &lt;code&gt;s/125&lt;/code&gt;，以尽可能的不要遗漏在全集中是 &lt;em&gt;frequent set&lt;/em&gt; 的数据。此后设置一个 &lt;em&gt;negative border&lt;/em&gt; 区间，以存放所有子集是 &lt;em&gt;frequent set&lt;/em&gt; 但是本身不是的集合。在 &lt;em&gt;second pass&lt;/em&gt; 的时候，计算所有候选 &lt;em&gt;frequent itemsets&lt;/em&gt; 和 &lt;em&gt;negative border&lt;/em&gt; 中的数据。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果 &lt;em&gt;negative border&lt;/em&gt; 中没有数据是 &lt;em&gt;frequent itemsets&lt;/em&gt; ，那么所得就是所需的 &lt;em&gt;frequent itemsets&lt;/em&gt; .&lt;/li&gt;
&lt;li&gt;如果在 &lt;em&gt;negative border&lt;/em&gt; 中找到了 &lt;em&gt;frequent itesmsets&lt;/em&gt; ，那么就需要重新取样进行计算，直到满足前一条的条件为止。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;证明：假设 &lt;code&gt;S&lt;/code&gt; 是一个在全集中的 &lt;em&gt;frequent itemset&lt;/em&gt; ，但是在抽样样本中，它既不属于 &lt;em&gt;frequent itemsets&lt;/em&gt; 也不是 &lt;em&gt;negative border&lt;/em&gt; 。那么可以找到一个 &lt;code&gt;T&lt;/code&gt; ，它是在样本中非 &lt;em&gt;frequent&lt;/em&gt; 的 &lt;code&gt;S&lt;/code&gt; 最小子集。即比 &lt;code&gt;T&lt;/code&gt; 小的子集在样本中都是 &lt;em&gt;frequent itemset&lt;/em&gt; ，那根据 &lt;em&gt;negative border&lt;/em&gt; 的定义，&lt;code&gt;T&lt;/code&gt; 一定会在 &lt;em&gt;negative border&lt;/em&gt; 中，否则它就不是是最小子集。由于空集总是 &lt;em&gt;frequent itemset&lt;/em&gt; ，所以 &lt;code&gt;T&lt;/code&gt; 一定存在。这样就有一个 &lt;code&gt;T&lt;/code&gt; ，它在全集中是 &lt;em&gt;frequent itemset&lt;/em&gt; ，在样本中是 &lt;em&gt;negative border&lt;/em&gt; ，于是，重新抽样吧。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W2 - Frequent Itemsets - Part I
</title>
      <link>http://hzmangel.github.io/post/mmds-w2-frequent-itemsets-p1/</link>
      <pubDate>Fri, 26 Jun 2015 19:35:21 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w2-frequent-itemsets-p1/</guid>
      <description>

&lt;p&gt;第二周的最后一块内容是 &lt;em&gt;Frequent Itemsets&lt;/em&gt; 。主要介绍了 &lt;em&gt;Frequent Itemsets&lt;/em&gt; ， &lt;em&gt;Association Rule&lt;/em&gt; 以及算法。这一部分介绍前面的，后面一篇会介绍算法和优化。&lt;/p&gt;

&lt;h2 id=&#34;market-basket-模型&#34;&gt;&lt;em&gt;Market-Basket&lt;/em&gt; 模型&lt;/h2&gt;

&lt;p&gt;这个模型一般用来描述两种数据之间的 &lt;em&gt;m2m&lt;/em&gt; 关系。其中的一个数据是 &lt;em&gt;item&lt;/em&gt; ，而另一个是 &lt;em&gt;baskets&lt;/em&gt; 。每一个 &lt;em&gt;basket&lt;/em&gt; 中包含有多个 &lt;em&gt;items&lt;/em&gt; ，即为 &lt;em&gt;itemset&lt;/em&gt; 。 通常认为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;basket&lt;/em&gt; 中的 &lt;em&gt;items&lt;/em&gt; 数量较少，远小于整体 &lt;em&gt;items&lt;/em&gt; 的数量。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;basket&lt;/em&gt; 的数量较大，不能完全的放于内存中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这是一个模型的例子：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Market&lt;/em&gt; ：一个超级市场。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Item&lt;/em&gt; ：市场中售卖的所有东西。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Basket&lt;/em&gt; ：用户的订单。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;frequent-itemsets&#34;&gt;&lt;em&gt;Frequent Itemsets&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Frequent Itemsets&lt;/em&gt; 即指在 &lt;em&gt;baskets&lt;/em&gt; 中经常出现的 &lt;em&gt;itemset&lt;/em&gt; ，它的定义为：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;假设 &lt;em&gt;I&lt;/em&gt; 是一个 &lt;em&gt;itemset&lt;/em&gt; ，定义 &lt;em&gt;I&lt;/em&gt; 的 &lt;em&gt;support&lt;/em&gt; 为包含 &lt;em&gt;I&lt;/em&gt; 的 &lt;em&gt;basket&lt;/em&gt; 个数。定义数字 &lt;em&gt;s&lt;/em&gt; 为 &lt;em&gt;support threshold&lt;/em&gt; ，所有 &lt;em&gt;support&lt;/em&gt; 数超过 &lt;em&gt;s&lt;/em&gt; 的 &lt;em&gt;I&lt;/em&gt; 即为 &lt;em&gt;frequent itemset&lt;/em&gt; 。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt; ：一个 &lt;em&gt;basket&lt;/em&gt; 可能包含有多个 &lt;em&gt;itemset&lt;/em&gt; ，并不是说一个 &lt;em&gt;basket&lt;/em&gt; 中包含的就是一个 &lt;em&gt;itemset&lt;/em&gt; 。&lt;/p&gt;

&lt;p&gt;一个 &lt;em&gt;itemset&lt;/em&gt; 可以包含有不定个元素。如果一个 &lt;em&gt;itemset&lt;/em&gt; 是 &lt;em&gt;frequent itemset&lt;/em&gt; ，那么它的的任何子集都是 &lt;em&gt;frequent itemset&lt;/em&gt; 。&lt;/p&gt;

&lt;h4 id=&#34;应用场景&#34;&gt;应用场景&lt;/h4&gt;

&lt;h5 id=&#34;超市商品&#34;&gt;超市商品&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;item&lt;/em&gt; ：超市商品。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;basket&lt;/em&gt; ：用户订单，上面包含有一个或多个商品。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过分析订单中的 &lt;em&gt;frequent itemsets&lt;/em&gt; ，可以得出哪些商品会经常被同时购买。&lt;/p&gt;

&lt;h5 id=&#34;相关内容查找&#34;&gt;相关内容查找&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;item&lt;/em&gt; ：单词&lt;/li&gt;
&lt;li&gt;&lt;em&gt;basket&lt;/em&gt; ：文档，如网页，blog，tweets等。包含一系列的单词。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;找出的 &lt;em&gt;frequent itemset&lt;/em&gt; 中，肯定包含有一些 common word，除去这些词后，可以揭示出一些单词之间的关系。&lt;/p&gt;

&lt;h5 id=&#34;剽窃检测&#34;&gt;剽窃检测&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;itme&lt;/em&gt; ：文档&lt;/li&gt;
&lt;li&gt;&lt;em&gt;basket&lt;/em&gt; ：句子。如果一个文档包含这个句子，那么它在这个 &lt;em&gt;basket&lt;/em&gt; 中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;找出的 &lt;em&gt;frequent itemset&lt;/em&gt; 表示这些文档中有大量的句子相似，可以检测抄袭的方法。&lt;/p&gt;

&lt;h5 id=&#34;生物标记&#34;&gt;生物标记&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;item&lt;/em&gt; ：生物标记，如基因，血蛋白，疾病等。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;basket&lt;/em&gt; ：病人数据，如基因检测，生化指标等。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;找到的包含疾病和生物标记的 &lt;em&gt;frequent itemset&lt;/em&gt; 可以用于疾病检测。&lt;/p&gt;

&lt;h3 id=&#34;association-rule&#34;&gt;&lt;em&gt;Association Rule&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;定义 &lt;code&gt;$I \rightarrow j$&lt;/code&gt; ，表示如果一个 &lt;em&gt;basket&lt;/em&gt; 包含有 &lt;code&gt;{I}&lt;/code&gt; 中所有的元素，那么它就有可能包含 &lt;code&gt;j&lt;/code&gt; 。即 &lt;code&gt;{I}&lt;/code&gt; 是一组元素，而 &lt;code&gt;j&lt;/code&gt; 是一个元素。这个推断，就是一个 &lt;em&gt;association rule&lt;/em&gt; 。 &lt;em&gt;association rule&lt;/em&gt; 有一个属性为 &lt;em&gt;Confidence&lt;/em&gt; ，它表示给了 &lt;code&gt;{I}&lt;/code&gt; 后出现 &lt;code&gt;j&lt;/code&gt; 的概率，即在所有包含&lt;code&gt;{I}&lt;/code&gt;的 &lt;em&gt;basket&lt;/em&gt; 中同时包含&lt;code&gt;j&lt;/code&gt;的概率。&lt;/p&gt;

&lt;p&gt;现在问题来了：找到所有 &lt;em&gt;support &amp;gt;= s&lt;/em&gt; 且 &lt;em&gt;confidence &amp;gt;= c&lt;/em&gt; 的 &lt;em&gt;association rule&lt;/em&gt; 。此处的 &lt;em&gt;support&lt;/em&gt; 是指 &lt;code&gt;{I}&lt;/code&gt; 的 &lt;em&gt;support&lt;/em&gt; ，而不是 &lt;code&gt;{I, j}&lt;/code&gt; 的。&lt;/p&gt;

&lt;p&gt;计算过程的描述如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;找到所有 &lt;em&gt;support &amp;gt;= sc&lt;/em&gt; 的集合&lt;/li&gt;
&lt;li&gt;找到所有 &lt;em&gt;support &amp;gt;= s&lt;/em&gt; 的集合&lt;/li&gt;
&lt;li&gt;如果 &lt;code&gt;{I, j}&lt;/code&gt; 的 &lt;em&gt;support &amp;gt;= cs&lt;/em&gt; ，那么找到它少一个元素，而且 &lt;em&gt;support &amp;gt;= s&lt;/em&gt; 的子集。&lt;/li&gt;
&lt;li&gt;当且仅当下面条件都满足时， &lt;code&gt;$I \rightarrow j$&lt;/code&gt; 才是一个被接受的 &lt;em&gt;rule&lt;/em&gt; 。

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;{I}&lt;/code&gt; 的 &lt;em&gt;support s1 &amp;gt;= s&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{I, j}&lt;/code&gt; 的 &lt;em&gt;support s2 &amp;gt;= sc&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;s2/s1 &amp;gt;= c&lt;/em&gt; （此比值即为 &lt;em&gt;confidence&lt;/em&gt; 的定义）.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在实现上，数据通常是以普通文件方式存储于磁盘上的，存储的方式是按 &lt;em&gt;basket&lt;/em&gt; 排列。在读入数据时，即将数据切分成不同长度的集合。不难看出，磁盘IO是数据读取过程中主要的开销。在实践中，数据是多次读取的。所以在计算时，读取次数也是一个需要考虑的部分。&lt;/p&gt;

&lt;p&gt;在计算过程中，由于设置的阈值比较高，所以通常能找到的就是 &lt;em&gt;frequent pair&lt;/em&gt; ，所以这也是需要面对的最大问题。&lt;/p&gt;

&lt;h4 id=&#34;一般算法&#34;&gt;一般算法&lt;/h4&gt;

&lt;p&gt;一般的算法就是读一次文件，在内存中计算出所有 &lt;em&gt;pair&lt;/em&gt; 出现的次数。对于一个有 &lt;em&gt;n&lt;/em&gt; 个元素的 &lt;em&gt;basket&lt;/em&gt; ，最后会生成 &lt;em&gt;n(n-1)/2&lt;/em&gt; 个 &lt;em&gt;pair&lt;/em&gt; 。所以如果 *n*&lt;em&gt;2&lt;/em&gt; 的大小超过内存，这个算法就会失败。&lt;/p&gt;

&lt;p&gt;在计算上，有两种方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;计算所有数据对出现的次数，并存放于三角矩阵中。&lt;/li&gt;
&lt;li&gt;使用类似稀疏矩阵的存储方法，使用坐标+次数的方式存储。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;第一种方式每个数据对需要 &lt;code&gt;4&lt;/code&gt; 个字节（假设一个整型数使用4个字节）。第二种方式每对需要12字节（坐标及计数），但是只有存在的数据对才会占用空间。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W2 - Nearest Neighbor Learning
</title>
      <link>http://hzmangel.github.io/post/mmds-w2-nearest-neighbor-learning/</link>
      <pubDate>Wed, 24 Jun 2015 02:20:33 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w2-nearest-neighbor-learning/</guid>
      <description>

&lt;p&gt;第二周的 &lt;em&gt;Nearest Neighbor Learning&lt;/em&gt; 只是一个大概的介绍。这是一个通过在训练集中找到离待查询数据最近的点从而做出预测的方法。&lt;/p&gt;

&lt;h2 id=&#34;supervised-learning&#34;&gt;Supervised Learning&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Supervised learning&lt;/em&gt; 是机器学习中的一种方法（还有两种分别是 &lt;a href=&#34;https://en.wikipedia.org/wiki/Unsupervised_learning&#34;&gt;Unsupervised learning&lt;/a&gt; 和 &lt;a href=&#34;https://en.wikipedia.org/wiki/Reinforcement_learning&#34;&gt;Reinforcement learning&lt;/a&gt; , via &lt;a href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&gt;wikipedia&lt;/a&gt; 。这个如果将来看了相应的课再写。）,它通过训练数据集建立一种模式或叫函数，再通过此模式去匹配待查询数据集从而得出预测结果。&lt;/p&gt;

&lt;h2 id=&#34;instance-based-learning&#34;&gt;Instance Based Learning&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Instance Based Learning&lt;/em&gt; 是机器学习的一种方法，也叫 &lt;em&gt;Memory Based Learning&lt;/em&gt; 。它不生成完整的匹配模式，而是在从训练数据中找到和待查询数据相近的数据（这些数据通常存放于内存中），并输出结果。将要说到的 &lt;em&gt;Nearest Neighbor Learning&lt;/em&gt; 即是其中的一种。此方法也分为 &lt;em&gt;1-Nearest Neighbor&lt;/em&gt; 和 &lt;em&gt;k-Nearest Neighbor&lt;/em&gt; 。进行计算需要了解如下4个元素：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如何衡量距离？&lt;/li&gt;
&lt;li&gt;找几个 Neighbor ？&lt;/li&gt;
&lt;li&gt;各点是否有不同的权重？&lt;/li&gt;
&lt;li&gt;如何确定输出？&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;1-nearest-neighbor-和-k-nearest-neighbor&#34;&gt;1-Nearest Neighbor 和 k-Nearest Neighbor&lt;/h3&gt;

&lt;p&gt;在 &lt;em&gt;1-Nearest Neighbor&lt;/em&gt; 的情况下，距离使用欧氏距离，寻找最近的&lt;code&gt;1&lt;/code&gt;个 &lt;em&gt;Neighbor&lt;/em&gt; ，各点权重相同，直接以最近 &lt;em&gt;Neighbor&lt;/em&gt; 的输出为查询数据的输出。而在 &lt;em&gt;k-Nearest Neighbor&lt;/em&gt; 中，查找的节点变成了 &lt;em&gt;k&lt;/em&gt; 个，而输出出变成了 &lt;em&gt;k&lt;/em&gt; 个节点的平均值。&lt;/p&gt;

&lt;h3 id=&#34;kernel-regression&#34;&gt;Kernel Regression&lt;/h3&gt;

&lt;p&gt;核回归，统计中的一种方法，具体的算法及使用场景没看太明白，此处暂时仅为记录而用。对于这个算法，它所涉及到的4个参量是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;欧氏距离&lt;/li&gt;
&lt;li&gt;查找所有训练集中的点&lt;/li&gt;
&lt;li&gt;权重函数： &lt;code&gt;$w_{i} = exp(-\frac{d(x_{i}, q)^{2}}{K_{w}})$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;输出：&lt;code&gt;$\frac{\sum_{i}w_{i}y_{i}}{\sum_{i}w_{i}}$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W2 - Locality-Sensitive Hashing
</title>
      <link>http://hzmangel.github.io/post/mmds-w2-locality-sensitive-hashing/</link>
      <pubDate>Wed, 17 Jun 2015 08:02:10 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w2-locality-sensitive-hashing/</guid>
      <description>

&lt;p&gt;Locality-Sensitive Hashing，LSH，局部敏感hash或叫位置敏感hash。它的想法是在对原始数据空间的数据做Hash后，让位置相邻的数据有很大概率被放到同一个或者相近的bucket中，而不相邻的点放在一起的概率要很小。这样就会减少后期数据处理的数据集，从而简化后续的工作。&lt;/p&gt;

&lt;h2 id=&#34;相似数据集&#34;&gt;相似数据集&lt;/h2&gt;

&lt;p&gt;许多数据挖掘的问题都能简化为查找相似数据集的问题，如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;查找含有相似单词的页面，用以给页面分类，或者找出页面的镜像站，或者检查剽窃等。&lt;/li&gt;
&lt;li&gt;NetFlix，理解成豆瓣就行，哪些用户有相似的爱好。&lt;/li&gt;
&lt;li&gt;以及，哪些电影有相似的粉丝。&lt;/li&gt;
&lt;li&gt;网上找到的个人信息，怎么才能确定哪些属于同一个人。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;先从相似文档找起，有三个关键技术：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Shingling：把文档，像页面啊，邮件啊，什么的，拆成sets。&lt;/li&gt;
&lt;li&gt;Minhashing：在保留相似性的基础上，把大的集合转化成短的标记。&lt;/li&gt;
&lt;li&gt;Locality-sensitive hashing：找出相似的对。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;文档相似性可以使用 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 来衡量。对于两个集合 &lt;code&gt;$S$&lt;/code&gt; 和 &lt;code&gt;$T$&lt;/code&gt; 来说，它们之间的 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 为 &lt;code&gt;$|S \cap T| / |S \cup T|$&lt;/code&gt;，记为 &lt;em&gt;SIM(S,T)&lt;/em&gt; 或 &lt;em&gt;J(S,T)&lt;/em&gt; 。不难看出，当此值为&lt;code&gt;0&lt;/code&gt;时表示两个集合没有交集，而为&lt;code&gt;1&lt;/code&gt;时则表示两个集合相等。&lt;/p&gt;

&lt;h3 id=&#34;k-shingling或叫k-gram&#34;&gt;k-shingling或叫k-gram&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;k-shingling&lt;/em&gt; 是指把文档按连续的 &lt;em&gt;k&lt;/em&gt; 个字母拆成子集的方法。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;例如，给定文档&lt;code&gt;$D$&lt;/code&gt;的内容为&lt;code&gt;abcdabd&lt;/code&gt;，在&lt;code&gt;$k=2$&lt;/code&gt;的情况下，获得的 &lt;em&gt;2-shingling&lt;/em&gt; 集合为 &lt;code&gt;{ab, bc, cd, da, bd}&lt;/code&gt;。shingling有一个变种是生成一个bag而非set，此时重复的元素不会被归并，而是按照其本来出现的次数出现在最后结果中，如本例中的&lt;code&gt;ab&lt;/code&gt;将会出现2次。&lt;/p&gt;

&lt;p&gt;对于空格的处理有多种选项，较常见的是把所有空格类的东西都替换成一个空格，然后将其作为一个正常元素参与到shingling中去。即shingling后的元素可能会包含2个或多个单词。&lt;/p&gt;

&lt;p&gt;为了避免虚假的相似度， &lt;code&gt;$k$&lt;/code&gt; 的取值需要足够大。一般而言，对于短的如电子邮件之类的文件，取&lt;code&gt;$k=5$&lt;/code&gt;，而对于长的文档，如研究报告这种，取&lt;code&gt;$k=9$&lt;/code&gt;会比较好。&lt;/p&gt;

&lt;p&gt;对于shingling中的元素，可以直接使用字符串，但是更好的办法是把它通过hash变化映射到某个bucket中，而将这个bucket的编号作为shingling元素进行比较。这样可以在shingling元素空间不变的情况下，降低运行时占用的内存。而且在比较上，整数比字符串要更有优势。这一步叫做 &lt;strong&gt;Compressing Shinglings&lt;/strong&gt; 。&lt;/p&gt;

&lt;h3 id=&#34;minhashing&#34;&gt;Minhashing&lt;/h3&gt;

&lt;p&gt;这一段是从 &lt;a href=&#34;https://en.wikipedia.org/wiki/MinHash&#34;&gt;wikipedia&lt;/a&gt; 上看到的定义：&lt;/p&gt;

&lt;p&gt;设有一个hash函数&lt;code&gt;$h$&lt;/code&gt;，可以将集合中的元素映射为不重复的整数值。这样对于任何集合&lt;code&gt;$S$&lt;/code&gt;，都能找到一个元素&lt;code&gt;$x$&lt;/code&gt;让&lt;code&gt;$h(S)$&lt;/code&gt;取到最小值&lt;code&gt;$h_{min}(S)$&lt;/code&gt;。这样就把对字符串比较，存储转换成了对整数的计算和存储。由于&lt;code&gt;$h_{min}(S)$&lt;/code&gt;只能得到一个值，所以需要使用 &lt;em&gt;Hash Function Family&lt;/em&gt; 去处理集合，以得到一个最小值的向量。在向量长度足够的情况下，两个集合的相似度等于最小值相等的概率。计算向量有两种办法，一种是选取足够多的hash函数，另一种是对一个hash得出的值作多次变换。&lt;/p&gt;

&lt;p&gt;在课程中，首先介绍了怎么抽取多个集合的 &lt;em&gt;Characteristic Matrix&lt;/em&gt; ，这个矩阵的每一列都是一个需要计算相似度的集合，记为&lt;code&gt;$S_{1}$&lt;/code&gt;到&lt;code&gt;$S_{N}$&lt;/code&gt;，而行是所有元素的集合，记为&lt;code&gt;$e_{1}$&lt;/code&gt;到&lt;code&gt;$e_{M}$&lt;/code&gt;。如果某个元素&lt;code&gt;$e_{i}$&lt;/code&gt;包含于集合&lt;code&gt;$S_{j}$&lt;/code&gt;中，则在矩阵相应的位置&lt;code&gt;$(i,j)$&lt;/code&gt;标上&lt;code&gt;1&lt;/code&gt;，反之则为&lt;code&gt;0&lt;/code&gt;。典型情况下这个矩阵是稀疏的。&lt;/p&gt;

&lt;p&gt;此后直接介绍了一个 &lt;em&gt;Minhashing&lt;/em&gt; 的函数簇。假设前述的 &lt;em&gt;Characteristic Matrix&lt;/em&gt; 的行排列是随机的，我们定义一个 &lt;em&gt;Minhashing&lt;/em&gt; 函数 &lt;code&gt;h(S)&lt;/code&gt; ，它的值是在特定排列下，列 &lt;code&gt;S&lt;/code&gt; 中第一次出现 &lt;code&gt;1&lt;/code&gt; 的行数。使用多个独立的哈希函数（如100个），即可为每一个集合创建一个 &lt;em&gt;signatures&lt;/em&gt; ，而多个集合的结果合并后可以生成一个新的矩阵， &lt;em&gt;signatures matrix&lt;/em&gt; 。这个矩阵的列是各个集合，而行是某一次计算 &lt;em&gt;Minhashing&lt;/em&gt; 时的结果。&lt;/p&gt;

&lt;p&gt;下面来分析下 *Jaccard Similarity*。首先看 &lt;em&gt;Characteristic Matrix&lt;/em&gt; 。设有两个需要比较的集合 &lt;code&gt;$S_{1}$&lt;/code&gt; 和 &lt;code&gt;$S_{2}$&lt;/code&gt; ，假设它们的 &lt;em&gt;Characteristic Matrix&lt;/em&gt; 为 &lt;code&gt;$M$&lt;/code&gt;，那么在矩阵 &lt;code&gt;$M$&lt;/code&gt; 中，每一行的元素只有4种组合： &lt;code&gt;(0,0)&lt;/code&gt; ，&lt;code&gt;(0,1)&lt;/code&gt; ， &lt;code&gt;(1,0)&lt;/code&gt; 和 &lt;code&gt;(1,1)&lt;/code&gt;。我们把这4种关系在M中的数量分别记为ABCD，不难看出，两个集合的相似度可以表示为 &lt;code&gt;$J(S_{1}, S_{2}) = D/(A+B+C)$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;然后再来看 &lt;em&gt;signatures matrix&lt;/em&gt; 。在某个特定的排列下，如果两个集合的 &lt;em&gt;Minhashing&lt;/em&gt; 值相同，那第它们一定是 &lt;code&gt;(1,1)&lt;/code&gt; 形式的，而其它三种形式不会有此结果）注意，此处只能保证， &lt;em&gt;Minhashing&lt;/em&gt; 值相同时，能保证这一行是 &lt;code&gt;(1,1)&lt;/code&gt;，但是一行是&lt;code&gt;(1,1)&lt;/code&gt;并不能说明这一行是 &lt;em&gt;Minhashing&lt;/em&gt; 值）。所以可以得知，两个集合 &lt;em&gt;Minhashing&lt;/em&gt; 值相等的概率，也就是两个集合的 &lt;em&gt;Jaccard&lt;/em&gt; 相似度，都是 &lt;code&gt;$J(S_{1}, S_{2}) = D/(A+B+C)$&lt;/code&gt; 。&lt;/p&gt;

&lt;p&gt;在实际实现上，给大量的数据做随机排列是比较难以实现的，所以更加通用的办法就是如wiki上说的，挑选多个 hash 函数来处理，下面是一段伪代码，计算某集合的 &lt;em&gt;Minhashing&lt;/em&gt; 向量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FOREACH hash_func_family:
  CALCULATE hi(r)

FOREACH columes:
  IF val(c) == 1:
    # Init value for SIG(i, c) is inf
    SIG(i, c) = min( SIG(i, c), hi(r) )
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;locality-sensitive-hash&#34;&gt;Locality-Sensitive Hash&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;By Me: 此处的概念还有些模糊，需要再啃啃。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;经过了前面的 &lt;em&gt;Shingling&lt;/em&gt; 和 &lt;em&gt;Minhashing&lt;/em&gt; ，需要处理的数据已经减少许多了，但是对于大文档集来说还不够。如果是需要找到任意两个集合之间的相似度，那么除了计算它们每两对之间的相似度以外没有其它任何办法。但是如果只是需要找到超过某个相似度阈值的集合对，则可以使用LSH，又叫 &lt;em&gt;Nearest Neighbor search&lt;/em&gt; 。&lt;/p&gt;

&lt;p&gt;LSH的一般做法是对元素使用多次Hash，让相似的元素落入同一个bucket中（即Hash冲突），而不相似的不在。对于上面生成的 &lt;em&gt;signatures matrix&lt;/em&gt; ，一个有效的办法是把矩阵按&lt;code&gt;r&lt;/code&gt;行分成&lt;code&gt;b&lt;/code&gt;个brand，对每一个brand中的每一小块长度为&lt;code&gt;r&lt;/code&gt;的特征值做hash，下面是分析（这块还是有些地方没想清楚，先记录下来）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;设矩阵分成了&lt;code&gt;b&lt;/code&gt;个 &lt;em&gt;brand&lt;/em&gt; ，每个 &lt;em&gt;brand&lt;/em&gt; 中有 &lt;code&gt;r&lt;/code&gt; 行。某特定两个文档的 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 是 &lt;code&gt;s&lt;/code&gt; 。即在 matrix 中某个Minhashing字符串与其它有&lt;code&gt;s&lt;/code&gt;的概率相似。&lt;/li&gt;
&lt;li&gt;某个brand中选定的特征列和其它所有列相似的概率是 &lt;code&gt;$s^{r}$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;某个brand中选定的特征列和至少一个其它列不相似的概率是&lt;code&gt;$1-s^{r}$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;一个特征列和每一个brand中都有至少一个不相似列的概率是&lt;code&gt;$(1-s^{r})^{b}$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;一个特征列和至少一个brand中所有列都相似，从而成为 &lt;em&gt;candidate pair&lt;/em&gt; 的概率为 &lt;code&gt;$1-(1-s^{r})^{b}$&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个曲线是一个S型的连续曲线，我们需要做的就是通过挑选&lt;code&gt;b&lt;/code&gt;和&lt;code&gt;r&lt;/code&gt;，让这条曲线在两端尽量的平缓，而在中间部分尽可能的陡峭。这样就不会有过多的 &lt;em&gt;False Positive&lt;/em&gt; 或者 &lt;em&gt;False Negative&lt;/em&gt; 。&lt;/p&gt;

&lt;h2 id=&#34;具体使用流程&#34;&gt;具体使用流程&lt;/h2&gt;

&lt;p&gt;综上所述，在实际应用中会有下面几步工作（文档比较）：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;选择整数 &lt;code&gt;$k$&lt;/code&gt; ，将输入文档转换为 &lt;em&gt;k-shingling&lt;/em&gt; 集合。此处可以通过Hash将 &lt;em&gt;k-shingles&lt;/em&gt; 转换为较短的 &lt;em&gt;bucket序号&lt;/em&gt; 。&lt;/li&gt;
&lt;li&gt;以 &lt;em&gt;shingle&lt;/em&gt; 排序 &lt;code&gt;&amp;lt;document, shingle&amp;gt;&lt;/code&gt; 对。&lt;/li&gt;
&lt;li&gt;选择长度 &lt;code&gt;$n$&lt;/code&gt; 用于 &lt;em&gt;Minhashing Signature&lt;/em&gt; ，并为所有文档计算特征值。&lt;/li&gt;
&lt;li&gt;确定一个概率 &lt;code&gt;$t$&lt;/code&gt; 作为文档相似度的阈值，选择 &lt;code&gt;$b$&lt;/code&gt; 和 &lt;code&gt;$r$&lt;/code&gt; 并保证 &lt;code&gt;$br=n$&lt;/code&gt; ，而且阈值&lt;code&gt;$t$&lt;/code&gt;接近&lt;code&gt;$(1/b)^{1/r}$&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;如果需要最大程度的避免 &lt;em&gt;False Negative&lt;/em&gt; ，那么选择 &lt;code&gt;$b$&lt;/code&gt; 和 &lt;code&gt;$r$&lt;/code&gt; 时要注意计算出来的值要小于 &lt;code&gt;$t$&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;如果需要保证速度而且避免 &lt;em&gt;False Positive&lt;/em&gt; ，那么选择 &lt;code&gt;$b$&lt;/code&gt; 和 &lt;code&gt;$r$&lt;/code&gt; 时注意计算出一个高的阈值。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;使用 &lt;em&gt;LSH&lt;/em&gt; 找到所有的 *candidate pairs*。&lt;/li&gt;
&lt;li&gt;检查选择出来的特征对，确定它们的相似度都大于 &lt;code&gt;$t$&lt;/code&gt; 。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;实例&#34;&gt;实例&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;注：此处只是列出课程中出现的示例，后续会尝试使用程序完成，再补齐说明。&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;entity-resolution&#34;&gt;Entity Resolution&lt;/h3&gt;

&lt;h3 id=&#34;fingerprints&#34;&gt;Fingerprints&lt;/h3&gt;

&lt;h3 id=&#34;similar-news-articles&#34;&gt;Similar News Articles&lt;/h3&gt;

&lt;h2 id=&#34;距离计算&#34;&gt;距离计算&lt;/h2&gt;

&lt;p&gt;此块知识的最后提到了距离的计算。从某种意义上说，计算LSH即是计算某两个点之间的距离，越相似的点距离越近。上面提到的 &lt;em&gt;Jaccard Similarity&lt;/em&gt; 并不是距离，用&lt;code&gt;1&lt;/code&gt;减去它才是。一般说来，有两种类型的距离，它们是 &lt;em&gt;欧氏距离&lt;/em&gt; 和 &lt;em&gt;非欧距离&lt;/em&gt; 。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;欧氏距离是指欧氏空间的距离，欧氏空间包含有实实在在维度，和密集的点。&lt;/li&gt;
&lt;li&gt;欧空中，可以在两个点之间找到中点。&lt;/li&gt;
&lt;li&gt;欧氏距离是基于欧空中点的位置来确定的&lt;/li&gt;
&lt;li&gt;其它的空间即被称为非欧空间。在非欧空间中，距离的计算是通过点的其它一些特性来完成的，因为非欧空间并没有位置这个概念。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;假设 &lt;code&gt;$x$&lt;/code&gt; ， &lt;code&gt;$y$&lt;/code&gt; 和 &lt;code&gt;$z$&lt;/code&gt; 是某个空间中的点，而 &lt;code&gt;$d$&lt;/code&gt; 是计算距离的函数，那么它有如下特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;$d(x,y) \geq 0$&lt;/code&gt; ：所有距离都是非负值。&lt;/li&gt;
&lt;li&gt;仅在 &lt;code&gt;$x$&lt;/code&gt; ， &lt;code&gt;$y$&lt;/code&gt; 是同一点时，才有 &lt;code&gt;$d(x,y) = 0$&lt;/code&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$d(x,y) = d(y,x)$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$d(x,z) \leq d(x,y) + d(y,z)$&lt;/code&gt; ：三角定理。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;欧氏距离&#34;&gt;欧氏距离&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;$L_{r}-norm = (\sum_{i=1}^{n} |x_{i} - y_{i}|^{r})^{1/r}$&lt;/code&gt; 。在&lt;code&gt;$r=2$&lt;/code&gt;时，即变成平方各开根号，即熟悉的距离计算。此外还有&lt;code&gt;$L_{1}-norm$&lt;/code&gt; 。&lt;/p&gt;

&lt;h3 id=&#34;非欧距离&#34;&gt;非欧距离&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Jaccard Distance&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;余弦距离&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Edit Distance&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Hamming Distance&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Set timezone in Python
</title>
      <link>http://hzmangel.github.io/post/timezone-in-python/</link>
      <pubDate>Tue, 16 Jun 2015 23:40:10 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/timezone-in-python/</guid>
      <description>&lt;p&gt;今天在写一个脚本的时候，发现使用&lt;code&gt;datetime.datetime.now()&lt;/code&gt;输出的是UTC时间，而同样的命令在ipython中输入的就是本地的时间。找了好久才找到不用&lt;code&gt;pytz&lt;/code&gt;的解决方案：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import datetime

print(datetime.datetime.now())
os.environ[&#39;TZ&#39;] = &#39;Asia/Shanghai&#39;
print(datetime.datetime.now())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;TZ&lt;/code&gt;的环境变量让datetime输出了指定时区的时间。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W1 - Link Analysis
</title>
      <link>http://hzmangel.github.io/post/mmds-w1-link-analysis/</link>
      <pubDate>Sun, 14 Jun 2015 17:10:58 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w1-link-analysis/</guid>
      <description>

&lt;p&gt;第一周的后半部分讲的是Link Analysis，主要讲的是&lt;strong&gt;PageRank&lt;/strong&gt;的计算。&lt;/p&gt;

&lt;p&gt;互联网在某种意义上是一个有向图，每个页面是图上的节点，而页面间的链接就是图的边。在经历了早期的目录式页面分类后，web现在进入了以Search为主的的组织方式。下面问题来了：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;怎么确定找到的信息是可以信赖的（或者相对可以信赖的）。&lt;/li&gt;
&lt;li&gt;当我们查找某个词时，哪个才是最好的结果。&lt;/li&gt;
&lt;li&gt;&lt;del&gt;搜索技术哪家强？&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以这里引入了给页面排序的做法，以确定页面的重要性。&lt;/p&gt;

&lt;h2 id=&#34;pagerank&#34;&gt;PageRank&lt;/h2&gt;

&lt;p&gt;PageRank，Google家的看家算法。核心思路是，如果一个页面是重要的，那么它指向的那个页面应该也是重要的。也就是用其它页面来证明这个页面的重要性。&lt;/p&gt;

&lt;h3 id=&#34;flow-formulation&#34;&gt;&lt;em&gt;Flow&lt;/em&gt; Formulation&lt;/h3&gt;

&lt;p&gt;在一张图中，假设有节点&lt;code&gt;$i$&lt;/code&gt;，它的PR值是&lt;code&gt;$r_{i}$&lt;/code&gt;，它有&lt;code&gt;$d_{i}$&lt;/code&gt;条出链，其中一条指向节点&lt;code&gt;$j$&lt;/code&gt;。那么&lt;code&gt;$j$&lt;/code&gt;上由&lt;code&gt;$i$&lt;/code&gt;带来的PR值即为&lt;code&gt;$\frac{r_{i}}{d_{i}}$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;根据这个需求，可以列出来一个方程组。显然，图中有几个节点，这个方程组就会有几个方程，为了给这个方程求得一个固定解，我们会人为的加上一个条件 &lt;code&gt;$\sum{r_{i}} = 1$&lt;/code&gt;，这样就可以求解出每个节点的PR值了。&lt;/p&gt;

&lt;p&gt;这个方法比较易于理解，但是对于大规模的页面集不适用，所以引入了&lt;em&gt;Matrix&lt;/em&gt; Formulation。&lt;/p&gt;

&lt;h3 id=&#34;matrix-formulation&#34;&gt;&lt;em&gt;Matrix&lt;/em&gt; Formulation&lt;/h3&gt;

&lt;p&gt;首先，把所有页面之间的跳转都用一个&lt;strong&gt;列随机矩阵(column stochastic matrix)&lt;/strong&gt;来表示，记为&lt;code&gt;$M$&lt;/code&gt;。对于每条链路&lt;code&gt;$i\rightarrow j$&lt;/code&gt;，都有相应的&lt;code&gt;$M_{ji} = 1/d_{i}$&lt;/code&gt;，其中&lt;code&gt;$d$&lt;/code&gt;是&lt;code&gt;$i$&lt;/code&gt;的出链路条数。下一步是&lt;strong&gt;Rank向量&lt;/strong&gt;，记为&lt;code&gt;$r$&lt;/code&gt;它是一个1维的列向量，每一个元素的值就是表示此节点的Rank值，记为&lt;code&gt;$r_{i}$&lt;/code&gt;，此向量满足&lt;code&gt;$\sum_{i}r_{i} = 1$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;有此定义后，上面介绍的*Flow*方程可以转换成这样： &lt;code&gt;$r = M \cdot r$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;由矩阵的定义，这个&lt;code&gt;$r$&lt;/code&gt;是矩阵&lt;code&gt;$M$&lt;/code&gt;的&lt;strong&gt;单位向量(eigenvector)&lt;/strong&gt;。即，页面的PageRank值，就是这些页面之间转移矩阵的单位向量，解出了这个向量，也就确定了这些页面的PageRank值。&lt;/p&gt;

&lt;p&gt;求解特征向量使用的是被称为*Power Iteration*的方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;初始化： &lt;code&gt;$r^{(0)} = [1/N, \ldots , 1/N]^{T}$&lt;/code&gt; ，其中&lt;code&gt;$N$&lt;/code&gt;是图中的节点数，也即所有页面个数。&lt;/li&gt;
&lt;li&gt;迭代： &lt;code&gt;$r^{(t+1)} = M \cdot r^{(t)}$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;停止条件： &lt;code&gt;$|r^{(t+1) = r^{(t)}}|_{1} &amp;lt; \epsilon$&lt;/code&gt;。其中&lt;code&gt;$|x_{i}|_{1} ＝ \sum_{i}|x_{i}|$&lt;/code&gt;是向量&lt;code&gt;$i$&lt;/code&gt;的 &lt;em&gt;L1范数&lt;/em&gt; （其实就是绝对值相加，范数的定义就是 &lt;code&gt;$|x_{i}|_{p} = (\sum_{i}|x_{i}|^{p})^{\frac{1}{p}}$&lt;/code&gt;）。此处可以使用其它的范数，如L2范数，即欧氏距离。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;teleports&#34;&gt;&lt;em&gt;Teleports&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;如果图中有 &lt;em&gt;dead end&lt;/em&gt; 节点，即只有进链没有出链；或者是在多个节点之间存在环，那么上面的迭代就会收敛到一个错误的结果上。解决方案就是，对于每次跳转，有&lt;code&gt;$\beta$&lt;/code&gt;的概率跟随链路去跳，而有&lt;code&gt;$1-\beta$&lt;/code&gt;的概率是一次随机传送 (&lt;em&gt;Teleports&lt;/em&gt;)，这样就解决上面提到的两个问题了。在实际使用中，一般&lt;code&gt;$\beta$&lt;/code&gt;取值为&lt;code&gt;0.8&lt;/code&gt;或&lt;code&gt;0.9&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;在计算上，就是引入一个Teleports矩阵，其中的每个值都是&lt;code&gt;$1/N$&lt;/code&gt;，&lt;code&gt;$N$&lt;/code&gt;为节点数目。而之前的转移矩阵&lt;code&gt;$M$&lt;/code&gt;则变为&lt;code&gt;$\beta M + (1-\beta)\frac{1}{N}e \cdot e^{T}$&lt;/code&gt;，记为&lt;code&gt;$A$&lt;/code&gt;。同样，状态跳转的迭代公式也变为&lt;code&gt;$r = A \cdot r$&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&#34;实际使用的样子&#34;&gt;实际使用的样子&lt;/h3&gt;

&lt;p&gt;以上说的都是理论，在那个神奇的世界里，计算机的内存都是无限大的，计算速度都是无限NB的，一句话，TA们都是超限界的。在回到了位于爬行界的真实世界后，还有其它需要考虑的东西。假设节点数目是 &lt;strong&gt;1 billion&lt;/strong&gt; ，那么计算前和后的向量各需要&lt;strong&gt;1 billion&lt;/strong&gt;，但是对于那个矩阵，它可是&lt;strong&gt;1 billion * 1 billion&lt;/strong&gt;，也就是&lt;strong&gt;10^18&lt;/strong&gt;。这个内存，有点贵哈～&lt;/p&gt;

&lt;p&gt;一般来说，转移矩阵都是稀疏的，这样在存储的时候可以不用存多少东西，但是加了那个Teleports后，它变成每个位置都有值了，这内存使用就duang的一下上来了。还好经过计算，发现公式&lt;code&gt;$r = A \cdot r$&lt;/code&gt; 可以改写成：&lt;code&gt;$r = \beta M \cdot r + [ \frac{1-\beta}{N} ]_{N}$&lt;/code&gt;。这就是说，矩阵还是那个稀疏的矩阵，但是在每次算完后，需要在向量上加上Teleports的结果。这样一来，占用的内存又回去了吧。&lt;/p&gt;

&lt;p&gt;基本上就是这样了~&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MMDS Notes: W1 - HDFS &amp; MR
</title>
      <link>http://hzmangel.github.io/post/mmds-w1-hdfs-mr/</link>
      <pubDate>Sat, 13 Jun 2015 23:13:32 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/mmds-w1-hdfs-mr/</guid>
      <description>

&lt;p&gt;前段时间在Cousera上各种挤时间跟完了一门 &lt;a href=&#34;https://class.coursera.org/mmds-002&#34;&gt;MMDS&lt;/a&gt; ，手上留下了一堆笔记，整理下，顺便给新blog开光吧。&lt;/p&gt;

&lt;p&gt;课程总共7周，这篇整理的第一周的 &lt;code&gt;HDFS&lt;/code&gt; 和 &lt;code&gt;MR&lt;/code&gt; 部分。&lt;/p&gt;

&lt;h2 id=&#34;dfs&#34;&gt;DFS&lt;/h2&gt;

&lt;p&gt;课程开始是从分布式存储DFS讲起，介绍性的东西居多。在大规模的集群中，硬件故障是个很容易发生的问题。解决方案要么就是堆大把的钱上NB的机器，要么就是多做备份。这里的DFS就是后一种解决方案。&lt;/p&gt;

&lt;p&gt;现在较为通用的分布式架构就是使用不那么NB的Linux机器组建Cluster，然后用不那么NB的网络把它们连接起来。而在分布式存储的情况下，把数据在不同节点之间复制是需要时间的，所以让程序去找数据就是一个比较正确的解决方案了。&lt;/p&gt;

&lt;p&gt;DFS中的节点有两类，&lt;code&gt;Chunk Server&lt;/code&gt;用来存放数据，&lt;code&gt;Master Node&lt;/code&gt;用来存放文件和&lt;code&gt;Chunk Server&lt;/code&gt;的对应关系。一个文件会被分成多处连续的&lt;code&gt;chunks&lt;/code&gt;，典型的大小是16~64MB。每一个&lt;code&gt;chunk&lt;/code&gt;都会复制2到3份，分别存储在不同的机器上（最好是在不同rack上）。而&lt;code&gt;Master Node&lt;/code&gt;就会存储这些机器和文件的映射关系。当需要查找这个文件时，先从&lt;code&gt;Master Node&lt;/code&gt;处取到&lt;code&gt;Chunk Server&lt;/code&gt;的信息，再从相应的server上获取文件内容。&lt;/p&gt;

&lt;h2 id=&#34;map-reduce&#34;&gt;Map Reduce&lt;/h2&gt;

&lt;p&gt;MR是一种编程模型，典型的应用场景就是 &lt;strong&gt;Word Cound&lt;/strong&gt; 。将MR应用到 WordCount 的先决条件为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;文件过大，不能完整的放到内存中去&lt;/li&gt;
&lt;li&gt;但是所有的&lt;code&gt;&amp;lt;key, value&amp;gt;&lt;/code&gt;对可以完全放到内存中&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;算法流程&#34;&gt;算法流程&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;将数据分块读入&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Map&lt;/code&gt; ：创建&lt;code&gt;&amp;lt;k, v&amp;gt;&lt;/code&gt;数据对。在本例中，即创建出一系列的&lt;code&gt;&amp;lt;word, 1&amp;gt;&lt;/code&gt;对。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Group by Key&lt;/code&gt; ：对&lt;code&gt;&amp;lt;k, v&amp;gt;&lt;/code&gt;进行排序。本例中即将同样key的&lt;code&gt;&amp;lt;key, value&amp;gt;&lt;/code&gt;对放在一起。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Reduce&lt;/code&gt; ：将同一个&lt;code&gt;key&lt;/code&gt;的&lt;code&gt;&amp;lt;k, v&amp;gt;&lt;/code&gt;对合并到一起，并对&lt;code&gt;v&lt;/code&gt;做相应处理，在本例中，就是把所有的值相加。&lt;/li&gt;
&lt;li&gt;输出结果&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在具体实现上，程序需要指定&lt;code&gt;Map&lt;/code&gt;和&lt;code&gt;Reduce&lt;/code&gt;两个方法，这两个方法的参数都是&lt;code&gt;&amp;lt;key, value&amp;gt;&lt;/code&gt;对。例如&lt;code&gt;Map&lt;/code&gt;函数的&lt;code&gt;key&lt;/code&gt;可以是文件名，&lt;code&gt;value&lt;/code&gt;是对应的某一行或者某一块文字。而在&lt;code&gt;Reduce&lt;/code&gt;函数中，输入的&lt;code&gt;key&lt;/code&gt;是某个单词，而&lt;code&gt;value&lt;/code&gt;是1。&lt;code&gt;Reduce&lt;/code&gt;负责归并结果，并输出。&lt;/p&gt;

&lt;h3 id=&#34;map-reduce的环境&#34;&gt;Map Reduce的环境&lt;/h3&gt;

&lt;p&gt;除了根据逻辑实现&lt;code&gt;Map&lt;/code&gt;和&lt;code&gt;Reduce&lt;/code&gt;两个函数外，还需要一个运行Map Reduce的环境，它需要提供下面几项功能：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;把输入数据分块&lt;/li&gt;
&lt;li&gt;在机器前调度程序运行&lt;/li&gt;
&lt;li&gt;处理 &lt;strong&gt;Group by key&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;处理机器故障&lt;/li&gt;
&lt;li&gt;管理机器间通信&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;出错处理&#34;&gt;出错处理&lt;/h3&gt;

&lt;p&gt;MR出错分几种，处理方式也不同：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Map&lt;/code&gt; Worker出错：MasterNode会将此块数据标为未完成，并等待下一轮调度&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Reduce&lt;/code&gt; Worker出错：MasterNode会将正在运行中的任务置为无效，并等待下一轮调度&lt;/li&gt;
&lt;li&gt;MasterNode：任务直接退出。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;map-和-reduce-的数目&#34;&gt;&lt;code&gt;Map&lt;/code&gt;和&lt;code&gt;Reduce&lt;/code&gt;的数目&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;Mapper&lt;/code&gt;的数目会比节点数目多许多，这样就能保证一台节点上会被会到多个任务，即使节点出错，也只会影响到正在运行中的小块任务，对于其它被分配到此节点但是还没有运行的任务来说，可以很方便的调度到其它节点上。如果任务很大，而又有一个节点在任务运行时故障的话，就需要回滚较多的部分。而且大任务也不方便充分利用空闲的机器资源。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Reduce&lt;/code&gt;的数目没有具体要求，但是一般会比&lt;code&gt;Mapper&lt;/code&gt;的数目要少。&lt;/p&gt;

&lt;h3 id=&#34;改良&#34;&gt;改良&lt;/h3&gt;

&lt;p&gt;还是拿WC作为例子。在&lt;code&gt;Reduce&lt;/code&gt;函数处，原始的办法需要传一堆&lt;code&gt;&amp;lt;word, 1&amp;gt;&lt;/code&gt;对到处理端，这会占用较多的带宽和传输时间，所以一个改良就是在传给&lt;code&gt;Reduce&lt;/code&gt;之前先归并一下，相当于多级&lt;code&gt;Reduce&lt;/code&gt;，而这一中间处理是在本地完成的，这样就可以减少对网络带宽的占用，以及乱序Mapper结果时的计算量。&lt;/p&gt;

&lt;p&gt;注意，此种方式并不适用所有计算，例如对多个数取平均值就不可以使用。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blog搬(zhe)家(teng)记</title>
      <link>http://hzmangel.github.io/post/new-github-blog/</link>
      <pubDate>Sat, 13 Jun 2015 11:36:18 +0800</pubDate>
      
      <guid>http://hzmangel.github.io/post/new-github-blog/</guid>
      <description>

&lt;p&gt;其实把blog从WP挪出来的想法很早前就有了，只是由于拖延症的原因一直没去弄。不过最近可能是处于病情的低谷期，所以就动手了。&lt;/p&gt;

&lt;p&gt;当初想把blog搬家的主要需求也就下面这些：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;能用Markdown写。除了写着舒服外，这不是还能装13嘛～&lt;/li&gt;
&lt;li&gt;之前那些内容能弄过来，折腾来折腾去弄了这么久，之前的东西还是想保留下的。&lt;/li&gt;
&lt;li&gt;模板稍微好看点（不过这个最后证实了还是需要自己弄，诶）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;hugo&#34;&gt;Hugo&lt;/h2&gt;

&lt;p&gt;最开始的时候也想着用 &lt;a href=&#34;http://jekyllrb.com/&#34;&gt;Jekyll&lt;/a&gt; 或者 &lt;a href=&#34;http://octopress.org/&#34;&gt;Octopress&lt;/a&gt; ，但是在转换的时候，发现有些&lt;code&gt;Latex&lt;/code&gt;的代码中的行末注释&lt;code&gt;{%&lt;/code&gt;会被认成模板字符串的开始字符，然后就悲剧的说我找不到另一半了啊啊啊我要去死啊啊啊，然后就没有然后了&amp;hellip;&amp;hellip; 想着如果自己写一个的话那又不知道要拖到什么时候了，所以在网上找到了 &lt;a href=&#34;http://gohugo.io/&#34;&gt;Hugo&lt;/a&gt;。所以，其实我选择它的原因也就是它能把我原来所有的东西都显示出来不出错（格式什么的再说哈）。&lt;/p&gt;

&lt;h2 id=&#34;wp2hugo&#34;&gt;wp2hugo&lt;/h2&gt;

&lt;p&gt;敲定了用的框架，下一步就是挪东西了。Jekyll提供了一个挪东西的Gem， &lt;a href=&#34;https://github.com/jekyll/jekyll-import&#34;&gt;Jekyll Import&lt;/a&gt; ，在WP上也有导出的插件，但是拿到的导出文件多少有些问题，有的是没有时间，有的是没有转成Markdown，当然有一个最不能忍的就是导出文件的文件名（我的blog里面的标题是中文，所以也就明白是咋回事了吧&amp;hellip;）。本来想去看那些代码自己弄，再一想算了，自己写吧，这种小工具应该不会引发拖延症的。所以就有了这么个玩意： &lt;a href=&#34;https://github.com/hzmangel/wp2hugo&#34;&gt;wp2hugo&lt;/a&gt; 。这货主要干的事情是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;解析Wordpress导出的XML文件。&lt;/li&gt;
&lt;li&gt;将站点信息存到&lt;code&gt;config.yaml&lt;/code&gt;中。&lt;/li&gt;
&lt;li&gt;将文章存放到带有元信息的Markdown文件中（使用 &lt;a href=&#34;https://pypi.python.org/pypi/html2text&#34;&gt;html2text&lt;/a&gt; 将原来的HTML内容转成Markdown ），使用 &lt;strong&gt;post id&lt;/strong&gt; 作为文件名。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基本上我目前的需求是可以被满足了，不过有一个问题是现在页面中引用的图像文件还是放在老站上的，考虑是不是在脚本中加一个选项把它给放到某个assets目录中去。&lt;/p&gt;

&lt;h2 id=&#34;twenty-ten&#34;&gt;Twenty Ten&lt;/h2&gt;

&lt;p&gt;原来的Blog用的模板是WP自带的 &lt;a href=&#34;https://wordpress.org/themes/twentyten/&#34;&gt;Twenty Ten&lt;/a&gt; ，试了几个Hugo的模板后不喜欢，于是着手把这货给弄过来。由于有一些Hugo的变量，还有就是在port到后面有点烦了所以直接把bootstrap弄进来了，所以也新开了一个 &lt;a href=&#34;https://github.com/hzmangel/hugo-twenty-ten&#34;&gt;GitHub repo&lt;/a&gt; 放这东西。不过有两个东西想吐槽，一是分页，二是标签云。&lt;/p&gt;

&lt;h3 id=&#34;分页&#34;&gt;分页&lt;/h3&gt;

&lt;p&gt;Hugo内置了分页，就是在页面中加入 &lt;code&gt;{{ template &amp;quot;_internal/pagination.html&amp;quot; . }}&lt;/code&gt; ，但是它是把所有页面都列出来的啊，然后我就发现我的页面下面放了2行页码。虽然说增加每页的文章数可以解决这个问题，但这也不是啥解决办法啊。提供的少的可怜的计算功能还有犯晕的模板语法也没法很快弄出来那种显示第一页最后一页的链接，高亮当前页并显示当前页前后各X页，其它页用&lt;code&gt;...&lt;/code&gt;代替的效果，所以最后就直接用上一页下一页了，回头有空的话考虑用那种页面底部加上loading按钮的做法吧。&lt;/p&gt;

&lt;h3 id=&#34;标签云&#34;&gt;标签云&lt;/h3&gt;

&lt;p&gt;Hugo中提供获取所有标签和标签下对应文章数的函数，但是对于生成字体大小不同的标签云来说，它没有提供对应的数学函数。最后选择的做法就是用Hugo把标签名称，数目，以及链接地址生成到某个div的data属性中，然后再用javascript取到其中信息，计算，并生成标签云的代码。Hugo输出的模板是这样的： &lt;a href=&#34;https://github.com/hzmangel/hugo-twenty-ten/blob/master/layouts/partials/sidebar.html&#34;&gt;sidebar.html&lt;/a&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;div class=&amp;quot;sidebar-block&amp;quot;&amp;gt;
  &amp;lt;h5&amp;gt;标签&amp;lt;/h5&amp;gt;
  &amp;lt;div id=&amp;quot;tag_cloud&amp;quot; data-tags=&amp;quot;{{ range $key, $value := .Site.Taxonomies.tags }} [&#39;{{$key}}&#39;, {{len $value}}, &#39;/tags/{{ $key | urlize }}&#39;]; {{ end }}&amp;quot; /&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后生成标签云的js是这样的：&lt;a href=&#34;https://github.com/hzmangel/hugo-twenty-ten/blob/master/static/js/index.js&#34;&gt;index.js&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;$( document ).ready(function() {
    var tag_list = eval($(&amp;quot;#tag_cloud&amp;quot;).data(&#39;tags&#39;).split(&#39;;&#39;));
    tag_list.pop();

    var tag_json = [];
    var total_cnt = 0;
    $.each(tag_list, function( idx, val ) {
        foo = eval(val);
        tag_json.push(foo);
        total_cnt += foo[1];
    })
    generate_tag_cloud(tag_json, total_cnt);
});

var generate_tag_cloud = function(tag_json, total_cnt) {
    tag_cloud_str = &amp;quot;&amp;lt;div id=&#39;tag_cloud_canvas&#39;&amp;gt;&amp;quot;;

    $.each(tag_json, function(idx, val) {
        font_size = 8 + Math.log(val[1])/Math.log(total_cnt) * 18;
        tag_cloud_str += &amp;quot;&amp;lt;a style=\&amp;quot;font-size:&amp;quot; + font_size +&amp;quot;pt\&amp;quot; href=&amp;quot; + val[2] + &amp;quot;&amp;gt;&amp;quot; + val[0] + &amp;quot;&amp;lt;/a&amp;gt;&amp;amp;nbsp; &amp;quot;;
    });

    tag_cloud_str += &amp;quot;&amp;lt;/div&amp;gt;&amp;quot;;

    $(&amp;quot;#tag_cloud&amp;quot;).append(tag_cloud_str);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前差不多就是这样了，偶尔想到啥新的东西再往上加吧，嘿嘿。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>